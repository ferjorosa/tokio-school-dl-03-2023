{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b822ac96-5d93-4a55-9174-1eb1dee8adf7",
   "metadata": {},
   "source": [
    "# 1 - Redes neuronales\n",
    "\n",
    "**Sumario**\n",
    "\n",
    "1. Introducción\n",
    "4. Estructura de la red\n",
    "5. Funciones de activación\n",
    "6. Back propagation\n",
    "7. Inicialización de capas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54518392-f04d-4fb0-b0c2-fced8376f2ed",
   "metadata": {},
   "source": [
    "## 1.1 - Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12a7a03-a26b-4342-8552-bdb0cc0cbdbc",
   "metadata": {},
   "source": [
    "Una red de neuronas artificial es una red representada de manera espacial mediante un grafo dirigido donde cada uno de los nodos representa neuronas, mientras que los arcos reflejan las sinapsis o conexiones entre estas. Esta estructura intenta imitar la distribución de las neuronas en el cerebro humano con el fin de construir un modelo capaz de simular la capacidad de procesamiento de nuestros cerebros.\n",
    "\n",
    "Las neuronas humanas están formadas por tres elementos:\n",
    "\n",
    "1. El **soma** o cuerpo celular, que es su parte principal\n",
    "2. Las **dendritas**, que son las múltiples prolongaciones que salen de distintas partes del soma, en cuyo extremo se encuentran los **botones sinápticos**, conectados a las dendritas de otras neuronas y cuya función consiste en recibir impulsos eléctricos de estas y transmitirlos al soma.\n",
    "3. El **axón**, que es una prlongación del soma la cual se extiende en dirección opuesta a las dendritas y cuya función consiste en conducir un impulso nerviso desde el soma hasta otra neurona, músculo o glándula del cuerpo humano\n",
    "\n",
    "<img src=\"images_1/neurona.jpg\" width=\"600\" data-align=\"center\">\n",
    "\n",
    "Aparentemente, las neuronas biológicas parecen comportarse de una manera bastante simple a nivel individual, pero, en realidad, se encuentran organizadas en una amplia red de miles de millones de neuronas conectadas entre sí que interactúan de manera conjunta y/o sincronizada. Este tipo de estructura permite, en teoría, que un elevado número de elementos sencillos (neuronas) sean capaces de realizar cálculos de gran complejidad de forma muy rápida."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7f49d7-9886-4be5-9bb7-7ec628cc9eea",
   "metadata": {},
   "source": [
    "### 1.1.1 - Primer modelo de red neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757a909e-a2e1-4b3d-a543-e9eab4d3c3b4",
   "metadata": {},
   "source": [
    "En base a esta estructura biológica, en 1943 los investigadores Pitts y McCulloch definieron el que se considera **el primer modelo de red neuronal en 1943**, que pasó a denominarse red de neuronas artificial (ANN, Artificial Neural Network, en inglés). \n",
    "\n",
    "Dicho modelo consistía en una red con dos capas de neuronas conectadas entre sí:\n",
    "1. La primera capa estaba formada por un **conjunto de nodos o neuronas formales**, que representaban la **entrada de la red**.\n",
    "2. La segunda capa estaba formada por un **único nodo o neurona formal**, que representa la **salida de la red**.\n",
    "\n",
    "Una neurona formal consituye una puerta lógica con dos posibles estados internos (encendido o apagado) representados por una variable. Esta red funcionaba como un discriminador del estado de la puerta lógica, de forma que:\n",
    "1. Las neuronas de la primera capa reciben las entradas. Las entradas $x$ podían ser positivas o negativas y las neuronas podían estar activas o no ($w$).\n",
    "2. Se aplica una operación matemática sobre el valor de las entradas\n",
    "3. Se aplica una función de activación con un umbral. De tal tal forma que si el valor supera el umbra la salida es 1 y sino, la salida es 0.\n",
    "\n",
    "<img src=\"images_1/red_neuronal_formal.png\" width=\"500\" data-align=\"center\">\n",
    "\n",
    "En definitiva, si consideramos una red $U$ formada por dos neuronas de entrada y una neurona de salida, tendríamos que calcular la salidad de la red del siguiente modo:\n",
    "\n",
    "$$\n",
    "Y(x) = f(U)\n",
    "$$\n",
    "\n",
    "Donde $f$ se corresponde con una función de activación definida de esta forma:\n",
    "\n",
    "$$\n",
    "f(U) = \\begin{cases}\n",
    "    1,& \\text{si} \\ U > 0\\\\\n",
    "    0,              & \\text{si} \\ U \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Y donde $U$ se calcula de la siguiente manera:\n",
    "\n",
    "$$\n",
    "U = w_{1} x_{1} + w_{2} x_{2} - \\theta\n",
    "$$\n",
    "\n",
    "donde $w_{1}$ y $w_{2}$ son los coeficientes de cada una de las neuronas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51691e42-c26f-4c64-ad32-83563ed3aa94",
   "metadata": {},
   "source": [
    "### 1.1.2 - Teoría Hebbiana"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862453a8-f637-44a4-b36f-7c9a32fe1d02",
   "metadata": {},
   "source": [
    "El modelo anterior no consideraba la actualización de estos coeficientes, es decir, no consideraba el aspecto de *aprendizaje*. En este sentido, surge la teoría hebbiana en 1949, donde se introduce un proceso genérico de modificación de coeficientes (también conocidos como pesos) de manera muy sencilla.\n",
    "\n",
    "Según la regla de Hebb, el peso entre dos neuronas **se incrementa si las dos neuronas se activan simultáneamente y se reduce si se activan por separado**. Los nodos que tienden a ser positivos o negativos al mismo tiempo tienen fuertes pesos positivos, mientras que aquellos que tienden a ser contrarios tienen fuertes pesos negativos. \n",
    "\n",
    "Dada una red de neuronas con vectores de entrada $X=\\{x_{1}, x_{2}, \\dots, x_{n}\\}$, e $Y=\\{y_{1}, y_{2}, \\dots, y_{m}\\}$, así como una matriz de pesos $W$, y un ritmo de aprendizaje $\\alpha$, se define una función destinada a modificar el valor de la matriz de pesos (según el aprendizaje hebbiano) de la siguiente forma:\n",
    "\n",
    "$$\n",
    "w_{ij}' = w_{ij} + \\alpha x_{i} y_{j}\n",
    "$$\n",
    "\n",
    "Mediante esta representación, se pudo definir un algoritmo que permitiera calcular el valor de los pesos a partir del **error cuadrático medio** de las salidas obtenidas con respecto a las salidas esperadas:\n",
    "\n",
    "$$\n",
    "\\frac{1}{N} || Y - WX||\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf54ab0-b52d-4a59-9377-7e2e9be2b90d",
   "metadata": {},
   "source": [
    "### 1.1.3 - Perceptrón simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c20509f-73e5-4aab-9479-26231781fdf6",
   "metadata": {},
   "source": [
    "A partir de ambos trabajos, se desarolló una versión más compleja de red de neuronas artificiales denominada **perceptrón** (Minsky, 1969) que utilizaba un nuevo tipo de neuronas denominadas unidades de umbral lineal (**LTU, Linear Threshold Unit**). En este caso **las entradas y las salidas eran de tipo numérico**, no de tipo binario, confiriéndose una mayor versatilidad sobre las operaciones que podían ser representadas. De esta forma, el resultado generado por una neurona se calculaba mediante la suma ponderada de las entradas ($X$) combinadas con los pesos $W$:\n",
    "\n",
    "$$\n",
    "U = X \\ W^{T} = (x_{1}, x_{2}, \\dots, x_{n}) \\binom{w_{1}}{w_{n}} = \\sum_{i=1}^{n} x_{i} w_{i} = x_{1}w_{1} + x_{2}w_{2} + \\dots +  x_{n}w_{n}\n",
    "$$\n",
    "\n",
    "Al igual que con la red neuronal formal, se aplicaba un función de umbral $f$ sobre el valor generado por la red con la finalidad de generar un resultado de salida normalizado:\n",
    "\n",
    "$$\n",
    "f(U) = \\begin{cases}\n",
    "    1,& \\text{si} \\ U > 0\\\\\n",
    "    0,              & \\text{si} \\ U \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "<img src=\"images_1/perceptron_simple.png\" width=\"500\" data-align=\"center\">\n",
    "\n",
    "La fase de entrenamiento del perceptron se llevaba a cabo mediante **una variación de la regla de Hebb que incorporaba el error cometido por la red**. Así para cada instancia de entrenamiento formada por un vector de entrada $x$ y un valor de salida $y$ en cada una de las neuronas de la red que genera un resultado erróneo, se realizaba una modificación de los pesos de las neuronas de entrada que contribuían a la salida correcta.\n",
    "\n",
    "De esta manera, el proceso definido para la actualización de los pesos sería el siguiente:\n",
    "\n",
    "$$\n",
    "w_{ij}^{t+1} = w_{ij}^{t} + \\alpha (\\hat{y}_{j} - y_{j}) x_{i}\n",
    "$$\n",
    "\n",
    "Donde $w_{ij}^{t}$ es el peso de la conexión entre la neurona de entrada $i$ y la neurona de salida $j$ en el instante de entrenamiento $t$. $x_{i}$ es el valor de entrada de la instancia de entrenamiento, $y_{j}$ es el valor de salida esperado, $\\hat{y}_{j}$ el valor de salidad de la neurona en el estado actual, y $\\alpha$ la tasa de aprendizaje.\n",
    "\n",
    "El perceptrón se diseñó para ser una máquina, en lugar de un programa. Es por ello que se implementó en un hardware especifico denominado como el perceptrón Mark 1. Esta máquina fue diseñada para el reconocimiento de imágenes: tenía una matriz de 400 fotocélulas, conectadas aleatoriamente a las \"neuronas\". Los pesos se codificaron en potenciómetros y las actualizaciones de peso durante el aprendizaje se realizaron mediante motores eléctricos.\n",
    "\n",
    "<img src=\"images_1/Mark_I_perceptron.jpg\" width=\"250\" data-align=\"center\">\n",
    "\n",
    "Aunque inicialmente el perceptrón parecía prometedor, rápidamente se demostró que no podía ser entrenado para reconocer muchas clases de patrones. Sólo era capaz de aprender patrones\n",
    "sencillos, pues el límite de decisión de cada una de las neuronas de salida era lineal.\n",
    "\n",
    "<img src=\"images_1/lineal_no_lineal.png\" width=\"500\" data-align=\"center\">\n",
    "\n",
    "Este y otros muchos problemas fueron identificados en el artículo Perceptrons (Minsky, 1969) donde se destacaba la incapacidad de esta técnica para resolver problemas triviales como el problema de clasificación XOR. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d15aba4-233a-45e4-832d-1bb4049415f3",
   "metadata": {},
   "source": [
    "### 1.1.4 - Perceptrón multicapa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115e81c8-b468-43af-bc33-831976098fca",
   "metadata": {},
   "source": [
    "El artículo de Minsky artículo ocasionó un declive en la investigación sobre redes neuronales hasta la aparición de una nueva técnica que **combinaba múltiples perceptrones** para construir lo que se denominó **perceptrón multicapa**, capaz de resolver muchos de los problemas enunciados por Minsky en su artículo.\n",
    "\n",
    "En el siguiente ejemplo, podemos observar un perceprón multicapa formado por cuatro capas:\n",
    "* Una capa de neuronas de entrada\n",
    "* Dos capas de neuronas ocultas de tipo LTU ($\\mathbb{R}$)\n",
    "* Una capa de neuronas de tipo LTU\n",
    "\n",
    "Cada capa de neuronas LTU tiene definida una **función de activación** y un **bias**. Normalmente, el bias se representa utilizando un tipo especial de neurona denominada neurona de bias que suele devolver siempre un mismo valor, por ejemplo $1$.\n",
    "\n",
    "<img src=\"images_1/perceptron_multicapa.png\" width=\"500\" data-align=\"center\">\n",
    "\n",
    "Esta nueva estructura permitía resolver problemas más complejos (**no estaba limitada a problemas linealmente separables**) mediante la combinación de las capas, pero, según se iba aumentando el número de neuronas, **el proceso de aprendizaje se volvía más complejo a nivel computacional**, hasta que en 1986 se presentó una nueva técnica de actualización de pesos llamada *backpropagation*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554b38b2-d75c-4fc7-abfe-9b751b80ad99",
   "metadata": {},
   "source": [
    "## 1.2 - Estructura de la red"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada9fe88-2d43-4bab-b632-dcede5e4464e",
   "metadata": {},
   "source": [
    "Desde el punto de vista estructural, una red de neuronas artificial es un **grafo dirigido y ponderado** donde:\n",
    "* Las neuronas son los nodos.\n",
    "* Las conexiones entre ellas constituyen los arcos del grafo.\n",
    "\n",
    "La conectividad entre neuronas viene definida según capas secuenciales conectadas entre si:\n",
    "* En primer lugar, una capa de entrada.\n",
    "* Después, una serie de capas ocultas.\n",
    "* Finalmente, una capa de salida\n",
    "\n",
    "Dependiendo de cómo se produzcan las conexiones entre estas capas, podemos diferenciar dos tipos de redes neuronales:\n",
    "\n",
    "* Las **redes de propagación hacia delante** (*feedforward networks*), donde no existen bucles en el grafo que define la red de neuronas.\n",
    "* Las **redes recurrentes** (*recurrent networks*), donde existen bucles producidos por las conexiones de retroalimentación que poseen algunas neuronas.\n",
    "\n",
    "<img src=\"images_1/fnn_rnn.png\" width=\"700\" data-align=\"center\">\n",
    "\n",
    "Con independencia del tipo de red, cada una de las capas, exceptuando las de entrada, está formada por tres elementos básicos:\n",
    "\n",
    "* **Operación matemática**.\n",
    "* **Bias**.\n",
    "* **Función de activación**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
