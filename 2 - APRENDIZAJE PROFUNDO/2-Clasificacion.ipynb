{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afaf5b26-0145-461b-912e-d2f86ddfc46d",
   "metadata": {},
   "source": [
    "# 2 - Clasificación\n",
    "\n",
    "**Sumario**\n",
    "\n",
    "1. Introducción\n",
    "2. Funciones de pérdida\n",
    "3. Métricas\n",
    "4. Clasificación binaria\n",
    "5. Clasificación multiclase\n",
    "6. Clasificación multietiqueta\n",
    "7. Optimización de hiperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44cd507-ca85-4d75-9f22-5849325174f1",
   "metadata": {},
   "source": [
    "## 2.1 - Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1261fd17-4a73-4db0-ad46-d4992c1e4666",
   "metadata": {},
   "source": [
    "En tareas de clasificación, nuestro objetivo es construir un modelo capaz de **predecir correctamente una o varias etiquetas para cada uno de las instancias de entrada**. \n",
    "\n",
    "En la siguiente figura se presenta uno de los problemas más comunes de clasificación: se quiere construir un modelo capaz de clasificar un conjunto de *emails* cuya clase está formada por dos etiquetas: \n",
    "1. correo válido\n",
    "2. correo spam\n",
    "\n",
    "El tipo de comportamiento del modelo vendrá definido por la estructura de la clase mediante la que están definidos los diferentes ejemplos. En función del tipo de clase que hayamos definido, podemos diferenciar tres tipos de modelos:\n",
    "* **Clasificación binaria** (una etiqueta, dos clases).\n",
    "* **Clasificación multiclase** (una etiqueta, múltiples clases).\n",
    "* **Clasificación multietiqueta** (múltiples etiquetas, dos clases)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59ac598-f503-42ea-bdf3-168b029ae134",
   "metadata": {},
   "source": [
    "## 2.2 - Funciones de pérdida (*loss*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb83f8b-84bc-42b9-9673-40320da9a920",
   "metadata": {},
   "source": [
    "A la hora de efectuar un proceso de clasficación, podemos aplicar diferentes funciones de pérdida dependiendo del tipo de salida que queramos obtener. Asi pues:\n",
    "* **Clasificación binaria**\n",
    "    * Binary crossentropy loss\n",
    "* **Clasificación multiclase**\n",
    "    * Multiclass crossentropy loss\n",
    "    * Sparse multiclass crossentropy loss\n",
    "* **Clasificación multietiqueta**\n",
    "    * Binary crossentropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0226872d-19f1-4a0e-aece-4910d6e4d362",
   "metadata": {},
   "source": [
    "### 2.2.1 - Binary crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4867784-0f2f-47fa-88b4-bd882f498546",
   "metadata": {},
   "source": [
    "La **entropía cruzada binaria** (binary crossentropy) es un tipo de función de pérdida utilizada para la construcción de modelos de **clasificación binaria**. Se calcula mediante la siguiente fórmula:\n",
    "\n",
    "$$\n",
    "f(y_{i}, \\hat{y}_{i}) = -\\frac{1}{n} \\sum_{i=1}^{n} y_{i} \\log(\\hat{y}_{i}) + (1 - y_{i}) \\log(1 - \\hat{y}_{i})\n",
    "$$\n",
    "\n",
    "donde\n",
    "* $n$ es el número de instancias de entrenamiento utilizadas para calcular el valor de pérdida.\n",
    "* $y_{i}$ es el valor de salida esperado\n",
    "* $\\hat{y}_{i}$ es el valor de salida real\n",
    "\n",
    "Para poder utilizar este tipo de función de pérdida es necesario recurrir a la **función sigmoidea como función de activación de la última capa de la red**, ya que es la única compatible con esta función de pérdida. Esto se debe a que la función de pérdida debe calcular el logaritmo de $y_{i}$, que solo existe cuando el valor de $\\hat{y}_{i}$ se sitúa entre $0$\n",
    "y $1$.\n",
    "\n",
    "<img src=\"images_2/binary_crossentropy.png\" width=\"700\" data-align=\"center\">\n",
    "\n",
    "**Nota:** La función softmax (con 2 valores) también valdría en este caso ya que al fin y al cabo **la función sigmoidea no es más que una versión binaria de la función softmax**.\n",
    "\n",
    "**Extra:** [**Derivación de la función de pérdida para su aplicación con el descenso por gradiente**](https://www.python-unleashed.com/post/derivation-of-the-binary-cross-entropy-loss-gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b0b2be-9126-4951-81bc-1348397b859c",
   "metadata": {},
   "source": [
    "### 2.2.2 - Multiclass crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a6a495-9064-4362-a8c1-d9a9f6fc5e65",
   "metadata": {},
   "source": [
    "La **entropía cruzada multiclase o categórica**  (multiclass crossentropy or categorical crossentropy) es un tipo de función de pérdida utilizada para la construcción de modelos de **clasificación multiclase**. Se calcula mediante la siguiente fórmula:\n",
    "\n",
    "$$\n",
    "f(y_{i}, \\hat{y}_{i}) = - \\sum_{i}^{n} y_{i} \\log \\hat{y}_{i}\n",
    "$$\n",
    "\n",
    "donde\n",
    "* $n$ es el número de instancias de entrenamiento utilizadas para calcular el valor de pérdida.\n",
    "* $y_{i}$ es el valor de salida esperado\n",
    "* $\\hat{y}_{i}$ es el valor de salida real\n",
    "\n",
    "Para poder utilizar este tipo de función de pérdida, se recomienda usar la **función softmax como función de activación de la última capa de la red**, pues esta solo **necesita que la salida del modelo sea positiva** (por el logaritmo). Por tanto, la función softmax se adapta perfectamente, ya que efectúa una reescalada de la salida, de manera que todos los valores son expresados entre $0$ y $1$.\n",
    "\n",
    "<img src=\"images_2/multiclass_crossentropy.png\" width=\"700\" data-align=\"center\">\n",
    "\n",
    "**Extra:** [**Derivación de la función de multiclass crossentropy loss para su aplicación con el descenso por gradiente**](https://towardsdatascience.com/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a85168b-19a3-4513-9ed0-4196098a5ea1",
   "metadata": {},
   "source": [
    "### 2.2.3 - Sparse multiclass crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cc10e2-dbb7-4341-8e6e-b28c30b7163e",
   "metadata": {},
   "source": [
    "La **entropía cruzada multiclase dispersa** (sparse multiclass crossentropy) es un tipo de función de pérdida utilizada para la construcción de modelos de **clasificación multiclase cuando el número de clases es muy elevado**. \n",
    "\n",
    "Existen situaciones donde el número posible de clases es muy grande,. or ejemplo, si tenemos que clasificar marcas a partir de una descripción de producto (hay millones de marcas en el mundo). Esta función de pérdida **realiza el mismo cálculo que la función de entropía cruzada multiclase**, pero **sin necesidad de que el vector de entrada sea una codificación de tipo one-hot**. Por tanto, se calcula mediante la siguiente fórmula:\n",
    "\n",
    "$$\n",
    "f(y_{i}, \\hat{y}_{i}) = - \\sum_{i}^{n} y_{i} \\log \\hat{y}_{i}\n",
    "$$\n",
    "\n",
    "\n",
    "<img src=\"images_2/sparse_multiclass_crossentropy.png\" width=\"700\" data-align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a122688f-51e4-4bb8-abbd-ca5dd5442fe9",
   "metadata": {},
   "source": [
    "## 2.3 - Métricas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938c581b-c324-48c5-8e6e-d289653b94d0",
   "metadata": {},
   "source": [
    "Para evaluar los modelos de clasificación, existen diferentes tipos de métricas de \"bondad\".\n",
    "\n",
    "La **matriz de confusión** (confusion matrix) es un procedimiento para extraer la información necesaria y calcular las diferentes métricas de \"bondad\" utilizadas por los algoritmos de clasificación. Consiste en una matriz $n \\times n$, donde $n$ es el número de clases, que describe el rendimiento del modelo a partir de los datos del conjunto de\n",
    "test. Su denominación alude a su finalidad: **identificar dónde está confundiendo las clases el modelo**.\n",
    "\n",
    "En el caso más básico, se aplica a una clasificación binaria una matriz de confusión formada por cuatro características:\n",
    "* **Verdaderos positivos (VP)**. Aquellos ejemplos del conjunto de test cuya clase esperada es 1 (verdadero) y el resultado generado por el modelo de aprendizaje es 1 (verdadero).\n",
    "* **Verdaderos negativos (VN)**. Aquellos ejemplos del conjunto de test cuya clase esperada es 0 (negativo) y el resultado generado por el modelo de aprendizaje es 0 (negativo).\n",
    "* **Falsos positivos (FP)**. Aquellos ejemplos del conjunto de test cuya clase esperada es 0 (falso) y el resultado generado por el modelo de aprendizaje es 1 (verdadero).\n",
    "* **Falsos negativos (FN)**. Aquellos ejemplos del conjunto de test cuya clase esperada es 1 (Verdadero) y el resultado generado por el modelo de aprendizaje es 0 (Falso).\n",
    "\n",
    "A continuación, vamos a definir las diferentes métricas de bondad que se obtienen a través de la matriz de confusión a partir del siguiente ejemplo:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Matriz de confusión binaria</th>\n",
    "        <th>Ejemplo</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/matriz_confusion_1.png\" width=\"500\" data-align=\"center\"></td>\n",
    "        <td><img src=\"images_2/matriz_confusion_3.png\" width=\"500\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "**Nota:** Esta representación también se puede aplicar a modelos de clasificación no binaria como, por ejemplo, aquellos que presentan tres clases:\n",
    "\n",
    "<img src=\"images_2/matriz_confusion_2.png\" width=\"500\" data-align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2977aa-218f-40cd-84aa-64c62c7ccc46",
   "metadata": {},
   "source": [
    "### 2.3.1 - Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b91f0df-f5e2-4f0f-92fb-c4e24fdbcece",
   "metadata": {},
   "source": [
    "La **exactitud** (**accuracy** en inglés) se define como **el grado de concordancia entre el resultado generado por el modelo y el resultado real**. Se obtiene a través de la siguiente fórmula:\n",
    "\n",
    "$$\n",
    "A = \\frac{VN + VP}{VP + VN +FP + FN}\n",
    "$$\n",
    "\n",
    "Con respecto a los datos del ejemplo definido anteriormente, obtendríamos un accuracy de $0.92$:\n",
    "\n",
    "$$\n",
    "A = \\frac{87 + 5}{87 + 5 + 2 + 6} = 0.92\n",
    "$$\n",
    "\n",
    "El accuracy **no es una métrica adecuada para situaciones donde las clases (labels) del conjunto de datos se encuentran desbalanceadas**. En estos casos, resulta mucho más conveniente usar las métricas de precision, recall o F1-Score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9c7f80-b35a-4e0f-b4e5-700c0a58f348",
   "metadata": {},
   "source": [
    "### 2.3.2 - Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52feb24-ae42-4d8a-8b78-b2da7c2bf6f7",
   "metadata": {},
   "source": [
    "La **precisión** (**precision** en ingles) es una medida centrada en el modelo porque nos indica **el grado de precision del modelo cuando predice una determinada clase**. Por ejemplo, en el caso de las imagenes de animales, cuando nuestro modelo que una imagen es un perro, cual es la probabilidad de que sea esto cierto. Se obtiene a través de la siguiente fórmula (ejemplo para 2 clases, centrándonos en la clase \"positivo\"):\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{VP}{VP + FP}\n",
    "$$\n",
    "\n",
    "Con respecto a los datos del ejemplo definido anteriormente, obtendríamos una precision de $0.45$:\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{5}{5 + 6} = 0.45\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a94f1e3-4f33-48f7-b919-0e10465166e1",
   "metadata": {},
   "source": [
    "### 2.3.3 - Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba262341-1510-4829-b7fe-4b3480bc41af",
   "metadata": {},
   "source": [
    "La **exhaustividad** (**recall** en inglés) es una medida centrada en los datos porque nos indica **como de bien se predice una clase de los datos**. Por ejemplo, en el caso de las imagenes de animales, cómo de bien identifica nuestro modelo a los perros. Se obtiene a través de la siguiente fórmula ejemplo para 2 clases, centrándonos en la clase \"positivo\"):\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{VP}{VP + FN}\n",
    "$$\n",
    "\n",
    "Con respecto a los datos del ejemplo definido anteriormente, obtendríamos una precision de $0.62$:\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{5}{2 + 6} = 0.62\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fed9bc-4c08-46ce-9041-a87618df8f62",
   "metadata": {},
   "source": [
    "### 2.3.4 - F1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203e9df6-4cc0-41be-875e-2867989c1002",
   "metadata": {},
   "source": [
    "La **puntuación F1** (**F1-score**) es la **media harmónica de la precisón y de la exhaustividad (recall)**. Así esta métrica es útil ya que nos permite combinar ambas métricas:\n",
    "\n",
    "$$\n",
    "\\text{F1-score} = 2 * \\frac{P * R}{P + R}\n",
    "$$\n",
    "\n",
    "Con respecto a los datos del ejemplo definido anteriormente, obtendríamos una F1-score de $0.52$:\n",
    "\n",
    "$$\n",
    "\\text{F1-score} = 2 * \\frac{0.45 * 0.62}{0.45 + 0.62} = 0.52\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceabea26-266f-4057-b3ea-214a524c4dfe",
   "metadata": {},
   "source": [
    "### 2.3.5 - F$\\beta$ score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af35ac2-2ea1-4503-9d26-a57fe8194063",
   "metadata": {},
   "source": [
    "La puntuación F$\\beta$ (**F$\\beta$ score**) es una métrica que **combina la precisión y la exhaustividad (recall) con un factor de variación** $\\beta$ que permite modificar el grado de importancia que deseamos asigna a cada una de estas métricas. **Cuanto más elevado el valor de $\\beta$, mayor es la importancia que concedemos al *recall***:\n",
    "\n",
    "$$\n",
    "\\text{F1-score} = (1 + \\beta^{2}) * \\frac{P * R}{(\\beta * P) + R}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c26e327-38c8-4004-bb60-0361084660c4",
   "metadata": {},
   "source": [
    "## 2.4 - Clasificación binaria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b03f3af-b920-4133-ac24-d693097c6fc9",
   "metadata": {},
   "source": [
    "A continuación vamos a describir cómo construir una red neuronal para un problema de clasificación binaria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c540d25-c21a-4da0-8f43-e2bd5800b36e",
   "metadata": {},
   "source": [
    "### 2.4.1 - Preparación de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971af680-9a51-418f-809c-451037362abc",
   "metadata": {},
   "source": [
    "La primera fase de todo proceso de ML es la preparación de los datos. En este caso, vamos a utilizar **el conjunto de datos sobre pruebas diagnósticas para la detección del cáncer de mama**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b052da20-bd68-4e40-bd22-4bfdab959719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data\", header=None)\n",
    "data.columns = [\n",
    "    \"Sample_code_number\",\n",
    "    \"Clump_Thickness\",\n",
    "    \"Uniformity_of_Cell_Size\",\n",
    "    \"Uniformity_of_Cell_Shape\",\n",
    "    \"Marginal_Adhesion\",\n",
    "    \"Single_Epithelial_Cell_Size\",\n",
    "    \"Bare_Nuclei\",\n",
    "    \"Bland_Chromatin\",\n",
    "    \"Normal_Nucleoli\",\n",
    "    \"Mitoses\",\n",
    "    \"Class\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3f9745-3ecc-4485-812d-def7142dd8d9",
   "metadata": {},
   "source": [
    "Una vez realizada la carga de los datos, debemos llevar a cabo una serie de transformaciones y modificaciones en la información del conjunto de datos con el objetivo de adecuarlos y poder construir nuestra red de neuronas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f788bd-8585-4c9c-b35f-e83f7255c670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminación de la columna que contiene el id de las muestras \n",
    "data.drop(\"Sample_code_number\", axis=\"columns\", inplace=True)\n",
    "\n",
    "# Modificación de los valores ? que existen en la columna Bare Nuclei (en este caso por el valor -1)\n",
    "data[\"Bare_Nuclei\"] = data['Bare_Nuclei'].replace('?', '-1')\n",
    "\n",
    "# Modificación de las etiquetas para que los valores sean 0 y 1 \n",
    "data[\"Class\"] = data[\"Class\"].map(lambda x: 1 if x == 4 else 0)\n",
    "\n",
    "# Generación de los conjuntos de entrenamiento y test split_handler (en este caso no vamos a generar de validación aqui, lo haremos mas adelante)\n",
    "# split proportion: 0.88 train, 0.12 test\n",
    "train_set, test_set = train_test_split(data, test_size=0.12, random_state=0, stratify=data[\"Class\"])\n",
    "\n",
    "# Transformación del conjunto de entrenamiento en tensores \n",
    "X_train = tf.convert_to_tensor (train_set.iloc[:, :9], np.int32) \n",
    "y_train = tf.convert_to_tensor (train_set.iloc[:, 9:], np.int8)\n",
    "\n",
    "# Transformación del conjunto de test en tensores \n",
    "X_test = tf.convert_to_tensor (test_set.iloc[:, :9], np.float32)\n",
    "y_test = tf.convert_to_tensor (test_set.iloc[:, 9:], np.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec4c235-48ef-4ec1-8ee5-0b07121acc39",
   "metadata": {},
   "source": [
    "**Transformamos los conjuntos de datos en tensores porque ese es el formato esperado por la red para los datos**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fafce0b-5b86-4631-b92d-6e44556925ab",
   "metadata": {},
   "source": [
    "### 2.4.2 - Construcción de la red"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac91d61-370f-4482-b4ae-3b4db0a112a8",
   "metadata": {},
   "source": [
    "Una vez preparados los datos, podemos construir nuestra red de neuronas. Para ello, vamos a crear una red formada por tres capas mediante Keras:\n",
    "\n",
    "* Una capa de entrada que acepte tensores unidimensionales de tamaño 9. Para ello vamos a usar `keras.layers.Flatten`.\n",
    "* Una capa densa con 18 neuronas cuya función de activación sea una ReLU.\n",
    "* Una capa densa con una sola neurona con una función de activación de tipo sigmoidea, de manera que nos devuelva un valor comprendido entre 0 y 1 que se corresponderá con una de las dos clases que podemos obtener."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f91ed58-5b43-4b82-91c5-30282a6a5724",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# Creación de las capas que forman la estructura de la red\n",
    "layers = [keras.layers.Flatten(input_shape=(9,)),\n",
    "          keras.layers.Dense(18, activation=tf.nn.relu),\n",
    "          keras.layers.Dense(1, activation=tf.nn.sigmoid),\n",
    "         ]\n",
    "\n",
    "# Compilación de la red de neuronas, agrupando las diferentas capas de forma secuencial\n",
    "model = keras.Sequential(layers, name=\"binary_classification_model\")\n",
    "\n",
    "# Configuración del algoritmo de optimización y de la función de loss\n",
    "model.compile(optimizer=\"sgd\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"]\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc20be8-3f08-43de-ba79-155d1b5731f6",
   "metadata": {},
   "source": [
    "Tras la compilación, podemos comprobar la estructura de nuestra red mediante la función `summary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2238403e-b77e-4c2c-82c3-62825bd63563",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24d090f-7df9-47ee-93d1-aed808531892",
   "metadata": {},
   "source": [
    "Con respecto a los parámetros observamos lo siguiente:\n",
    "* La capa de entrada `flatten_1` no tiene parámetros ya que simplemente nos sirve para introducir información en la red.\n",
    "* La capa densa `dense_2` está formada por 18 neuronas, cada una de ellas con 9 parámetros de entrada y un parámetro de *bias*, haciendo un total de **180**.\n",
    "* La capa densa `dense_3` está formada por 1 neurona con 18 parámetros de entrada y un parámetro de *bias*, haciendo un total de **19**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f2f65f-fc10-4f53-acc0-dc7afc606e1d",
   "metadata": {},
   "source": [
    "### 2.4.3 - Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daefa3c7-2749-4bcd-8977-dabc755b21a7",
   "metadata": {},
   "source": [
    "Una vez compilada nuestra red de neuronas, podemos abordar el proceso de entrenamiento mediante la función `fit`. En este caso, hemos seleccionado las siguientes opciones:\n",
    "* Número de iteraciones (*epochs*): 25\n",
    "* Tamaño del batch de entrenamiento: 100 ejemplos\n",
    "* Tamaño del conjunto de validación: 8% del conjunto de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314f9850-c1a7-49b3-9eee-b8a5faa152ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from time import time\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "# Definición de los callback de TF Board\n",
    "tensorboard_callback = TensorBoard(log_dir=\"logs\")\n",
    "\n",
    "# Ejecución del proceso de aprendizaje\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=25, # Numero de iteraciones\n",
    "    batch_size=100, # Tamaño de los batches\n",
    "    validation_split=0.08, # Tamaño del conjunto de validación\n",
    "    callbacks=[tensorboard_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf28da2e-2ba2-422a-983c-8bd20740ed06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluación del modelo mediante el conjunto de test\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355e3848-f30e-4145-b310-1e881d333a2f",
   "metadata": {},
   "source": [
    "### 2.4.4 - Inferencia y visualización"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56245714-e905-4781-bc4e-d7f5e2e7c06b",
   "metadata": {},
   "source": [
    "Una vez finalizado el proceso de aprendizaje, podemos recurrir a TensorBoard para comprobar la evolución del proceso de entrenamiento.\n",
    "\n",
    "**Nota:** Dado el rapido aprendizaje del modelo, TensorBoard se confunde un poco al generar los gráficos ya que está acostumbrado a que se siga un proceso con forma \"logarítmica\". [Para ver un ejemplo más \"común\", podemos ejecutar el siguiente notebook en Google Colab.](https://colab.research.google.com/github/tensorflow/tensorboard/blob/master/docs/tensorboard_in_notebooks.ipynb#scrollTo=ixZlmtWhMyr4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f49a99-2ca5-4949-97ac-46cb3fc11a4c",
   "metadata": {},
   "source": [
    "#### Ejecución en local\n",
    "\n",
    "En caso de que estemos ejecutando el notebook en nuestra máquina local, simplemente tendríamos que situarnos con la terminal en el directorio del notebook y correr el siguiente comando:\n",
    "```\n",
    "tensorboard --logdir logs\n",
    "```\n",
    "Se nos indicará entonces cual es la dirección local a la que tenemos que acceder y mostrará la aplicación de TensorBoard con los logs de la ejecución actual:\n",
    "\n",
    "<img src=\"images_2/tensorboard.png\" width=\"800\" data-align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf467b3-83fa-4902-9061-2ba36290bae4",
   "metadata": {},
   "source": [
    "#### Ejecución en Google Colab\n",
    "\n",
    "En caso de que estemos ejecutando este notebook en Google Colab, no podriamos acceder al directorio local y en su lugar deberiamos utilizar una extensiónde que nos permitiría generar TensorBoard dentro de Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac0611f-4ef5-4490-ba91-03bf8d1d2159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solo en Google Colab\n",
    "# %load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cfc5a6-879a-478b-a4e8-d70b3f4f9806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962cbbc2-3932-4ea3-a05b-1a8f93c42baf",
   "metadata": {},
   "source": [
    "## 2.5 - Clasificación multiclase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f8f036-e41d-4382-b5d4-e5eef3cd695a",
   "metadata": {},
   "source": [
    "A continuación vamos a describir cómo construir una red neuronal para un problema de clasificación multiclase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03df94f1-03d9-4d24-b38d-600655f0e061",
   "metadata": {},
   "source": [
    "### 2.5.1 - Preparación de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d7631c-b891-4472-a10e-2f11b43c87f8",
   "metadata": {},
   "source": [
    "Como ya sabemos, la primera fase de todo proceso de ML es la preparación de los datos. En este caso, **vamos a utilizar el famoso dataset Iris**, que consiste en clasificar flores en tres variedades según carateristicas del pétalo y el sépalo de la flor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348c3e39-62ef-4681-845c-6823adcdbcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\", header=None)\n",
    "data.columns = [\n",
    "    \"sepal_length\",\n",
    "    \"sepal_width\",\n",
    "    \"petal_length\",\n",
    "    \"petal_width\",\n",
    "    \"class\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ce336b-8189-4093-9041-f80602933b38",
   "metadata": {},
   "source": [
    "Una vez realizada la carga de los datos, debemos llevar a cabo una serie de transformaciones y modificaciones en la información del conjunto de datos con el objetivo de adecuarlos y poder construir nuestra red de neuronas:\n",
    "* Codificación numérica de las clases que están representadas mediante cadenas de caracteres.\n",
    "* Codificación de la clase en formato *one-hot*.\n",
    "* Codificación numérica de todos los atributos en formato decimal (*float*).\n",
    "* Generación de los splits de entrenamiento y test.\n",
    "* Transformación de los splits a formato *tensor* para poder ser ingeridos por la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3cef9e-74e7-4fcd-913b-8bfc7718a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Definimos un codificador para transformar las clases en números enteros\n",
    "class_encoder = LabelEncoder()\n",
    "class_encoder.fit (data[\"class\"])\n",
    "integer_class_representation = class_encoder.transform (data[\"class\"])\n",
    "\n",
    "# Definimos un codificador para transformar las clases de formato entero a formato one-hot\n",
    "one_hot_encoder = OneHotEncoder()\n",
    "\n",
    "# Generación de los conjuntos de entrenamiento y test split_handler (en este caso no vamos a generar de validación aqui, lo haremos mas adelante)\n",
    "# split proportion: 0.88 train, 0.12 test\n",
    "train_set, test_set = train_test_split(data, test_size=0.12, random_state=0, stratify=data[\"class\"])\n",
    "\n",
    "# Transformación del conjunto de entrenamiento en tensores \n",
    "X_train = tf.convert_to_tensor(train_set.iloc[:, :4], np.int32)\n",
    "y_train = tf.convert_to_tensor(one_hot_encoder.fit_transform(train_set.iloc[:, 4:]).toarray())\n",
    "\n",
    "# Transformación del conjunto de test en tensores \n",
    "X_test = tf.convert_to_tensor(test_set.iloc[:, :4], np.int32)\n",
    "y_test = tf.convert_to_tensor(one_hot_encoder.fit_transform(test_set.iloc[:, 4:]).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c610c3b0-ef5e-4bca-b278-e39928b5aec6",
   "metadata": {},
   "source": [
    "En vez de utilizar el one-hot encoder, podemos utilizar la forma entera de las clases con la función de pérdida "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6258d42d-0360-4823-b61b-c2aa6fb5b279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformación del conjunto de entrenamiento en tensores donde la clase está en formato entero, en vez de en formato one-hot\n",
    "y_train_integer = tf.convert_to_tensor(class_encoder.fit_transform(train_set.iloc[:, 4:])) \n",
    "\n",
    "# Transformación del conjunto de test en tensores donde la clase está en formato entero, en vez de en formato one-hot\n",
    "y_test_integer = tf.convert_to_tensor(class_encoder.fit_transform(test_set.iloc[:, 4:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2a8396-1921-4087-80e9-c1494fa1d6e1",
   "metadata": {},
   "source": [
    "### 2.5.2 - Construcción de la red"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccadab18-8486-4141-b1a1-1f515cae4df0",
   "metadata": {},
   "source": [
    "Una vez preparados los datos, podemos construir nuestra red. En este sentido, vamos a crear una red formada por tres capas mediante Keras:\n",
    "* Una capa de de tipo `flatten` que acepte como entrada tensores unidimensionales de dimensión 4.\n",
    "* Una capa densa de 8 neuronas con una función de activación de tipo ReLU.\n",
    "* Una capa densa de 3 neuronas con una función de activación de tipo `softmax` que nos devolverá la probabilidad de que el ejemplo pertenezca a cada una de las clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d919f2f-0c47-4440-96a7-6e518b539431",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# Creación de las capas que forman la estructura de la red\n",
    "layers_onehot = [keras.layers.Flatten(input_shape=(4,)),\n",
    "                 keras.layers.Dense(8, activation=tf.nn.relu),\n",
    "                 keras.layers.Dense(3, activation=tf.nn.softmax),\n",
    "                ]\n",
    "\n",
    "# Compilación de la red de neuronas, agrupando las diferentas capas de forma secuencial\n",
    "model_onehot = keras.Sequential(layers_onehot, name=\"multiclass_classification_model_onehot\")\n",
    "\n",
    "# Configuración del algoritmo de optimización y de la función de loss\n",
    "# Nota, en este caso vamos a usar Adam en vez de SGD simplemente para demostrar \n",
    "# como se le llama en Keras\n",
    "model_onehot.compile(optimizer=\"adam\",\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"]\n",
    "             )\n",
    "\n",
    "model_onehot.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cef1ba1-bc5c-4c09-af9a-7b5c564a5780",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_integer = [keras.layers.Flatten(input_shape=(4,)),\n",
    "                  keras.layers.Dense(8, activation=tf.nn.relu),\n",
    "                  keras.layers.Dense(3, activation=tf.nn.softmax),\n",
    "                 ]\n",
    "\n",
    "# Compilación de la red de neuronas, agrupando las diferentas capas de forma secuencial\n",
    "model_integer = keras.Sequential(layers_integer, name=\"multiclass_classification_model_integer\")\n",
    "\n",
    "# Configuración del algoritmo de optimización y de la función de loss\n",
    "# Nota, en este caso vamos a usar Adam en vez de SGD simplemente para demostrar \n",
    "# como se le llama en Keras\n",
    "model_integer.compile(optimizer=\"adam\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"]\n",
    "             )\n",
    "\n",
    "model_integer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5d7d0c-6a3e-447a-9078-f61e1fbcdb9d",
   "metadata": {},
   "source": [
    "Con respecto a los parámetros observamos lo siguiente:\n",
    "* La capa de entrada `flatten_3` no tiene parámetros ya que simplemente nos sirve para introducir información en la red.\n",
    "* La capa densa `dense_6` está formada por 8 neuronas, cada una de ellas con 4 parámetros de entrada y un parámetro de *bias*, haciendo un total de **40**.\n",
    "* La capa densa `dense_7` está formada por 3 neuronas con 8 parámetros de entrada y un parámetro de *bias*, haciendo un total de **27**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a0cf9d-4c4c-438f-ba75-de902c9cbe4d",
   "metadata": {},
   "source": [
    "### 2.5.3 - Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfae5d68-7623-4c18-9fbb-1f1c195e8357",
   "metadata": {},
   "source": [
    "Una vez compilada nuestra red de neuronas, podemos abordar el proceso de entrenamiento mediante la función `fit`. En este caso, hemos seleccionado las siguientes opciones:\n",
    "* Número de iteraciones (*epochs*): 125\n",
    "* Tamaño del batch de entrenamiento: 10 ejemplos\n",
    "* Tamaño del conjunto de validación: 8% del conjunto de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7f6e0e-360d-46c6-b1ab-4731adb29ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "# Definición de los callback de TF Board\n",
    "tensorboard_callback = TensorBoard(log_dir=\"logs_multiclass_onehot\")\n",
    "\n",
    "# Ejecución del proceso de aprendizaje\n",
    "model_onehot.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=125,\n",
    "    batch_size=10,\n",
    "    validation_split=0.08,\n",
    "    callbacks=[tensorboard_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b73209e-0af0-4bb1-a85d-cfe117d8382e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "# Definición de los callback de TF Board\n",
    "tensorboard_callback = TensorBoard(log_dir=\"logs_multiclass_integer\")\n",
    "\n",
    "# Ejecución del proceso de aprendizaje\n",
    "model_integer.fit(\n",
    "    X_train,\n",
    "    y_train_integer,\n",
    "    epochs=125,\n",
    "    batch_size=10,\n",
    "    validation_split=0.08,\n",
    "    callbacks=[tensorboard_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743c261b-c607-46ac-9ef7-dbe81ce65132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluación de los modelos mediante el conjunto de test\n",
    "test_loss_onehot, test_acc_onehot = model_onehot.evaluate(X_test, y_test)\n",
    "test_loss_integer, test_acc_integer = model_integer.evaluate(X_test, y_test_integer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a00f6c-db89-40e3-b32c-5e0e1646b605",
   "metadata": {},
   "source": [
    "### 2.5.4 - Inferencia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31f78aa-d419-4a00-bde8-8d13f72f6257",
   "metadata": {},
   "source": [
    "En este caso, en vez de visualizar los resultados obtenidos durante el proceso de entrenamiento, vamos a ver cómo podríamos almacenar el modelo y volver a cargarlo para efecturar predicciones. Así, vamos a almacenar el modelo en el fichero `multiclass_classification_model.h5` y, después, vamos a cargarlo de neuvo para generar las predicciones sobre el conjunto de test comprobando si estas cuadran con el resultado esperado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2167f22-a9f2-45c6-90de-2c13339fb5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_onehot.save(\"multiclass_classification_model.h5\")\n",
    "\n",
    "old_model = keras.models.load_model(\"multiclass_classification_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632ddd2e-8c2c-44d6-8ef8-30e067963dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1\n",
    "predictions_onehot = model_onehot.predict(X_test)\n",
    "predictions_integer = model_integer.predict(X_test)\n",
    "\n",
    "print(f\"Predicción (one-hot): {predictions_onehot[index]}\")\n",
    "print(f\"Predicción (integer): {predictions_integer[index]}\\n\")\n",
    "print(f\"Real (one-hot): {y_test[index]}\")\n",
    "print(f\"Real (integer): {y_test_integer[index]}\\n\")\n",
    "print(test_set.iloc[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc24033-27dd-499d-9ebe-c88486e53d84",
   "metadata": {},
   "source": [
    "Como se puede observar, el valor real esperado es la primera clase (Iris-setosa), la cual es modelo predice correctamente con una confianza (probabilidad) de mas del 90%. Esta predicción es correcta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2304dc8-41ed-4ad4-9eb5-2143a3759dc9",
   "metadata": {},
   "source": [
    "## 2.6 - Clasificación multietiqueta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d460cc59-2f0b-4f30-a8fd-7cb5ef8e16ea",
   "metadata": {},
   "source": [
    "A continuación, vamos a describir cómo construir una red de neuronas mediante la utilización de **un conjunto de valores artificiales generados con el objetivo de\n",
    "explicar este tipo de técnica**. Más adelante volveremos a este tipo de red en el ámbito de clasificación de imágenes, donde los problemas multietiqueta son comunes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52b0bab-8dd4-4d08-bdde-168f5c17763a",
   "metadata": {},
   "source": [
    "### 2.6.1 - Preparación de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384681f8-930f-4c15-8327-ff298a8e7f8f",
   "metadata": {},
   "source": [
    "La primera fase de todo proceso de ML es la preparación de los datos. En este caso, vamos a construir un conjunto de datos artificial donde contaremos con 16 atributos por cada instancia y 5 posibles etiquetas. Cada una de las instancias de entrenamiento tendrá asignada, al menos, 2 etiquetas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d11aaf-b523-4cc6-a6d7-16e044deeb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "n_features = 16\n",
    "n_classes = 5\n",
    "\n",
    "data_x, data_y = make_multilabel_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=n_features,\n",
    "    n_classes=n_classes,\n",
    "    n_labels=2,\n",
    "    random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae322fb7-b77b-4729-a4dd-5be53503e761",
   "metadata": {},
   "source": [
    "Una vez realizada la carga de los datos, debemos llevar a cabo una serie de transformaciones y modificaciones en la información del conjunto de datos con el objetivo de adecuarlos y poder construir nuestra red neuronal:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2630357c-7faa-458e-8208-7ed613de776a",
   "metadata": {},
   "source": [
    "### 2.6.2 - Construcción de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9396d5-b14f-4ae5-a53a-d99d8b3effd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3054ef5b-1518-4f68-9433-761f2be7b988",
   "metadata": {},
   "source": [
    "### 2.6.3 - Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac55073-b68f-4d2f-b4fb-9de057538630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab61cb57-ee78-4c19-bcac-e3765c688224",
   "metadata": {},
   "source": [
    "### 2.6.4 - Inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fde672-0d62-4462-8b08-ded925920212",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b3c7f08-10c0-4f7e-83cf-4f27680940d2",
   "metadata": {},
   "source": [
    "## 2.7 - Optimización de parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e4c477-f5dd-468a-9517-78ce6123d47a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22d80990-1c23-4b6d-a706-9c7c014d27c5",
   "metadata": {},
   "source": [
    "### 2.7.1 - Preparación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6003c30b-4e97-4099-9223-02a2ea0b04cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "022910e8-c562-409a-a8f8-1af53a50164a",
   "metadata": {},
   "source": [
    "### 2.7.2 - Definición de hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54932d06-7858-4cc9-b9f2-57ae910b4e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73c0b3c7-9362-495e-9041-f62d688c2b49",
   "metadata": {},
   "source": [
    "### 2.7.3 - Definición del proceso de búsqueda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8904077-a8df-456e-b31c-9f7d972691d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5cb04942-3813-427c-a6e8-04532ebe1a98",
   "metadata": {},
   "source": [
    "### 2.7.4 - Ejecución del proceso de búsqueda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94524473-5db0-4ffe-b917-3f3bbe8d8a1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b95d86f-92c4-4b5e-ab69-47e291d79bec",
   "metadata": {},
   "source": [
    "### 2.7.5 - Selección del la configuración final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15afe89-9706-4903-a34e-7c36a1d72b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
