{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48ea1445-9262-4923-942b-f88674cbb3c4",
   "metadata": {},
   "source": [
    "# 2 - Aprendizaje por refuerzo\n",
    "\n",
    "**Sumario**\n",
    "\n",
    "1. Introducción\n",
    "2. El framewok de aprendizaje por refuerzo\n",
    "3. Métodos de aprendizaje por refuerzo basados en valor\n",
    "4. Q-Learning\n",
    "5. Construyendo un agente para FrozenLake\n",
    "6. Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc605d89-89c6-4ed6-ad6b-d2c879b72b94",
   "metadata": {},
   "source": [
    "## 2.1 - Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0f39ee-cac5-452a-9d89-65b6fb5f8bd4",
   "metadata": {},
   "source": [
    "Cuando reflexionamos sobre la palabra **aprendizaje**, es posible que lo primero que nos venga a la mente sea **la forma en la que los seres humanos aprendemos mediante la interacción con nuestro entorno**.\n",
    "\n",
    "Así, uno de los primeros ejemplos que seguramente visualicemos sea el proceso de aprendizaje que experimentan los niños y las niñas cuando empiezan a andar, donde interactúan con el entorno mediante un modelo de causa y efecto que voluciona con experimentación. De este modo, los seres humanos vamos adaptando poco a poco nuestro sistema motriz cada vez que se producen caidas (**\"penalizan\" el modelo**) o que conseguimos alcanzar una determinada localización en nuestro entorno (**\"refuerzan\"** el modelo).\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/humans.jpg\" width=\"500\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Dicho sistema de **penalización** y **refuerzo** se utilizó como inspiración para definir **el aprendizaje por refuerzo** (Sutton, 1998). El cual define un modelo de comportamiento que simula el proceso de interacción entre un agente y un entorno mediante la utilización de estímulos (refuerzos) que le indicasen cuales son las decisiones mas prometedoras para alcanzar un determinado objetivo.\n",
    "\n",
    "Comenzó con acermientos basados en **programación dinámica**, y luego fue evolucionando hacia los **algoritmos basados en tablas**, como *Q-Learning* o *SARSA*, los cuales construyen una tabla (normalmente llamada **tabla Q**) donde para cada estado, se incluía un valor de calidad para cada una de las posibles acciones que se podrian realizar en un determinado estado.\n",
    "\n",
    "Este tipo de acercamientos implicaban un gran **problema computacional** ya que hacia que muchos problemas resultasen intratables por la **combinación de estados y acciones que definía el tamaño de la tabla**. Este inconveniente se solventó con el surgimiento del aprendizaje profundo (*Deep Learning*), permitiendo que el aprendizaje por refuerzo pudiera resolver problemas antes inabordables, es decir, problemas donde el tamaño de la tabla Q seria demasiado grande.\n",
    "\n",
    "En el **aprendizaje por refuerzo profundo** (***Deep Reinforcement Learning***) utilizamos una red neuronal en vez de una tabla Q para dar un valor \"aproximado\" de la calidad de una acción para un determinado estado. Este tipo de acercamiento fue un gran avance en el área, permitiendo desarrollar modelos para tareas impensables hasta el momento, como videojuegos (e.g., [**Atari**](https://openai.com/research/gym-retro) o [**Starcraft II**](https://www.deepmind.com/blog/alphastar-grandmaster-level-in-starcraft-ii-using-multi-agent-reinforcement-learning)), [**el juego de Go**](https://www.youtube.com/watch?v=WXuK6gekU1Y) o [**el plegamiento de proteinas**](https://www.deepmind.com/research/highlighted-research/alphafold).\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/alphago.jpg\" width=\"500\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "A lo largo de esta unidad, describiremos los conceptos básicos asociados al aprendizaje por refuerzo y estudiaremos cómo este ha evolucionado desde los algoritmos tradicionales hasta las redes de neuronas por refuerzo, que utilizaremos, finalmente, para construir un agente que aprenda a jugar a un videojuego."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e9f807-cd18-4874-87eb-bedcdb149e3a",
   "metadata": {},
   "source": [
    "## 2.2 - El framework de aprendizaje por refuerzo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f5d4ce-9275-44e5-a392-fcb094d06ca0",
   "metadata": {},
   "source": [
    "La idea detrás del aprendizaje por refuerzo es que un **agente** (una IA) aprenderá del **entorno** interactuando con él (a través de prueba y error) y recibiendo **recompensas** (negativas o positivas) como retroalimentación por realizar acciones.\n",
    "\n",
    "Por ejemplo, imagina poner a tu hermano pequeño frente a un videojuego que nunca jugó, darle un controlador y dejarlo solo. Tu hermano podria interactuar con el \"entorno\" (el videojuego) pulsando los botones del mando. Si obtiene una moneda, recibirá un recompensa positivo (e.g., +1). En cambio, si toca un enemigo, recibirá una recompensa negativa (e.g., -1).\n",
    "\n",
    "Al interactuar con su entorno a través de prueba y error, tu hermano acabaría entendiendo que necesita recolectar monedas y evitar enemigos. Sin ninguna supervisión el niño mejoraría  cada vez más en el juego.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/RL_process_game.jpg\" width=\"500\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "* **Agente**. La entidad que interactúa con el entorno obteniendo información de éste y modificándolo  mediante acciones.\n",
    "* **Entorno**. La representación virtual del mundo con el que interactúa el agente.\n",
    "* **Estado**. La representación completa del entorno en un instante específico de tiempo, así como la representación del agente en dicho instante.\n",
    "* **Acción**. Aquella que ejecuta el agente ene el entorno con el fin de producir una variación en este.\n",
    "* **Recompensa o refuerzo**. Valor numérico obtenido tras la ejecución de una acción en el entorno. Se utiliza como medida para evaluar si la ejecución de la acción ha resultado positiva o negativa para el agente\n",
    "\n",
    "**Este proceso es un bucle, donde el objetivo del agente es maximizar la recompensa acumulada, que representa el retorno esperado**.\n",
    "\n",
    "Hay varios componentes/asunciones relevantes en el framework del aprendizaje por refuerzo (los veremos ahora uno por uno):\n",
    "* La hipótesis de recompensa.\n",
    "* La propiedad de Markov.\n",
    "* El grado de observabilidad del entorno (total o parcial).\n",
    "* El espacio de acción (finito o infinito).\n",
    "* Las recompensa y el factor de descuento.\n",
    "* *Tradeoff* entre exploración y explotación\n",
    "* Enfoque de aprendizaje (basado en política o en valor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f675d4-b891-4ba9-81e9-b881a1babed0",
   "metadata": {},
   "source": [
    "### 2.2.1 - La hipótesis de recompensa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879bb3a9-1418-4902-8d5a-8f6bfe67e738",
   "metadata": {},
   "source": [
    "El aprendizaje por refuerzo se basa en la **hipótesis de recompensa**, la cual indica que **cualquier meta puede describirse como la maximización del rendimiento esperado** (recompensa acumulada esperada).\n",
    "\n",
    "Es por eso que en el aprendizaje por refuerzo, para tener **el mejor comportamiento**, nuestro objetivo es aprender a **tomar acciones que maximicen la recompensa acumulada esperada**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbebc9b8-6830-45dd-b84e-ddbefa8349cb",
   "metadata": {},
   "source": [
    "### 2.2.2 - La propiedad de Markov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe08125-edc3-40a3-8ad7-70f7d61fa62d",
   "metadata": {},
   "source": [
    "El proceso de RL también se denomina **Proceso de decisión de Markov** (*Markov Decission Process*, *MDP* por sus siglas en inglés).\n",
    "\n",
    "La propiedad de Markov implica que **nuestro agente solo necesita el estado actual para decidir qué acción tomar** y no el historial de todos los estados y acciones que tomó antes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb637d72-0ed4-4c8b-a313-cefd927e7908",
   "metadata": {},
   "source": [
    "### 2.2.3 - El grado de observabilidad del entorno"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5e43b7-be91-4c4e-b2cb-d74b8f304361",
   "metadata": {
    "tags": []
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/obs_space.jpg\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "-----\n",
    "\n",
    "**Nota**: Ciertos autores hacen la distinción entre \"estado\" y \"observación\" para nombrar a $S_{t}$ en el bucle de aprendizaje por refuerzo.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ee373a-817e-4dfb-a38d-119031bc869a",
   "metadata": {},
   "source": [
    "### 2.2.4 - El espacio de acción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce803aa-98d6-4e3f-a871-20c00f469e5e",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/action_space.jpg\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657440f7-9176-4fd4-a0ac-33d14b0737b2",
   "metadata": {},
   "source": [
    "### 2.2.5 - La recompensa y el factor de descuento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3532338-139f-493a-a141-f7d3e058d78f",
   "metadata": {},
   "source": [
    "La recompensa es un elemento fundamental en el proceso de aprendizaje automático porque **es el único feedback que recibe nuestro agente**. Gracias a ella, sabe si esta acción ha sido **positiva o negativa**.\n",
    "\n",
    "La recompensa acumulada para un instante de tiempo $t$ se puede escribir como:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/rewards_1.png\" width=\"400\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Esto es equivalente a:\n",
    "\n",
    "$$\n",
    "R(\\tau) = \\sum_{k=0}^{\\infty} r_{t+k+1}\n",
    "$$\n",
    "\n",
    "Sin embargo, en realidad, no podemos simplemente agregar las recompensas así. Las recompensas que llegan antes (al comienzo del juego) **tienen más probabilidades de suceder**, ya que son más predecibles que las recompensas futuras a largo plazo.\n",
    "\n",
    "Digamos que tu agente es este pequeño ratón que puede mover una ficha en cada paso de tiempo, y tu oponente es el gato (que también puede moverse). **El objetivo del ratón es comer la máxima cantidad de queso antes de ser comido por el gato.**\n",
    "\n",
    "En consecuencia, la recompensa cerca del gato, aunque sea más grande (más queso), **estará más rebajada ya que no estamos muy seguros de poder comérnoslo**.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/rewards_3.jpg\" width=\"400\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Para descontar las recompensas, procedemos así:\n",
    "\n",
    "1. Definimos una tasa de descuento $\\gamma$. Debe estar entre 0 y 1. Normalmente suele tomar valores cercanos a 0.99 o 0.95.\n",
    "    * Cuanto mayor sea $\\gamma$, **menor será el descuento**. Esto implica que el agente se centra más la **recompensa futura**.\n",
    "    * cuanto menor sea $\\gamma$, **mayor será el duescuento**. Esto implica que el agente se centra más en la **recompensa cercana**.\n",
    "2. Luego, cada recompensa será descontada por $\\gamma$ elevada al instante de tiempo actual. A medida que aumenta el paso de tiempo, el gato se acerca a nosotros, por lo que la recompensa futura es cada vez menos probable.\n",
    "\n",
    "La recompensa acumulada (**descontada**) para un instante de tiempo $t$ se puede escribir como:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/rewards_2.png\" width=\"400\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "$$\n",
    "R(\\tau) = \\sum_{k=0}^{\\infty} \\gamma^{k} r_{t+k+1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43eb1ad-34c1-469e-b8e4-56d45ffbb353",
   "metadata": {},
   "source": [
    "### 2.2.6 - *Tradeoff* entre exploración y explotación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb40b144-aeb4-4562-9e72-54097898396f",
   "metadata": {},
   "source": [
    "Antes de describir los diferentes enfoques de aprendizaje, debemos entender un aspecto muy importante en los problemas de aprendizaje por refuerzo: **el *tradeoff* entre exploración y explotación**.\n",
    "\n",
    "Recordemos que el objetivo de nuestro agente es el de maximizar la recompensa acumulada. Sin embargo, **podemos caer en una trampa común**. Para entenderla, consideremos el siguiente juego donde nuestro ratón puede tener una cantidad *infinita* de queso (+1 por cada uno que recoge), pero en la parte superior del \"laberinto\" hay una cantidad gigante de queso (+1000).\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/exp_1.jpg\" width=\"400\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Si solo nos centramos en **recoger el queso más cercano sin explorar nuestro entorno**, nuestro agente nunca llegara a la suma gigante de queso. Sin embargo, **si realiza algo de exploración  (perdiendo algo de recompensa en el corto espacio de tiempo) podriamos conseguir una recompensa aún mayor**. Esto es lo que denominamos como el *tradeoff* entre exploración y explotación.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/expexpltradeoff.jpg\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dd5284-1243-4fce-acf2-590b08a7c02c",
   "metadata": {},
   "source": [
    "### 2.2.7 - Enfoque de aprendizaje"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aab5328-3e5a-43de-8e1c-e1aeb6bd5c46",
   "metadata": {},
   "source": [
    "**La política $\\pi$ es la que define el comportamiento del agente de tal forma que sus acciones maximicen la recompensa acumulada**. Podemos verla como el **\"cerebro\" del agente**.\n",
    "\n",
    "Nuestro objetivo es encontrar la política óptima $\\pi^{*}$, la cual **maximiza la el retorno esperado** (i.e., la recompensa acumulada) cuando el agente actúa de acuerdo con ella. **La política óptima $\\pi^{*}$ se encuentra mediante el entrenamiento.**\n",
    "\n",
    "Hay dos posibles acercamientos para entrenar a nuestro agente a encontrar la política óptima $\\pi^{*}$:\n",
    "* **Directamente**, enseñando al agente que acción debe tomar en cada estado: **Policy-based methods**.\n",
    "* **Indirectamente**, enseñando al agente que estado es más valioso, de tal forma que tome acciones que le lleven a estados valiosos: **Value-based methods**.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/two_approaches.png\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beece9f5-485a-4851-9179-43477497f9e6",
   "metadata": {},
   "source": [
    "## 2.3 - Métodos de aprendizaje por refuerzo basados en valor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942ef2e4-7eda-431c-9d99-c6c899286172",
   "metadata": {},
   "source": [
    "Aprendemos una función que asigna a cada estado el valor esperado de estar en él. Dicho valor es el rendimiento descontado esperado que el agente puede obtener si comienza en ese estado y luego actúa de acuerdo con nuestra política.\n",
    "\n",
    "-----\n",
    "\n",
    "<span style=\"color:red\"><b>Pregunta:</b></span> Pero, ¿que significa actuar con respecto a nuestra política? Al fin y al cabo, no tenemos una política en métodos basados en valor dado que lo que aprendemos es una función de valor, no una política en sí misma.\n",
    "\n",
    "<span style=\"color:blue\"><b>Respuesta</b></span> En este caso, la política es \"latente\", no definimos a mano el comportamiento de nuestra política; **es el entrenamiento el que indirectamente la definirá**. Ahora, dado que la política no está entrenada/aprendida, **necesitamos especificar su comportamiento**. Por ejemplo, si queremos una política que, dada la función de valor, realice acciones que siempre conduzcan a la mayor recompensa, crearemos una **política codiciosa**.\n",
    "\n",
    "-----\n",
    "\n",
    "Tenemos dos posibles acercamientos:\n",
    "* Métodos de **estado-valor**.\n",
    "* Métodos de **acción-estado-valor**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5863d2aa-95e2-429e-a8ce-5b4516f3e21a",
   "metadata": {},
   "source": [
    "### 2.3.1 - Métodos de estado-valor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382f373d-1715-45e1-8ea7-b057f2af7459",
   "metadata": {},
   "source": [
    "Para cada estado $s$, la función devuelve el retorno esperado si el agente empieza en ese estado y sigue la política los siguientes instantes de tiempo.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/state-value-function-2.png\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Escribimos la función de valor para un estado $s$ bajo una política $\\pi$ de la siguiente forma:\n",
    "\n",
    "<!-- $$\n",
    "V_{\\pi}(s) = \\mathbf{E}_{\\pi}[G_{t}[S_{t} = s]]\n",
    "$$\n",
    " -->\n",
    " \n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/state-value-function-1.png\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb5ada7-3b1c-4e6d-849f-eb7a133f20e2",
   "metadata": {},
   "source": [
    "### 2.3.2 - Métodos de acción-estado-valor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08533ed9-a638-414a-991d-ccf598d4f99b",
   "metadata": {},
   "source": [
    "En este caso, la función devuelve el valor para cada par de acción-estado. Es decir, el retorno esperado si el agente comenzase en ese estado, tomáse esa acción y siguiese su política hasta terminar el episodio.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/action-state-value-function-2.jpg\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "El valor de tomar la acción $a$ en el estado $s$ bajo la política $\\pi$ es:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/action-state-value-function-1.png\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760efa5a-4d01-4710-a254-8e1a37c56a52",
   "metadata": {},
   "source": [
    "### 2.3.3 - Ecuación de Bellman"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c4b07d-d56a-4f55-9427-e384882cdefe",
   "metadata": {},
   "source": [
    "El problema es que para calcular el valor de cada estado debemos **sumar todas las recompensas que el agente puede obtener si empieza en dicho estado**. Esto puede ser computacionalmente muy caro, especialmente si tenemos un gran número de estados posibles. \n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/bellman2.png\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Para remediarlo, existe la <span style=\"color:blue\"><b>ecuación de Bellman</b></span>.\n",
    "\n",
    "La ecuación de Bellman es una función recursiva inspirada en la [programación dinámica](https://es.wikipedia.org/wiki/Programaci%C3%B3n_din%C3%A1mica) que \"acumula\" la suma local de recompensas para cada estado de tal forma que podamos agilizar el proceso de cómputo. Consideramos cada estado como **la recompensa inmediata $R_{t+1}$ (es decir, la recompensa de pasar al siguiente estado) + el valor descontado del estado siguiente** ($\\gamma * V(S_{t+1})$).\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/bellman4.png\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ee1033-3d70-4ebb-9d70-a4f69564aedb",
   "metadata": {},
   "source": [
    "### 2.3.2 - *Monte Carlo* vs *Temporal Difference*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e59135-792b-4b08-a816-5cb9a2a23c9f",
   "metadata": {},
   "source": [
    "Existen dos estrategias con las que aprender nuestra función de valor:\n",
    "* ***Monte Carlo***. Utiliza la experiencia obtenida de todo un episodio antes de aprender.\n",
    "* ***Temporal Difference***. Utiliza la experiencia obtnida después de cada paso en el episodio para aprender."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7eb47e-4983-4828-baf3-d56300c5d304",
   "metadata": {},
   "source": [
    "#### *Monte Carlo*\n",
    "\n",
    "Monte Carlo espera hasta terminar el episodio, calcula el retorno $G_{t}$ obtenido para cada paso $t$ realizado y utiliza esta información como referencia para actualizar el valor de cada estado $V(S_{t})$:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/monte-carlo-approach.jpg\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Consideremos el juego del ratón y el gato presentado anteriormente como ejemplo:\n",
    "\n",
    "* Ponemos un límite al episodio. Por ejemplo:\n",
    "    * Si el gato se come al ratón\n",
    "    * Si el ratón realiza > 10 pasos\n",
    "* Siempre empezamos el episodio en el **mismo punto de inicio**.\n",
    "* **El agente realizará acciones con respecto a su política especificada**. Por ejemplo, puede usar una política codiciosa $\\epsilon$ (i.e., $\\epsilon$-*Greedy Strategy*), la cual alterna entre exploración y explotación de forma probabilística.\n",
    "* En cada instante de tiempo del episodio, obtenemos la **recompensa para ese estado** y cual es el **siguiente estado al que ir**.\n",
    "* Al final del episodio, **sumamos las recompensas que ha obtenido** el agente (para ver cuan bien lo ha hecho).\n",
    "* **Actualizamos el valor de cada estado** $V(S_{t})$ según la fórmula.\n",
    "* Comenzar un nuevo episodio con este nuevo conocimiento.\n",
    "\n",
    "Consideremos el siguiente episodio de ejemplo para ver como realizamos estos pasos:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/MC-4p.jpg\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Una vez terminado el episodio, tenemos una lista con los diferentes estados en los que ha estado el agente asi como las recompensas correspondientes. A partir de esto, podemos calcular el **retorno total** $G_{t}$ del episodio:\n",
    "\n",
    "* $G_{t} = R_{t+1} + R_{t+2} + R_{t+3} + \\dots$\n",
    "* $G_{t} = 1 + 0 + 0 + 0 + 0 + 0 + 1 + 1 + 0 + 0 = 3$\n",
    "\n",
    "Con esta información y si asumimos que la tasa de aprendizaje ($\\alpha$) es $0.1$ podemos el valor del estado $S_{0}$:\n",
    "\n",
    "* $V(S_{0})^{\\text{new}} = V_{S_{0}} + \\alpha * [G_{t} - V(S_{0})]$\n",
    "* $V(S_{0})^{\\text{new}} = 0 + 0.1 * [3 - 0] = 0.3$\n",
    "\n",
    "Podemos repetir este mismo procedimiento para el resto de estados $S_{1}$, $S_{2}$, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7f454e-c64d-47da-a39d-092100439592",
   "metadata": {},
   "source": [
    "#### *Temporal Difference*\n",
    "\n",
    "*Temporal Difference* (TD) simplemente espera una iteración (un instante de tiempo) $S_{t+1}$ para generar un **retorno parcial** y actualizar $V(S_{t})$ utilizando $R_{t+1}$ y ($\\gamma * V(S_{t+1})$).\n",
    "\n",
    "Dado que no hemos experimento todo el episodio, no tenemos $G_{t}$ (el retorno total). **El retorno parcial aproxima $G_{t}$ mediante la suma de la recomensa inmidiatamente posterior $R_{t+1}$ y el valor descontado del siguiente estado $\\gamma * V(S_{t+1})$**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/TD-1.jpg\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Si tomamos el mismo ejemplo que en el caso de *Monte Carlo* (juego del ratón y el gato):\n",
    "* Consideramos que es el comienzo del entrenamiento, asi que el valor de todo los estados es $0$.\n",
    "* Nuestra tasa de aprendizaje $\\alpha$ es $0.1$ y nuestra tasa de descuento $\\gamma$ es 1 (no se descuenta, recordemos que $0$ implica máximo descuento).\n",
    "* Nuestro ratón **decide explorar el entorno y toma una acción aleatoria**: e.g., va hacia la izquierda.\n",
    "* Recibe una recompensa $R_{t+1}$ ya que come una pieza de queso.\n",
    "\n",
    "Con esta información podemos actualizar el valor del estado $S_{0}$:\n",
    "* $V(S_{0})^{\\text{new}} = V(S_{0}) + \\alpha * [R_{t+1} + \\gamma * V(S_{1}) - V(S_{0})]$\n",
    "* $V(S_{0})^{\\text{new}} = 0 + 0.1 * [1 + 1 * 0 - 0] = 0.1$\n",
    "\n",
    "El agente continuaria actualizando su función de valor mientras interactúa con el entorno. En este caso con la versión actualizada de $V(S_{0})$. Al igual que con Monte Carlo, una vez termine el episodio, podemos repetirlo para seguir aprendiendo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0627fd-e2dd-4294-a57f-d912a327591b",
   "metadata": {},
   "source": [
    "## 2.4 - Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebb8280-32d2-43f6-b35a-c91511899b65",
   "metadata": {},
   "source": [
    "Q-learning es un metodo de aprendizaje por refuerzo basado en valor que utiliza una estrategia *Temporal Difference* para aprender su función de acción-estado-valor. **Q-learning es el algoritmo que utilizamos para aprender nuestra funcíon Q**, la cual es del tipo **acción-estado-valor** y determina el valor de tomar una determinada acción en un estado particular.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/Q-function.png\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Dado un estado y una acción, nuestra función Q devuelve un valor (llamado valor Q).\n",
    "\n",
    "**La \"Q\" se origina de la palabra \"Quality\", que indica la calidad de tomar dicha acción en ese estado**. Recordemos brevemente  la diferencia entre \"valor\" y \"recompensa\":\n",
    "* El **valor de un estado, o de un par estado-acción** es la recompensa acumulada esperada si nuestro agente empieza en dicho estado (o empieza con ese par estado-acción) y después actua siguiendo su política hasta el final.\n",
    "* La **recompensa** es el feedback que el agente recibe del entorno al realizar una acción en dicho estado.\n",
    "\n",
    "**Internamente, nuestra función Q tiene una tabla Q, donde cada celda corresponde con el valor de un par estado-acción.** <span style=\"color:blue\"><b>Podemos pensar en esta tabla como la \"memoria\" de nuestra función Q.</b></span>\n",
    "\n",
    "Tomemos el siguiente \"minijuego\" como ejemplo:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/Maze-3.png\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Como resumen, <span style=\"color:blue\">Q-learning</span> es un algoritmo de aprendizaje por refuerzo que:\n",
    "\n",
    "* Aprende una función Q (del tipo **acción-estado-valor**) que internamente es **una tabla con todos los pares estado-acción posibles**.\n",
    "* Dado un estado y una acción, nuestra función Q **busca en su tabla Q el valor correspondiente**.\n",
    "* **Cuando el entrenamiento ha terminado, tenemos una función Q óptima, lo que significa que tenemos una tabla Q óptima**.\n",
    "* Si tenemos una función Q óptima, **tenemos una política óptima porque sabemos para cada estado cual es la mejor acción a tomar**.\n",
    "\n",
    "Sin embargo, al principio del aprendizaje nuestra tabla Q es \"inútil\" porque contiene valores arbitrarios para cada par estado-acción (la mayor parte del tiempo inicializamos la tabla con 0s). **Según el agente va explorando el entorno y actulizamos la tabla Q, nos devolverá mejores aproximaciones de la política óptima**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/Q-learning-1.png\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6cac5b-bf91-49eb-8ff2-01dfd3e3e949",
   "metadata": {},
   "source": [
    "### 2.4.1 - Algoritmo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1736c4d-82fc-4d15-b80f-560f2fdd06a3",
   "metadata": {},
   "source": [
    "Ahora que tenemos una idea de cómo funciona el algoritmo Q-learning, podemos introducirnos más en los pasos específicos que lo componen:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/Q-learning-2.png\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "**Pseudocódigo:**\n",
    "\n",
    "```\n",
    "Initialize Q-table\n",
    "For episode in the total of training episodes:\n",
    "\n",
    "Reduce epsilon (since we need less and less exploration)\n",
    "Reset the environment\n",
    "\n",
    "  For step in max timesteps:    \n",
    "    Choose the action At using epsilon greedy policy\n",
    "    Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "    Update the Q-value Q(s,a) using Bellman equation Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "    If done, finish the episode\n",
    "    Our next state is the new state\n",
    "```\n",
    "\n",
    "#### Paso 1: Inicializar la tabla Q (generalmente con 0s)\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/Q-learning-3.png\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "#### Paso 2: Escoger una acción siguiendo la política codiciosa $\\epsilon$\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/Q-learning-4.png\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "La idea de esta política es que empecemos con un valor inicial de $\\epsilon = 1.0$, de tal forma que:\n",
    "* Con probabilidad $1 - \\epsilon$: **explotamos el entorno** (i.e., nuestro agente selecciona la accion con el máximo valor)\n",
    "* con probabilibilidad $\\epsilon$: **exploramos el entorno** (i.e., nuestro agente realiza una acción al azar)\n",
    "\n",
    "Al comienzo del entrenamiento la probabilidad de que el agente explore es muy alta dado que **$\\epsilon$ tiene un valor muy elevado**. Sin embargo, según avance el entrenamiento y nuestra tabla Q se vuelva mejor y mejor en sus estimaciones, **iremos reduciendo el valor $\\epsilon$** de tal manera que necesitaremos menos exploración y más explotación.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/Q-learning-5.png\" width=\"250\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "#### Paso 3: Realizar una acción $A_{t}$, obtener recompensa $R_{t+1}$ y el siguiente estado $S_{t+1}$\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/Q-learning-6.png\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "#### Paso 4: Actualizar $Q(S_{t}, A_{t})$\n",
    "\n",
    "Recordemos que según la estrategia de *Temporal Difference*, actualizamos nuestra función de valor tras cada interacción que haga el agente con el entorno. Para Q-learning la fórmula toma la siguiente forma:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/Q-learning-8.png\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Esto implica que para actualizar nuestra función $Q(S_{t}, A_{t})$ necesitamos:\n",
    "* Las variables $S_{t}$, $A_{t}$, $R_{t+1}$, $S_{t+1}$\n",
    "* El TD-target\n",
    "\n",
    "**¿Cómo obtenemos el TD-target?**\n",
    "\n",
    "1. Obtenemos la recompensa inmediata $R_{t+1}$ de tomar la acción $A_{t}$.\n",
    "2. Para obtener el siguiente par de estado-acción, utilizamos una política codiciosa. Nótese que no es la política codiciosa $\\epsilon$. En este caso seleccionamos directamente aquella accion con el valor Q más alto.\n",
    "\n",
    "Si nos fijamos, **el algoritmo Q-learning utiliza una política distinta al actuar (inferencia) y al actualizar el valor Q (aprendizaje)**. A este comportamiento se le denaomina ***off-policy***. En el caso de que fuera *on-policy* tendria que utilizar la política codiciosa $\\epsilon$ tanto al actual como al actualizar el valor Q. [Este es el caso de un algoritmo llamado SARSA, muy similar a Q-learning](https://en.wikipedia.org/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action).\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/off-on-4.png\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae36bf1-e4ab-4f32-8f3c-df4ed18c548a",
   "metadata": {},
   "source": [
    "### 2.4.3 - Ejemplo de ejecución"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd92ad5-39c7-49ca-88aa-3a54ae1885b8",
   "metadata": {},
   "source": [
    "Para comprender mejor el funcionamiento del algoritmo Q-learning, consideremos el siguiente ejemplo.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/q-ex-1.jpg\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Las recompensas del \"juego\" son:\n",
    "* +0: ir a un estado que no tiene queso\n",
    "* +1: ir a un estado con queso\n",
    "* +10: ir a un estado con una gran pila de queso\n",
    "* -10: ir al estado con veneno\n",
    "\n",
    "Vamos a presentar un ejemplo con 3 instantes de tiempo (i.e., $t=0$, $t=1$, $t=2$)\n",
    "\n",
    "#### $t=0$:\n",
    "\n",
    "Solo podemos ejecutar el paso 1 del algoritmo (i.e., inicializar)\n",
    "\n",
    "##### **Paso 1: Inicializar tabla Q**\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/Example-1.png\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "#### $t=1$:\n",
    "\n",
    "Ejecutamos los pasos 2, 3 y 4 del algoritmo\n",
    "\n",
    "##### **Paso 2: Escoger una acción mediante la estrategia codiciosa $\\epsilon$**\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/q-ex-3.jpg\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "##### **Paso 3: Realizar acción $A_{t}$, obtener $R_{t+1}$ y $S_{t+1}$\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/q-ex-3.jpg\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "El ratón va a la derecha, obtiene queso (i.e., $R_{t+1} = 1$) y se encuentra en un nuevo estado.\n",
    "\n",
    "##### **Paso 4: Actualizar $Q(S_{t}, A_{t})$**\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/q-ex-5.jpg\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/Example-4.png\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "#### $t=2$:\n",
    "\n",
    "Ejecutamos los pasos 2, 3 y 4 del algoritmo\n",
    "\n",
    "##### **Paso 2: Escoger una acción mediante la estrategia codiciosa $\\epsilon$**\n",
    "\n",
    "Dado que epsilon es todavia muy alto (i.e., $0.99$), el ratón toma una acción aleatoria de nuevo, la cual le lleva al veneno y por tanto terminaria el episodio.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/q-ex-6.jpg\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "##### **Paso 3: Realizar acción $A_{t}$, obtener $R_{t+1}$ y $S_{t+1}$\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/q-ex-7.jpg\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "##### **Paso 4: Actualizar $Q(S_{t}, A_{t})$**\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/q-ex-8.jpg\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "En este momento, terminaria el episodio, pero el entrenamiento no acaba ahi. Empezariamos uno nuevo con la información aprendida de tal forma que el agente es más inteligente ahora que al principio de nuestro entrenamiento (sabe que el queso es bueno y que el veneno no lo se). **Según vayamos explroando y explotando el entorno, la tabla Q nos irá dando mejores y mejores aproximaciones de la política óptima.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cad0293-f19a-4a52-9aac-f14dd4508b66",
   "metadata": {},
   "source": [
    "## 2.5 - Construyendo un agente para FrozenLake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48249a6f-fba5-4cac-9489-b5043757b89a",
   "metadata": {},
   "source": [
    "Vamos a entrenar nuestro agente con el algoritmo Q-learning para que aprenda a navegar por el entorno de Frozen Lake.\n",
    "\n",
    "* <a href=\"https://gymnasium.farama.org/environments/toy_text/frozen_lake/\"><span style=\"color:blue\"><b>Documentación sobre el entorno <i>FrozenLake</i></b></span></a>\n",
    "* <a href=\"https://gymnasium.farama.org/tutorials/training_agents/FrozenLake_tuto/\"><span style=\"color:blue\"><b>Versión extendida del ejemplo</b></span></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9c8274-d818-48ae-99ef-a12b9663316f",
   "metadata": {},
   "source": [
    "### 2.5.1 - Importación de librerías"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d068f8-548e-4c89-9bf2-ff581ff59d25",
   "metadata": {},
   "source": [
    "A la hora de construir nuestro agente, es necesario instalar una serie de librerías que nos permitan jugar al\n",
    "videojuego y obtener la información referente al entorno (imagen y puntuación), así como aplicar acciones sobre\n",
    "él. En este sentido, utilizaremos el instalador de paquetes de Python para poder ejecutar las siguientes librerías:\n",
    "\n",
    "```\n",
    "pip install gymnasium\n",
    "pip install imageio\n",
    "pip install imageio_ffmpeg\n",
    "pip install gymnasium[toy-text]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6096977-7c4e-463b-b249-7e170b0e4d5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import imageio\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9165480b-4fe4-4ede-a746-835d43406d23",
   "metadata": {},
   "source": [
    "### 2.5.2 - Crear el entorno"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d0f5fa-0b7f-42a5-bccf-7da6a8e46b60",
   "metadata": {},
   "source": [
    "El entorno cuenta con 4 tipo de estados:\n",
    "* **S**: el estado inicial.\n",
    "* **G**: el estado objetivo.\n",
    "* **F**: estados por las que se puede andar.\n",
    "* **H**: estados \"agujero\" que se deberian evitar.\n",
    "\n",
    "El tamaño del mapa viene dado por su nombre:\n",
    "* `map_name=\"4x4\"`: un mapa de tamaño 4x4.\n",
    "* `map_name=\"8x8\"`: un mapa de tamaño 8x8.\n",
    "\n",
    "El entorno tiene dos modos:\n",
    "* `is_slippery=False`. El agente se mueve siempre en la dirección deseada (deterministico).\n",
    "* `is_slippery=True`. El agente no siempre se mueve en la dirección deseada data la naturaleza resbaladiza del suelo (estocástico)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b2bf16d-6087-4386-b8d4-215bd055258f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f31e61-0339-4cc5-b312-19e4c493f1ec",
   "metadata": {},
   "source": [
    "Podriamos crear nuestro propio mapa de la siguiente forma (aunque vamos a usar el mapa por defecto):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ae29e8d-2b2b-49e1-852c-a92cdc6a2916",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<OrderEnforcing<PassiveEnvChecker<FrozenLakeEnv<FrozenLake-v1>>>>>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"]\n",
    "gym.make('FrozenLake-v1', desc=desc, is_slippery=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab010cf3-877a-49f6-9297-8e462664e5cc",
   "metadata": {},
   "source": [
    "El mapa de 4x4 tiene 16 posibles observaciones que van del 0 al 15, siendo el 0 el estado de inicio y el 15 el estado objetivo.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/frozenlake_map.png\" width=\"300\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2583f8de-fdaa-4bbd-83af-bbbef514b4b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____OBSERVATION SPACE_____ \n",
      "\n",
      "Observation Space Discrete(16)\n",
      "Sample observation 1\n"
     ]
    }
   ],
   "source": [
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(\"Observation Space\", env.observation_space)\n",
    "print(\"Sample observation\", env.observation_space.sample()) # Obtenemos una observación aleatoria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41909594-cbb2-42f6-9890-87899a53ec43",
   "metadata": {},
   "source": [
    "El espacio de acciones que puede tomar el agente es el siguiente:\n",
    "* **0**: Izquierda\n",
    "* **1**: Abajo\n",
    "* **2**: Derecha\n",
    "* **3**: Arriba\n",
    "\n",
    "La función de recompensa es la siguiente:\n",
    "* **Estado objetivo (G)**: +1\n",
    "* **Estado agujero (H)**: 0\n",
    "* **Estado congelado (F)**: 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032b7831-d13e-4081-a782-2f50ad495634",
   "metadata": {},
   "source": [
    "### 2.5.3 - Crear e inicializar la tabla Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0bb9124-a788-4063-952b-653ab432a5ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  16  possible states\n",
      "There are  4  possible actions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_space = env.observation_space.n\n",
    "print(\"There are \", state_space, \" possible states\")\n",
    "\n",
    "action_space = env.action_space.n\n",
    "print(\"There are \", action_space, \" possible actions\")\n",
    "\n",
    "# Let's create our Qtable of size (state_space, action_space) and initialized each values at 0 using np.zeros\n",
    "def initialize_q_table(state_space, action_space):\n",
    "    Qtable = np.zeros((state_space, action_space))\n",
    "    return Qtable\n",
    "\n",
    "Qtable_frozenlake = initialize_q_table(state_space, action_space)\n",
    "Qtable_frozenlake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765883a6-f62b-4875-878f-811e514b3429",
   "metadata": {},
   "source": [
    "### 2.5.4 - Definir nuestras políticas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df60800-268e-4aac-9221-ddab96b276d2",
   "metadata": {},
   "source": [
    "Recordemos que en Q-learning utilizamos políticas diferentes para **actuar** y para **actualizar nuestra función de valor**:\n",
    "* **Actuar**: politica codiciosa $\\epsilon$\n",
    "* **Actualizar**: política codiciosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f561b341-b7f7-494d-a07c-51d00caf1dad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Política codiciosa para actuar\n",
    "def greedy_policy(Qtable, state):\n",
    "  # Explotación: tomamos la acción que nos lleva al estado de más valor\n",
    "  action = np.argmax(Qtable[state][:])\n",
    "  \n",
    "  return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "725a8235-2dfd-4ed0-a687-7bd1aa18bee6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Política codiciosa epsilon para actualizar la función de valor\n",
    "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
    "  # Generamos un número aleatorio entre 0 y 1\n",
    "  random_int = random.uniform(0,1)\n",
    "  # Si el número > epsilon --> exploitación (acción que nos lleva al estado de más valor)\n",
    "  if random_int > epsilon:\n",
    "    action = greedy_policy(Qtable, state)\n",
    "  # En caso contrario --> exploración (acción aleatoria)\n",
    "  else:\n",
    "    action = env.action_space.sample()\n",
    "  \n",
    "  return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b4608c-0961-41cc-a7f6-e01e0423feb0",
   "metadata": {},
   "source": [
    "### 2.5.5 - Definir hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0503080-8f6a-4e85-a62f-058e3723d744",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "n_training_episodes = 10000  # Número total de episodios de entrenamiento\n",
    "learning_rate = 0.7          # Ratio de aprendizaje\n",
    "\n",
    "# Evaluation parameters\n",
    "n_eval_episodes = 100        # Número total de episodios de test\n",
    "\n",
    "# Environment parameters\n",
    "env_id = \"FrozenLake-v1\"     # Nombre del entorno\n",
    "max_steps = 99               # Número máximo de pasos por episodio\n",
    "gamma = 0.95                 # Ratio de descuento para la función de valor\n",
    "eval_seed = []               # Semilla para el entorno de evaluación\n",
    "\n",
    "# Exploration parameters\n",
    "max_epsilon = 1.0             # Probabilidad de exploración al inicio\n",
    "min_epsilon = 0.05            # Probabilidad mínima de exploración\n",
    "decay_rate = 0.0005           # Tasa de decaimiento exponencial para la exploración"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04adc92c-3923-49a5-9569-c64d1e5fa4df",
   "metadata": {},
   "source": [
    "### 2.5.6 - Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0454e3e5-7c44-4671-a35e-0b5bb52bd293",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
    "    \n",
    "    for episode in tqdm(range(n_training_episodes)):\n",
    "        # Reducimos epsilon (porque cada vez necesitamos menos exploración)\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "        # Reseteamos el entorno para que el agente parta de 0\n",
    "        state, info = env.reset()\n",
    "        step = 0\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        # Repetimos para cada paso t\n",
    "        for step in range(max_steps):\n",
    "            # Escogemos una acción A_t mediante la política codiciosa epsilon\n",
    "            action = epsilon_greedy_policy(Qtable, state, epsilon)\n",
    "\n",
    "            # Tomamos la acción A_t y observamos R_t+1 y S_t+1\n",
    "            new_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            # Actualizamos la función de valor Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "            Qtable[state][action] = Qtable[state][action] + learning_rate * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action])   \n",
    "\n",
    "            # Si terminated == True o truncated == True, terminamos el episodio\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "                \n",
    "            # \"Movemos\" el agente, actualizando su estado\n",
    "            state = new_state\n",
    "            \n",
    "    return Qtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "318b02a6-e1dc-4fd1-9ff7-ddc84bfdf5fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32f5f00814d84d51823c96e12c4a9e51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Entrenamos nuestro agente con 10000 episodios\n",
    "Qtable_frozenlake = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_frozenlake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebdf9752-ffd0-4b37-9df5-92f17cf748fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.73509189, 0.77378094, 0.77378094, 0.73509189],\n",
       "       [0.73509189, 0.        , 0.81450625, 0.77378094],\n",
       "       [0.77378094, 0.857375  , 0.77378094, 0.81450625],\n",
       "       [0.81450625, 0.        , 0.77378094, 0.77378094],\n",
       "       [0.77378094, 0.81450625, 0.        , 0.73509189],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.9025    , 0.        , 0.81450625],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.81450625, 0.        , 0.857375  , 0.77378094],\n",
       "       [0.81450625, 0.9025    , 0.9025    , 0.        ],\n",
       "       [0.857375  , 0.95      , 0.        , 0.857375  ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.9025    , 0.95      , 0.857375  ],\n",
       "       [0.9025    , 0.95      , 1.        , 0.9025    ],\n",
       "       [0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qtable_frozenlake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23d2180-a73b-454c-9d1a-57799f0e6f36",
   "metadata": {},
   "source": [
    "### 2.5.7 - Evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a4c13f-04c5-4905-8421-7e85eca4e532",
   "metadata": {},
   "source": [
    "Evaluación muy sencilla para comprobar que el agente ha aprendido a llegar correctamente al estado objetivo en el entorno que le hemos preparado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e4dca0c-80d0-41e8-b0d1-78bf082d30be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_agent(env, max_steps, n_eval_episodes, Q, seed):\n",
    "    \n",
    "    episode_rewards = []\n",
    "    for episode in tqdm(range(n_eval_episodes)):\n",
    "        if seed:\n",
    "            state, info = env.reset(seed=seed[episode])\n",
    "        else:\n",
    "            state, info = env.reset()\n",
    "        step = 0\n",
    "        truncated = False\n",
    "        terminated = False\n",
    "        total_rewards_ep = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # Tomamos la acción (indice) que tiene la maxima recompensa futura dado ese estado\n",
    "            action = greedy_policy(Q, state)\n",
    "            new_state, reward, terminated, truncated, info = env.step(action)\n",
    "            total_rewards_ep += reward\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "            state = new_state\n",
    "        episode_rewards.append(total_rewards_ep)\n",
    "        \n",
    "    mean_reward = np.mean(episode_rewards)\n",
    "    std_reward = np.std(episode_rewards)\n",
    "\n",
    "    return mean_reward, std_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "146d52d0-e4ab-4727-86da-0d3116432c29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b1526be4701433ca3cedc9c25b2e6d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean_reward=1.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "# Evaluamos nuestro agente con 100 episodios de prueba\n",
    "mean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed)\n",
    "print(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ffd42d-9f0a-47f9-8d98-527182349b3a",
   "metadata": {},
   "source": [
    "## 2.6 - Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c77457-5e20-45fe-859d-973b3856f988",
   "metadata": {},
   "source": [
    "Hemos visto que Q-Learning es un algoritmo para entrenar la función Q, una función que determina el valor de estar en un estado particular y realizar una acción específica en ese estado. \n",
    "\n",
    "El problema es que internamente **esta función es una tabla** lo cual hace que el método **no sea escalable**. Por ejemplo, si queremos aprender un agente que sepa jugar a videojuegos de Atari. En ese caso el número de estados sería demasiado grande para el algoritmo tradicional de Q-learning. \n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/atari-envs.gif\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "En el caso de un videojuego de Atari, podemos hacer que el agente **tome la información directamente de los frames de pantalla**. En ese caso, si tenemos imagenes RGB de 210x160 pixeles (donde cada pixel contiene un valor que va de 0 a 255), contariamos con un espacio de estados posibles de $256^{210 \\times 160 \\times 3} = 256^{100800}$.\n",
    "\n",
    "Por lo tanto, crear y actualizar una tabla Q no sería eficiente para este problema. Es necesario aproximar los valores Q mediante una función parametrizada $Q_{\\theta}(s,a)$. Esta función parametrizada sería la red neuronal.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/deep_q_learning.jpg\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Este tipo de redes de neuronas se denominan redes de neuronas profundas de aprendizaje por refuerzo (*Reinforcement Learning Deep NN*) o redes de neuronas profundas de tipo Q (*Deep Q Neural Network*, *DQNN* por sus siglas en inglés).\n",
    "\n",
    "La primera implementación fue desarrollada por los ingenieros de la empresa Deep Mind Technologies, donde demostraron cómo una máquina era capaz de aprender a jugar a los videojuegos de una Atari 2600 mediante el **análisis de los píxeles de las imágenes que representaban el estado del juego**. Uno de los elementos más destacables de esta implementación es que había sido entrenada para jugar a **siete videojuegos diferentes que utilizaban la misma interfaz de control** (muestra de \"generalización\").\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/deep-q-network.jpg\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d258f9-4e16-44f1-bfbc-5745ba8ec8c5",
   "metadata": {},
   "source": [
    "### 2.6.1 - Preprocesamiento para RL con imágenes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12016df-dd66-4aef-9c2d-6851e46ecda0",
   "metadata": {},
   "source": [
    "Cuando pasamos las imágenes a nuestra red para entrenar el modelo, necesitamos realizar dos preprocesamientos:\n",
    "* **Reducir la dimensionalidad** (i.e., número de pixeles) para reducir la complejidad de computación\n",
    "* **Agrupar varios frames para eliminar la limitación temporal** (i.e., *temporal limitation*)\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/preprocessing.jpg\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "**El problema de la limitación temporal aprece cuando aprendemos a partir de información visual**. Ee observa en el hecho de que con un único frame no sabemos como va a evolucionar el entorno.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/temporal-limitation.jpg\" width=\"600\" data-align=\"center\"></td>\n",
    "        <td><img src=\"images_2/temporal-limitation-2.jpg\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
