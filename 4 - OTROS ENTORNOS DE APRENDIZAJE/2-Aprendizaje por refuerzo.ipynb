{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48ea1445-9262-4923-942b-f88674cbb3c4",
   "metadata": {},
   "source": [
    "# 2 - Redes recurrentes orientadas al procesamiento de lenguaje natural\n",
    "\n",
    "**Sumario**\n",
    "\n",
    "1. Introducción\n",
    "2. El framewok de aprendizaje por refuerzo\n",
    "3. Deep Reinforcement Learning\n",
    "4. Construyendo un agente mediante una red DQNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc605d89-89c6-4ed6-ad6b-d2c879b72b94",
   "metadata": {},
   "source": [
    "## 2.1 - Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0f39ee-cac5-452a-9d89-65b6fb5f8bd4",
   "metadata": {},
   "source": [
    "Cuando reflexionamos sobre la palabra **aprendizaje**, es posible que lo primero que nos venga a la mente sea **la forma en la que los seres humanos aprendemos mediante la interacción con nuestro entorno**.\n",
    "\n",
    "Así, uno de los primeros ejemplos que seguramente visualicemos sea el proceso de aprendizaje que experimentan los niños y las niñas cuando empiezan a andar, donde interactúan con el entorno mediante un modelo de causa y efecto que voluciona con experimentación. De este modo, los seres humanos vamos adaptando poco a poco nuestro sistema motriz cada vez que se producen caidas (**\"penalizan\" el modelo**) o que conseguimos alcanzar una determinada localización en nuestro entorno (**\"refuerzan\"** el modelo).\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/humans.jpg\" width=\"500\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Dicho sistema de **penalización** y **refuerzo** se utilizó como inspiración para definir **el aprendizaje por refuerzo** (Sutton, 1998). El cual define un modelo de comportamiento que simula el proceso de interacción entre un agente y un entorno mediante la utilización de estímulos (refuerzos) que le indicasen cuales son las decisiones mas prometedoras para alcanzar un determinado objetivo.\n",
    "\n",
    "Comenzó con acermientos basados en **programación dinámica**, y luego fue evolucionando hacia los **algoritmos basados en tablas**, como *Q-Learning* o *SARSA*, los cuales construyen una tabla (normalmente llamada **tabla Q**) donde para cada estado, se incluía un valor de calidad para cada una de las posibles acciones que se podrian realizar en un determinado estado.\n",
    "\n",
    "Este tipo de acercamientos implicaban un gran **problema computacional** ya que hacia que muchos problemas resultasen intratables por la **combinación de estados y acciones que definía el tamaño de la tabla**. Este inconveniente se solventó con el surgimiento del aprendizaje profundo (*Deep Learning*), permitiendo que el aprendizaje por refuerzo pudiera resolver problemas antes inabordables, es decir, problemas donde el tamaño de la tabla Q seria demasiado grande.\n",
    "\n",
    "En el **aprendizaje por refuerzo profundo** (***Deep Reinforcement Learning***) utilizamos una red neuronal en vez de una tabla Q para dar un valor \"aproximado\" de la calidad de una acción para un determinado estado. Este tipo de acercamiento fue un gran avance en el área, permitiendo desarrollar modelos para tareas impensables hasta el momento, como videojuegos (e.g., [**Atari**](https://openai.com/research/gym-retro) o [**Starcraft II**](https://www.deepmind.com/blog/alphastar-grandmaster-level-in-starcraft-ii-using-multi-agent-reinforcement-learning)), [**el juego de Go**](https://www.youtube.com/watch?v=WXuK6gekU1Y) o [**el plegamiento de proteinas**](https://www.deepmind.com/research/highlighted-research/alphafold).\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/alphago.jpg\" width=\"500\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "A lo largo de esta unidad, describiremos los conceptos básicos asociados al aprendizaje por refuerzo y estudiaremos cómo este ha evolucionado desde los algoritmos tradicionales hasta las redes de neuronas por refuerzo, que utilizaremos, finalmente, para construir un agente que aprenda a jugar a un videojuego."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e9f807-cd18-4874-87eb-bedcdb149e3a",
   "metadata": {},
   "source": [
    "## 2.2 - El framework de aprendizaje por refuerzo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f5d4ce-9275-44e5-a392-fcb094d06ca0",
   "metadata": {},
   "source": [
    "La idea detrás del aprendizaje por refuerzo es que un **agente** (una IA) aprenderá del **entorno** interactuando con él (a través de prueba y error) y recibiendo **recompensas** (negativas o positivas) como retroalimentación por realizar acciones.\n",
    "\n",
    "Por ejemplo, imagina poner a tu hermano pequeño frente a un videojuego que nunca jugó, darle un controlador y dejarlo solo. Tu hermano podria interactuar con el \"entorno\" (el videojuego) pulsando los botones del mando. Si obtiene una moneda, recibirá un recompensa positivo (e.g., +1). En cambio, si toca un enemigo, recibirá una recompensa negativa (e.g., -1).\n",
    "\n",
    "Al interactuar con su entorno a través de prueba y error, tu hermano acabaría entendiendo que necesita recolectar monedas y evitar enemigos. Sin ninguna supervisión el niño mejoraría  cada vez más en el juego.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/RL_process_game.jpg\" width=\"500\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "* **Agente**. La entidad que interactúa con el entorno obteniendo información de éste y modificándolo  mediante acciones.\n",
    "* **Entorno**. La representación virtual del mundo con el que interactúa el agente.\n",
    "* **Estado**. La representación completa del entorno en un instante específico de tiempo, así como la representación del agente en dicho instante.\n",
    "* **Acción**. Aquella que ejecuta el agente ene el entorno con el fin de producir una variación en este.\n",
    "* **Recompensa o refuerzo**. Valor numérico obtenido tras la ejecución de una acción en el entorno. Se utiliza como medida para evaluar si la ejecución de la acción ha resultado positiva o negativa para el agente\n",
    "\n",
    "**Este proceso es un bucle, donde el objetivo del agente es maximizar la recompensa acumulada, que representa el retorno esperado**.\n",
    "\n",
    "Hay varios componentes/asunciones relevantes en el framework del aprendizaje por refuerzo (los veremos ahora uno por uno):\n",
    "* La hipótesis de recompensa.\n",
    "* La propiedad de Markov.\n",
    "* El grado de observabilidad del entorno (total o parcial).\n",
    "* El espacio de acción (finito o infinito).\n",
    "* Las recompensa y el factor de descuento.\n",
    "* Enfoque de aprendizaje (basado en política o en valor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f675d4-b891-4ba9-81e9-b881a1babed0",
   "metadata": {},
   "source": [
    "### 2.2.1 - La hipótesis de recompensa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879bb3a9-1418-4902-8d5a-8f6bfe67e738",
   "metadata": {},
   "source": [
    "El aprendizaje por refuerzo se basa en la **hipótesis de recompensa**, la cual indica que **cualquier meta puede describirse como la maximización del rendimiento esperado** (recompensa acumulada esperada).\n",
    "\n",
    "Es por eso que en el aprendizaje por refuerzo, para tener **el mejor comportamiento**, nuestro objetivo es aprender a **tomar acciones que maximicen la recompensa acumulada esperada**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbebc9b8-6830-45dd-b84e-ddbefa8349cb",
   "metadata": {},
   "source": [
    "### 2.2.2 - La propiedad de Markov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe08125-edc3-40a3-8ad7-70f7d61fa62d",
   "metadata": {},
   "source": [
    "El proceso de RL también se denomina **Proceso de decisión de Markov** (*Markov Decission Process*, *MDP* por sus siglas en inglés).\n",
    "\n",
    "La propiedad de Markov implica que **nuestro agente solo necesita el estado actual para decidir qué acción tomar** y no el historial de todos los estados y acciones que tomó antes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb637d72-0ed4-4c8b-a313-cefd927e7908",
   "metadata": {},
   "source": [
    "### 2.2.3 - El grado de observabilidad del entorno"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5e43b7-be91-4c4e-b2cb-d74b8f304361",
   "metadata": {
    "tags": []
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/obs_space.jpg\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "-----\n",
    "\n",
    "**Nota**: Ciertos autores hacen la distinción entre \"estado\" y \"observación\" para nombrar a $S_{t}$ en el bucle de aprendizaje por refuerzo.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ee373a-817e-4dfb-a38d-119031bc869a",
   "metadata": {},
   "source": [
    "### 2.2.4 - El espacio de acción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce803aa-98d6-4e3f-a871-20c00f469e5e",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/action_space.jpg\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657440f7-9176-4fd4-a0ac-33d14b0737b2",
   "metadata": {},
   "source": [
    "### 2.2.5 - La recompensa y el factor de descuento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3532338-139f-493a-a141-f7d3e058d78f",
   "metadata": {},
   "source": [
    "La recompensa es un elemento fundamental en el proceso de aprendizaje automático porque **es el único feedback que recibe nuestro agente**. Gracias a ella, sabe si esta acción ha sido **positiva o negativa**.\n",
    "\n",
    "La recompensa acumulada para un instante de tiempo $t$ se puede escribir como:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/rewards_1.png\" width=\"400\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Esto es equivalente a:\n",
    "\n",
    "$$\n",
    "R(\\tau) = \\sum_{k=0}^{\\infty} r_{t+k+1}\n",
    "$$\n",
    "\n",
    "Sin embargo, en realidad, no podemos simplemente agregar las recompensas así. Las recompensas que llegan antes (al comienzo del juego) **tienen más probabilidades de suceder**, ya que son más predecibles que las recompensas futuras a largo plazo.\n",
    "\n",
    "Digamos que tu agente es este pequeño ratón que puede mover una ficha en cada paso de tiempo, y tu oponente es el gato (que también puede moverse). **El objetivo del ratón es comer la máxima cantidad de queso antes de ser comido por el gato.**\n",
    "\n",
    "En consecuencia, la recompensa cerca del gato, aunque sea más grande (más queso), **estará más rebajada ya que no estamos muy seguros de poder comérnoslo**.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/rewards_3.jpg\" width=\"400\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Para descontar las recompensas, procedemos así:\n",
    "\n",
    "1. Definimos una tasa de descuento $\\gamma$. Debe estar entre 0 y 1. Normalmente suele tomar valores cercanos a 0.99 o 0.95.\n",
    "    * Cuanto mayor sea $\\gamma$, **menor será el descuento**. Esto implica que el agente se centra más la **recompensa futura**.\n",
    "    * cuanto menor sea $\\gamma$, **mayor será el duescuento**. Esto implica que el agente se centra más en la **recompensa cercana**.\n",
    "2. Luego, cada recompensa será descontada por $\\gamma$ elevada al instante de tiempo actual. A medida que aumenta el paso de tiempo, el gato se acerca a nosotros, por lo que la recompensa futura es cada vez menos probable.\n",
    "\n",
    "La recompensa acumulada (**descontada**) para un instante de tiempo $t$ se puede escribir como:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/rewards_2.png\" width=\"400\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "$$\n",
    "R(\\tau) = \\sum_{k=0}^{\\infty} \\gamma^{k} r_{t+k+1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dd5284-1243-4fce-acf2-590b08a7c02c",
   "metadata": {},
   "source": [
    "### 2.2.6 - Enfoque de aprendizaje"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aab5328-3e5a-43de-8e1c-e1aeb6bd5c46",
   "metadata": {},
   "source": [
    "**La política $\\pi$ es la que define el comportamiento del agente de tal forma que sus acciones maximicen la recompensa acumulada**. Podemos verla como el **\"cerebro\" del agente**.\n",
    "\n",
    "Nuestro objetivo es encontrar la política óptima $\\pi^{*}$, la cual **maximiza la el retorno esperado** (i.e., la recompensa acumulada) cuando el agente actúa de acuerdo con ella. **La política óptima $\\pi^{*}$ se encuentra mediante el entrenamiento.**\n",
    "\n",
    "Hay dos posibles acercamientos para entrenar a nuestro agente a encontrar la política óptima $\\pi^{*}$:\n",
    "* **Directamente**, enseñando al agente que acción debe tomar en cada estado: **Policy-based methods**.\n",
    "* **Indirectamente**, enseñando al agente que estado es más valioso, de tal forma que tome acciones que le lleven a estados valiosos: **Value-based methods**.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/two_approaches.png\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8936d3-41c9-4bd9-bcad-430ebae1f385",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7983905-b0c0-42a3-9b64-5f17667e9c97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9226c666-64f9-4d78-a9d9-91087f50d714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24a632c9-7c06-49df-a12b-c4f9ae38b94f",
   "metadata": {},
   "source": [
    "### 2.2.1 - Cadena de Markov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8edb3fe-d482-4eef-ab02-3b598ccfa1e9",
   "metadata": {},
   "source": [
    "### 2.2.2 - Proceso de decisión de Markov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d51124-05ef-47c2-b5d8-fd5261c9e2a5",
   "metadata": {},
   "source": [
    "### 2.2.3 - Funciones de valor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962d177b-635f-45ec-a83d-42820ede752d",
   "metadata": {},
   "source": [
    "### 2.2.4 - Función de Bellman"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3332ec-27f1-4ee9-a831-09648f424d17",
   "metadata": {},
   "source": [
    "### 2.2.5 - Algoritmos tradicionales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af8884d-7773-419f-a776-55c05f3c0ab2",
   "metadata": {},
   "source": [
    "### 2.2.6 - Algoritmos basados en tablas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ffd42d-9f0a-47f9-8d98-527182349b3a",
   "metadata": {},
   "source": [
    "## 2.3 - Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb910ed-f74f-413e-8e10-3d0dfc5927c8",
   "metadata": {},
   "source": [
    "## 2.4 - Construyendo un agente mediante una red DQNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54613f5-0914-48ad-bd32-f78742f4d45e",
   "metadata": {},
   "source": [
    "### 2.4.1 - Importación de librerías"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215f72c4-46f0-4b9b-9030-02b1a0a9ff95",
   "metadata": {},
   "source": [
    "### 2.4.2 - Construcción del agente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e45b49-5ef2-4c08-bd3c-e5e957af5bec",
   "metadata": {},
   "source": [
    "### 2.4.3 - Construcción del método `main()`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
