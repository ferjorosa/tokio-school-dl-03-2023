{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48ea1445-9262-4923-942b-f88674cbb3c4",
   "metadata": {},
   "source": [
    "# 2 - Aprendizaje por refuerzo\n",
    "\n",
    "**Sumario**\n",
    "\n",
    "1. Introducción\n",
    "2. El framewok de aprendizaje por refuerzo\n",
    "3. Métodos de aprendizaje por refuerzo basados en valor\n",
    "4. Q-Learning\n",
    "5. Deep Reinforcement Learning\n",
    "6. Construyendo un agente mediante una red DQNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc605d89-89c6-4ed6-ad6b-d2c879b72b94",
   "metadata": {},
   "source": [
    "## 2.1 - Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0f39ee-cac5-452a-9d89-65b6fb5f8bd4",
   "metadata": {},
   "source": [
    "Cuando reflexionamos sobre la palabra **aprendizaje**, es posible que lo primero que nos venga a la mente sea **la forma en la que los seres humanos aprendemos mediante la interacción con nuestro entorno**.\n",
    "\n",
    "Así, uno de los primeros ejemplos que seguramente visualicemos sea el proceso de aprendizaje que experimentan los niños y las niñas cuando empiezan a andar, donde interactúan con el entorno mediante un modelo de causa y efecto que voluciona con experimentación. De este modo, los seres humanos vamos adaptando poco a poco nuestro sistema motriz cada vez que se producen caidas (**\"penalizan\" el modelo**) o que conseguimos alcanzar una determinada localización en nuestro entorno (**\"refuerzan\"** el modelo).\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/humans.jpg\" width=\"500\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Dicho sistema de **penalización** y **refuerzo** se utilizó como inspiración para definir **el aprendizaje por refuerzo** (Sutton, 1998). El cual define un modelo de comportamiento que simula el proceso de interacción entre un agente y un entorno mediante la utilización de estímulos (refuerzos) que le indicasen cuales son las decisiones mas prometedoras para alcanzar un determinado objetivo.\n",
    "\n",
    "Comenzó con acermientos basados en **programación dinámica**, y luego fue evolucionando hacia los **algoritmos basados en tablas**, como *Q-Learning* o *SARSA*, los cuales construyen una tabla (normalmente llamada **tabla Q**) donde para cada estado, se incluía un valor de calidad para cada una de las posibles acciones que se podrian realizar en un determinado estado.\n",
    "\n",
    "Este tipo de acercamientos implicaban un gran **problema computacional** ya que hacia que muchos problemas resultasen intratables por la **combinación de estados y acciones que definía el tamaño de la tabla**. Este inconveniente se solventó con el surgimiento del aprendizaje profundo (*Deep Learning*), permitiendo que el aprendizaje por refuerzo pudiera resolver problemas antes inabordables, es decir, problemas donde el tamaño de la tabla Q seria demasiado grande.\n",
    "\n",
    "En el **aprendizaje por refuerzo profundo** (***Deep Reinforcement Learning***) utilizamos una red neuronal en vez de una tabla Q para dar un valor \"aproximado\" de la calidad de una acción para un determinado estado. Este tipo de acercamiento fue un gran avance en el área, permitiendo desarrollar modelos para tareas impensables hasta el momento, como videojuegos (e.g., [**Atari**](https://openai.com/research/gym-retro) o [**Starcraft II**](https://www.deepmind.com/blog/alphastar-grandmaster-level-in-starcraft-ii-using-multi-agent-reinforcement-learning)), [**el juego de Go**](https://www.youtube.com/watch?v=WXuK6gekU1Y) o [**el plegamiento de proteinas**](https://www.deepmind.com/research/highlighted-research/alphafold).\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/alphago.jpg\" width=\"500\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "A lo largo de esta unidad, describiremos los conceptos básicos asociados al aprendizaje por refuerzo y estudiaremos cómo este ha evolucionado desde los algoritmos tradicionales hasta las redes de neuronas por refuerzo, que utilizaremos, finalmente, para construir un agente que aprenda a jugar a un videojuego."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e9f807-cd18-4874-87eb-bedcdb149e3a",
   "metadata": {},
   "source": [
    "## 2.2 - El framework de aprendizaje por refuerzo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f5d4ce-9275-44e5-a392-fcb094d06ca0",
   "metadata": {},
   "source": [
    "La idea detrás del aprendizaje por refuerzo es que un **agente** (una IA) aprenderá del **entorno** interactuando con él (a través de prueba y error) y recibiendo **recompensas** (negativas o positivas) como retroalimentación por realizar acciones.\n",
    "\n",
    "Por ejemplo, imagina poner a tu hermano pequeño frente a un videojuego que nunca jugó, darle un controlador y dejarlo solo. Tu hermano podria interactuar con el \"entorno\" (el videojuego) pulsando los botones del mando. Si obtiene una moneda, recibirá un recompensa positivo (e.g., +1). En cambio, si toca un enemigo, recibirá una recompensa negativa (e.g., -1).\n",
    "\n",
    "Al interactuar con su entorno a través de prueba y error, tu hermano acabaría entendiendo que necesita recolectar monedas y evitar enemigos. Sin ninguna supervisión el niño mejoraría  cada vez más en el juego.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/RL_process_game.jpg\" width=\"500\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "* **Agente**. La entidad que interactúa con el entorno obteniendo información de éste y modificándolo  mediante acciones.\n",
    "* **Entorno**. La representación virtual del mundo con el que interactúa el agente.\n",
    "* **Estado**. La representación completa del entorno en un instante específico de tiempo, así como la representación del agente en dicho instante.\n",
    "* **Acción**. Aquella que ejecuta el agente ene el entorno con el fin de producir una variación en este.\n",
    "* **Recompensa o refuerzo**. Valor numérico obtenido tras la ejecución de una acción en el entorno. Se utiliza como medida para evaluar si la ejecución de la acción ha resultado positiva o negativa para el agente\n",
    "\n",
    "**Este proceso es un bucle, donde el objetivo del agente es maximizar la recompensa acumulada, que representa el retorno esperado**.\n",
    "\n",
    "Hay varios componentes/asunciones relevantes en el framework del aprendizaje por refuerzo (los veremos ahora uno por uno):\n",
    "* La hipótesis de recompensa.\n",
    "* La propiedad de Markov.\n",
    "* El grado de observabilidad del entorno (total o parcial).\n",
    "* El espacio de acción (finito o infinito).\n",
    "* Las recompensa y el factor de descuento.\n",
    "* *Tradeoff* entre exploración y explotación\n",
    "* Enfoque de aprendizaje (basado en política o en valor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f675d4-b891-4ba9-81e9-b881a1babed0",
   "metadata": {},
   "source": [
    "### 2.2.1 - La hipótesis de recompensa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879bb3a9-1418-4902-8d5a-8f6bfe67e738",
   "metadata": {},
   "source": [
    "El aprendizaje por refuerzo se basa en la **hipótesis de recompensa**, la cual indica que **cualquier meta puede describirse como la maximización del rendimiento esperado** (recompensa acumulada esperada).\n",
    "\n",
    "Es por eso que en el aprendizaje por refuerzo, para tener **el mejor comportamiento**, nuestro objetivo es aprender a **tomar acciones que maximicen la recompensa acumulada esperada**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbebc9b8-6830-45dd-b84e-ddbefa8349cb",
   "metadata": {},
   "source": [
    "### 2.2.2 - La propiedad de Markov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe08125-edc3-40a3-8ad7-70f7d61fa62d",
   "metadata": {},
   "source": [
    "El proceso de RL también se denomina **Proceso de decisión de Markov** (*Markov Decission Process*, *MDP* por sus siglas en inglés).\n",
    "\n",
    "La propiedad de Markov implica que **nuestro agente solo necesita el estado actual para decidir qué acción tomar** y no el historial de todos los estados y acciones que tomó antes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb637d72-0ed4-4c8b-a313-cefd927e7908",
   "metadata": {},
   "source": [
    "### 2.2.3 - El grado de observabilidad del entorno"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5e43b7-be91-4c4e-b2cb-d74b8f304361",
   "metadata": {
    "tags": []
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/obs_space.jpg\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "-----\n",
    "\n",
    "**Nota**: Ciertos autores hacen la distinción entre \"estado\" y \"observación\" para nombrar a $S_{t}$ en el bucle de aprendizaje por refuerzo.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ee373a-817e-4dfb-a38d-119031bc869a",
   "metadata": {},
   "source": [
    "### 2.2.4 - El espacio de acción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce803aa-98d6-4e3f-a871-20c00f469e5e",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/action_space.jpg\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657440f7-9176-4fd4-a0ac-33d14b0737b2",
   "metadata": {},
   "source": [
    "### 2.2.5 - La recompensa y el factor de descuento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3532338-139f-493a-a141-f7d3e058d78f",
   "metadata": {},
   "source": [
    "La recompensa es un elemento fundamental en el proceso de aprendizaje automático porque **es el único feedback que recibe nuestro agente**. Gracias a ella, sabe si esta acción ha sido **positiva o negativa**.\n",
    "\n",
    "La recompensa acumulada para un instante de tiempo $t$ se puede escribir como:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/rewards_1.png\" width=\"400\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Esto es equivalente a:\n",
    "\n",
    "$$\n",
    "R(\\tau) = \\sum_{k=0}^{\\infty} r_{t+k+1}\n",
    "$$\n",
    "\n",
    "Sin embargo, en realidad, no podemos simplemente agregar las recompensas así. Las recompensas que llegan antes (al comienzo del juego) **tienen más probabilidades de suceder**, ya que son más predecibles que las recompensas futuras a largo plazo.\n",
    "\n",
    "Digamos que tu agente es este pequeño ratón que puede mover una ficha en cada paso de tiempo, y tu oponente es el gato (que también puede moverse). **El objetivo del ratón es comer la máxima cantidad de queso antes de ser comido por el gato.**\n",
    "\n",
    "En consecuencia, la recompensa cerca del gato, aunque sea más grande (más queso), **estará más rebajada ya que no estamos muy seguros de poder comérnoslo**.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/rewards_3.jpg\" width=\"400\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Para descontar las recompensas, procedemos así:\n",
    "\n",
    "1. Definimos una tasa de descuento $\\gamma$. Debe estar entre 0 y 1. Normalmente suele tomar valores cercanos a 0.99 o 0.95.\n",
    "    * Cuanto mayor sea $\\gamma$, **menor será el descuento**. Esto implica que el agente se centra más la **recompensa futura**.\n",
    "    * cuanto menor sea $\\gamma$, **mayor será el duescuento**. Esto implica que el agente se centra más en la **recompensa cercana**.\n",
    "2. Luego, cada recompensa será descontada por $\\gamma$ elevada al instante de tiempo actual. A medida que aumenta el paso de tiempo, el gato se acerca a nosotros, por lo que la recompensa futura es cada vez menos probable.\n",
    "\n",
    "La recompensa acumulada (**descontada**) para un instante de tiempo $t$ se puede escribir como:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/rewards_2.png\" width=\"400\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "$$\n",
    "R(\\tau) = \\sum_{k=0}^{\\infty} \\gamma^{k} r_{t+k+1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43eb1ad-34c1-469e-b8e4-56d45ffbb353",
   "metadata": {},
   "source": [
    "### 2.2.6 - *Tradeoff* entre exploración y explotación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb40b144-aeb4-4562-9e72-54097898396f",
   "metadata": {},
   "source": [
    "Antes de describir los diferentes enfoques de aprendizaje, debemos entender un aspecto muy importante en los problemas de aprendizaje por refuerzo: **el *tradeoff* entre exploración y explotación**.\n",
    "\n",
    "Recordemos que el objetivo de nuestro agente es el de maximizar la recompensa acumulada. Sin embargo, **podemos caer en una trampa común**. Para entenderla, consideremos el siguiente juego donde nuestro ratón puede tener una cantidad *infinita* de queso (+1 por cada uno que recoge), pero en la parte superior del \"laberinto\" hay una cantidad gigante de queso (+1000).\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/exp_1.jpg\" width=\"400\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Si solo nos centramos en **recoger el queso más cercano sin explorar nuestro entorno**, nuestro agente nunca llegara a la suma gigante de queso. Sin embargo, **si realiza algo de exploración  (perdiendo algo de recompensa en el corto espacio de tiempo) podriamos conseguir una recompensa aún mayor**. Esto es lo que denominamos como el *tradeoff* entre exploración y explotación.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/expexpltradeoff.jpg\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dd5284-1243-4fce-acf2-590b08a7c02c",
   "metadata": {},
   "source": [
    "### 2.2.7 - Enfoque de aprendizaje"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aab5328-3e5a-43de-8e1c-e1aeb6bd5c46",
   "metadata": {},
   "source": [
    "**La política $\\pi$ es la que define el comportamiento del agente de tal forma que sus acciones maximicen la recompensa acumulada**. Podemos verla como el **\"cerebro\" del agente**.\n",
    "\n",
    "Nuestro objetivo es encontrar la política óptima $\\pi^{*}$, la cual **maximiza la el retorno esperado** (i.e., la recompensa acumulada) cuando el agente actúa de acuerdo con ella. **La política óptima $\\pi^{*}$ se encuentra mediante el entrenamiento.**\n",
    "\n",
    "Hay dos posibles acercamientos para entrenar a nuestro agente a encontrar la política óptima $\\pi^{*}$:\n",
    "* **Directamente**, enseñando al agente que acción debe tomar en cada estado: **Policy-based methods**.\n",
    "* **Indirectamente**, enseñando al agente que estado es más valioso, de tal forma que tome acciones que le lleven a estados valiosos: **Value-based methods**.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/two_approaches.png\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beece9f5-485a-4851-9179-43477497f9e6",
   "metadata": {},
   "source": [
    "## 2.3 - Métodos de aprendizaje por refuerzo basados en valor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942ef2e4-7eda-431c-9d99-c6c899286172",
   "metadata": {},
   "source": [
    "Aprendemos una función que asigna a cada estado el valor esperado de estar en él. Dicho valor es el rendimiento descontado esperado que el agente puede obtener si comienza en ese estado y luego actúa de acuerdo con nuestra política.\n",
    "\n",
    "-----\n",
    "\n",
    "<span style=\"color:red\"><b>Pregunta:</b></span> Pero, ¿que significa actuar con respecto a nuestra política? Al fin y al cabo, no tenemos una política en métodos basados en valor dado que lo que aprendemos es una función de valor, no una política en sí misma.\n",
    "\n",
    "<span style=\"color:blue\"><b>Respuesta</b></span> En este caso, la política es \"latente\", no definimos a mano el comportamiento de nuestra política; **es el entrenamiento el que indirectamente la definirá**. Ahora, dado que la política no está entrenada/aprendida, **necesitamos especificar su comportamiento**. Por ejemplo, si queremos una política que, dada la función de valor, realice acciones que siempre conduzcan a la mayor recompensa, crearemos una **política codiciosa**.\n",
    "\n",
    "-----\n",
    "\n",
    "<!-- <table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/link_value_policy.png\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table> -->\n",
    "\n",
    "Escribimos la función de valor para un estado $s$ bajo una política $\\pi$ de la siguiente forma:\n",
    "\n",
    "$$\n",
    "V_{\\pi}(s) = \\mathbf{E}_{\\pi}[G_{t}[S_{t} = s]]\n",
    "$$\n",
    "\n",
    "Para cada estado $s$, la función devuelve el retorno esperado si el agente empieza en ese estado y sigue la política los siguientes instantes de tiempo.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/state-value-function-2.png\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "El problema es que para calcular el valor de cada estado debemos **sumar todas las recompensas que el agente puede obtener si empieza en dicho estado**. Esto puede ser computacionalmente muy caro, especialmente si tenemos un gran número de estados posibles. \n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/bellman2.png\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Para remediarlo, existe la <span style=\"color:blue\"><b>ecuación de Bellman</b></span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760efa5a-4d01-4710-a254-8e1a37c56a52",
   "metadata": {},
   "source": [
    "### 2.3.1 - Ecuación de Bellman"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c4b07d-d56a-4f55-9427-e384882cdefe",
   "metadata": {},
   "source": [
    "La ecuación de Bellman es una función recursiva inspirada en la [programación dinámica](https://es.wikipedia.org/wiki/Programaci%C3%B3n_din%C3%A1mica) que \"acumula\" la suma local de recompensas para cada estado de tal forma que podamos agilizar el proceso de cómputo. Consideramos cada estado como **la recompensa inmediata $R_{t+1}$ (es decir, la recompensa de pasar al siguiente estado) + el valor descontado del estado siguiente** ($\\gamma * V(S_{t+1})$).\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/bellman4.png\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ee1033-3d70-4ebb-9d70-a4f69564aedb",
   "metadata": {},
   "source": [
    "### 2.3.2 - *Monte Carlo* vs *Temporal Difference*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e59135-792b-4b08-a816-5cb9a2a23c9f",
   "metadata": {},
   "source": [
    "Existen dos estrategias con las que aprender nuestra función de valor:\n",
    "* ***Monte Carlo***. Utiliza la experiencia obtenida de todo un episodio antes de aprender.\n",
    "* ***Temporal Difference***. Utiliza la experiencia obtnida después de cada paso en el episodio para aprender."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7eb47e-4983-4828-baf3-d56300c5d304",
   "metadata": {},
   "source": [
    "#### *Monte Carlo*\n",
    "\n",
    "Monte Carlo espera hasta terminar el episodio, calcula el retorno $G_{t}$ obtenido para cada paso $t$ realizado y utiliza esta información como referencia para actualizar el valor de cada estado $V(S_{t})$:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/monte-carlo-approach.jpg\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Consideremos el juego del ratón y el gato presentado anteriormente como ejemplo:\n",
    "\n",
    "* Ponemos un límite al episodio. Por ejemplo:\n",
    "    * Si el gato se come al ratón\n",
    "    * Si el ratón realiza > 10 pasos\n",
    "* Siempre empezamos el episodio en el **mismo punto de inicio**.\n",
    "* **El agente realizará acciones con respecto a su política especificada**. Por ejemplo, puede usar una política codiciosa *epsilon* (i.e., *Epsilon Greedy Strategy*), la cual alterna entre exploración y explotación de forma probabilística.\n",
    "* En cada instante de tiempo del episodio, obtenemos la **recompensa para ese estado** y cual es el **siguiente estado al que ir**.\n",
    "* Al final del episodio, **sumamos las recompensas que ha obtenido** el agente (para ver cuan bien lo ha hecho).\n",
    "* **Actualizamos el valor de cada estado** $V(S_{t})$ según la fórmula.\n",
    "* Comenzar un nuevo episodio con este nuevo conocimiento.\n",
    "\n",
    "Consideremos el siguiente episodio de ejemplo para ver como realizamos estos pasos:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/MC-4p.jpg\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Una vez terminado el episodio, tenemos una lista con los diferentes estados en los que ha estado el agente asi como las recompensas correspondientes. A partir de esto, podemos calcular el **retorno total** $G_{t}$ del episodio:\n",
    "\n",
    "* $G_{t} = R_{t+1} + R_{t+2} + R_{t+3} + \\dots$\n",
    "* $G_{t} = 1 + 0 + 0 + 0 + 0 + 0 + 1 + 1 + 0 + 0 = 3$\n",
    "\n",
    "Con esta información y si asumimos que la tasa de aprendizaje ($\\alpha$) es $0.1$ podemos el valor del estado $S_{0}$:\n",
    "\n",
    "* $V(S_{0})^{\\text{new}} = V_{S_{0}} + \\alpha * [G_{t} - V(S_{0})]$\n",
    "* $V(S_{0})^{\\text{new}} = 0 + 0.1 * [3 - 0] = 0.3$\n",
    "\n",
    "Podemos repetir este mismo procedimiento para el resto de estados $S_{1}$, $S_{2}$, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7f454e-c64d-47da-a39d-092100439592",
   "metadata": {},
   "source": [
    "#### *Temporal Difference*\n",
    "\n",
    "*Temporal Difference* (TD) simplemente espera una iteración (un instante de tiempo) $S_{t+1}$ para generar un **retorno parcial** y actualizar $V(S_{t})$ utilizando $R_{t+1}$ y ($\\gamma * V(S_{t+1})$).\n",
    "\n",
    "Dado que no hemos experimento todo el episodio, no tenemos $G_{t}$ (el retorno total). **El retorno parcial aproxima $G_{t}$ mediante la suma de la recomensa inmidiatamente posterior $R_{t+1}$ y el valor descontado del siguiente estado $\\gamma * V(S_{t+1})$**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/TD-1.jpg\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Si tomamos el mismo ejemplo que en el caso de *Monte Carlo* (juego del ratón y el gato):\n",
    "* Consideramos que es el comienzo del entrenamiento, asi que el valor de todo los estados es $0$.\n",
    "* Nuestra tasa de aprendizaje $\\alpha$ es $0.1$ y nuestra tasa de descuento $\\gamma$ es 1 (no se descuenta, recordemos que $0$ implica máximo descuento).\n",
    "* Nuestro ratón **decide explorar el entorno y toma una acción aleatoria**: e.g., va hacia la izquierda.\n",
    "* Recibe una recompensa $R_{t+1}$ ya que come una pieza de queso.\n",
    "\n",
    "Con esta información podemos actualizar el valor del estado $S_{0}$:\n",
    "* $V(S_{0})^{\\text{new}} = V(S_{0}) + \\alpha * [R_{t+1} + \\gamma * V(S_{1}) - V(S_{0})]$\n",
    "* $V(S_{0})^{\\text{new}} = 0 + 0.1 * [1 + 1 * 0 - 0] = 0.1$\n",
    "\n",
    "El agente continuaria actualizando su función de valor mientras interactúa con el entorno. En este caso con la versión actualizada de $V(S_{0})$. Al igual que con Monte Carlo, una vez termine el episodio, podemos repetirlo para seguir aprendiendo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0627fd-e2dd-4294-a57f-d912a327591b",
   "metadata": {},
   "source": [
    "## 2.4 - Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9226c666-64f9-4d78-a9d9-91087f50d714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74ffd42d-9f0a-47f9-8d98-527182349b3a",
   "metadata": {},
   "source": [
    "## 2.5 - Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb910ed-f74f-413e-8e10-3d0dfc5927c8",
   "metadata": {},
   "source": [
    "## 2.6 - Construyendo un agente mediante una red DQNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54613f5-0914-48ad-bd32-f78742f4d45e",
   "metadata": {},
   "source": [
    "### 2.6.1 - Importación de librerías"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215f72c4-46f0-4b9b-9030-02b1a0a9ff95",
   "metadata": {},
   "source": [
    "### 2.6.2 - Construcción del agente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e45b49-5ef2-4c08-bd3c-e5e957af5bec",
   "metadata": {},
   "source": [
    "### 2.6.3 - Construcción del método `main()`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
