{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48ea1445-9262-4923-942b-f88674cbb3c4",
   "metadata": {},
   "source": [
    "# 2 - Aprendizaje por refuerzo\n",
    "\n",
    "**Sumario**\n",
    "\n",
    "1. Introducción\n",
    "2. El framewok de aprendizaje por refuerzo\n",
    "3. Métodos de aprendizaje por refuerzo basados en valor\n",
    "4. Q-Learning\n",
    "5. Deep Reinforcement Learning\n",
    "6. Construyendo un agente mediante una red DQNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc605d89-89c6-4ed6-ad6b-d2c879b72b94",
   "metadata": {},
   "source": [
    "## 2.1 - Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0f39ee-cac5-452a-9d89-65b6fb5f8bd4",
   "metadata": {},
   "source": [
    "Cuando reflexionamos sobre la palabra **aprendizaje**, es posible que lo primero que nos venga a la mente sea **la forma en la que los seres humanos aprendemos mediante la interacción con nuestro entorno**.\n",
    "\n",
    "Así, uno de los primeros ejemplos que seguramente visualicemos sea el proceso de aprendizaje que experimentan los niños y las niñas cuando empiezan a andar, donde interactúan con el entorno mediante un modelo de causa y efecto que voluciona con experimentación. De este modo, los seres humanos vamos adaptando poco a poco nuestro sistema motriz cada vez que se producen caidas (**\"penalizan\" el modelo**) o que conseguimos alcanzar una determinada localización en nuestro entorno (**\"refuerzan\"** el modelo).\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/humans.jpg\" width=\"500\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Dicho sistema de **penalización** y **refuerzo** se utilizó como inspiración para definir **el aprendizaje por refuerzo** (Sutton, 1998). El cual define un modelo de comportamiento que simula el proceso de interacción entre un agente y un entorno mediante la utilización de estímulos (refuerzos) que le indicasen cuales son las decisiones mas prometedoras para alcanzar un determinado objetivo.\n",
    "\n",
    "Comenzó con acermientos basados en **programación dinámica**, y luego fue evolucionando hacia los **algoritmos basados en tablas**, como *Q-Learning* o *SARSA*, los cuales construyen una tabla (normalmente llamada **tabla Q**) donde para cada estado, se incluía un valor de calidad para cada una de las posibles acciones que se podrian realizar en un determinado estado.\n",
    "\n",
    "Este tipo de acercamientos implicaban un gran **problema computacional** ya que hacia que muchos problemas resultasen intratables por la **combinación de estados y acciones que definía el tamaño de la tabla**. Este inconveniente se solventó con el surgimiento del aprendizaje profundo (*Deep Learning*), permitiendo que el aprendizaje por refuerzo pudiera resolver problemas antes inabordables, es decir, problemas donde el tamaño de la tabla Q seria demasiado grande.\n",
    "\n",
    "En el **aprendizaje por refuerzo profundo** (***Deep Reinforcement Learning***) utilizamos una red neuronal en vez de una tabla Q para dar un valor \"aproximado\" de la calidad de una acción para un determinado estado. Este tipo de acercamiento fue un gran avance en el área, permitiendo desarrollar modelos para tareas impensables hasta el momento, como videojuegos (e.g., [**Atari**](https://openai.com/research/gym-retro) o [**Starcraft II**](https://www.deepmind.com/blog/alphastar-grandmaster-level-in-starcraft-ii-using-multi-agent-reinforcement-learning)), [**el juego de Go**](https://www.youtube.com/watch?v=WXuK6gekU1Y) o [**el plegamiento de proteinas**](https://www.deepmind.com/research/highlighted-research/alphafold).\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/alphago.jpg\" width=\"500\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "A lo largo de esta unidad, describiremos los conceptos básicos asociados al aprendizaje por refuerzo y estudiaremos cómo este ha evolucionado desde los algoritmos tradicionales hasta las redes de neuronas por refuerzo, que utilizaremos, finalmente, para construir un agente que aprenda a jugar a un videojuego."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e9f807-cd18-4874-87eb-bedcdb149e3a",
   "metadata": {},
   "source": [
    "## 2.2 - El framework de aprendizaje por refuerzo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f5d4ce-9275-44e5-a392-fcb094d06ca0",
   "metadata": {},
   "source": [
    "La idea detrás del aprendizaje por refuerzo es que un **agente** (una IA) aprenderá del **entorno** interactuando con él (a través de prueba y error) y recibiendo **recompensas** (negativas o positivas) como retroalimentación por realizar acciones.\n",
    "\n",
    "Por ejemplo, imagina poner a tu hermano pequeño frente a un videojuego que nunca jugó, darle un controlador y dejarlo solo. Tu hermano podria interactuar con el \"entorno\" (el videojuego) pulsando los botones del mando. Si obtiene una moneda, recibirá un recompensa positivo (e.g., +1). En cambio, si toca un enemigo, recibirá una recompensa negativa (e.g., -1).\n",
    "\n",
    "Al interactuar con su entorno a través de prueba y error, tu hermano acabaría entendiendo que necesita recolectar monedas y evitar enemigos. Sin ninguna supervisión el niño mejoraría  cada vez más en el juego.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/RL_process_game.jpg\" width=\"500\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "* **Agente**. La entidad que interactúa con el entorno obteniendo información de éste y modificándolo  mediante acciones.\n",
    "* **Entorno**. La representación virtual del mundo con el que interactúa el agente.\n",
    "* **Estado**. La representación completa del entorno en un instante específico de tiempo, así como la representación del agente en dicho instante.\n",
    "* **Acción**. Aquella que ejecuta el agente ene el entorno con el fin de producir una variación en este.\n",
    "* **Recompensa o refuerzo**. Valor numérico obtenido tras la ejecución de una acción en el entorno. Se utiliza como medida para evaluar si la ejecución de la acción ha resultado positiva o negativa para el agente\n",
    "\n",
    "**Este proceso es un bucle, donde el objetivo del agente es maximizar la recompensa acumulada, que representa el retorno esperado**.\n",
    "\n",
    "Hay varios componentes/asunciones relevantes en el framework del aprendizaje por refuerzo (los veremos ahora uno por uno):\n",
    "* La hipótesis de recompensa.\n",
    "* La propiedad de Markov.\n",
    "* El grado de observabilidad del entorno (total o parcial).\n",
    "* El espacio de acción (finito o infinito).\n",
    "* Las recompensa y el factor de descuento.\n",
    "* *Tradeoff* entre exploración y explotación\n",
    "* Enfoque de aprendizaje (basado en política o en valor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f675d4-b891-4ba9-81e9-b881a1babed0",
   "metadata": {},
   "source": [
    "### 2.2.1 - La hipótesis de recompensa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879bb3a9-1418-4902-8d5a-8f6bfe67e738",
   "metadata": {},
   "source": [
    "El aprendizaje por refuerzo se basa en la **hipótesis de recompensa**, la cual indica que **cualquier meta puede describirse como la maximización del rendimiento esperado** (recompensa acumulada esperada).\n",
    "\n",
    "Es por eso que en el aprendizaje por refuerzo, para tener **el mejor comportamiento**, nuestro objetivo es aprender a **tomar acciones que maximicen la recompensa acumulada esperada**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbebc9b8-6830-45dd-b84e-ddbefa8349cb",
   "metadata": {},
   "source": [
    "### 2.2.2 - La propiedad de Markov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe08125-edc3-40a3-8ad7-70f7d61fa62d",
   "metadata": {},
   "source": [
    "El proceso de RL también se denomina **Proceso de decisión de Markov** (*Markov Decission Process*, *MDP* por sus siglas en inglés).\n",
    "\n",
    "La propiedad de Markov implica que **nuestro agente solo necesita el estado actual para decidir qué acción tomar** y no el historial de todos los estados y acciones que tomó antes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb637d72-0ed4-4c8b-a313-cefd927e7908",
   "metadata": {},
   "source": [
    "### 2.2.3 - El grado de observabilidad del entorno"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5e43b7-be91-4c4e-b2cb-d74b8f304361",
   "metadata": {
    "tags": []
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/obs_space.jpg\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "-----\n",
    "\n",
    "**Nota**: Ciertos autores hacen la distinción entre \"estado\" y \"observación\" para nombrar a $S_{t}$ en el bucle de aprendizaje por refuerzo.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ee373a-817e-4dfb-a38d-119031bc869a",
   "metadata": {},
   "source": [
    "### 2.2.4 - El espacio de acción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce803aa-98d6-4e3f-a871-20c00f469e5e",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/action_space.jpg\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657440f7-9176-4fd4-a0ac-33d14b0737b2",
   "metadata": {},
   "source": [
    "### 2.2.5 - La recompensa y el factor de descuento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3532338-139f-493a-a141-f7d3e058d78f",
   "metadata": {},
   "source": [
    "La recompensa es un elemento fundamental en el proceso de aprendizaje automático porque **es el único feedback que recibe nuestro agente**. Gracias a ella, sabe si esta acción ha sido **positiva o negativa**.\n",
    "\n",
    "La recompensa acumulada para un instante de tiempo $t$ se puede escribir como:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/rewards_1.png\" width=\"400\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Esto es equivalente a:\n",
    "\n",
    "$$\n",
    "R(\\tau) = \\sum_{k=0}^{\\infty} r_{t+k+1}\n",
    "$$\n",
    "\n",
    "Sin embargo, en realidad, no podemos simplemente agregar las recompensas así. Las recompensas que llegan antes (al comienzo del juego) **tienen más probabilidades de suceder**, ya que son más predecibles que las recompensas futuras a largo plazo.\n",
    "\n",
    "Digamos que tu agente es este pequeño ratón que puede mover una ficha en cada paso de tiempo, y tu oponente es el gato (que también puede moverse). **El objetivo del ratón es comer la máxima cantidad de queso antes de ser comido por el gato.**\n",
    "\n",
    "En consecuencia, la recompensa cerca del gato, aunque sea más grande (más queso), **estará más rebajada ya que no estamos muy seguros de poder comérnoslo**.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/rewards_3.jpg\" width=\"400\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Para descontar las recompensas, procedemos así:\n",
    "\n",
    "1. Definimos una tasa de descuento $\\gamma$. Debe estar entre 0 y 1. Normalmente suele tomar valores cercanos a 0.99 o 0.95.\n",
    "    * Cuanto mayor sea $\\gamma$, **menor será el descuento**. Esto implica que el agente se centra más la **recompensa futura**.\n",
    "    * cuanto menor sea $\\gamma$, **mayor será el duescuento**. Esto implica que el agente se centra más en la **recompensa cercana**.\n",
    "2. Luego, cada recompensa será descontada por $\\gamma$ elevada al instante de tiempo actual. A medida que aumenta el paso de tiempo, el gato se acerca a nosotros, por lo que la recompensa futura es cada vez menos probable.\n",
    "\n",
    "La recompensa acumulada (**descontada**) para un instante de tiempo $t$ se puede escribir como:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/rewards_2.png\" width=\"400\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "$$\n",
    "R(\\tau) = \\sum_{k=0}^{\\infty} \\gamma^{k} r_{t+k+1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43eb1ad-34c1-469e-b8e4-56d45ffbb353",
   "metadata": {},
   "source": [
    "### 2.2.6 - *Tradeoff* entre exploración y explotación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb40b144-aeb4-4562-9e72-54097898396f",
   "metadata": {},
   "source": [
    "Antes de describir los diferentes enfoques de aprendizaje, debemos entender un aspecto muy importante en los problemas de aprendizaje por refuerzo: **el *tradeoff* entre exploración y explotación**.\n",
    "\n",
    "Recordemos que el objetivo de nuestro agente es el de maximizar la recompensa acumulada. Sin embargo, **podemos caer en una trampa común**. Para entenderla, consideremos el siguiente juego donde nuestro ratón puede tener una cantidad *infinita* de queso (+1 por cada uno que recoge), pero en la parte superior del \"laberinto\" hay una cantidad gigante de queso (+1000).\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/exp_1.jpg\" width=\"400\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Si solo nos centramos en **recoger el queso más cercano sin explorar nuestro entorno**, nuestro agente nunca llegara a la suma gigante de queso. Sin embargo, **si realiza algo de exploración  (perdiendo algo de recompensa en el corto espacio de tiempo) podriamos conseguir una recompensa aún mayor**. Esto es lo que denominamos como el *tradeoff* entre exploración y explotación.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/expexpltradeoff.jpg\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dd5284-1243-4fce-acf2-590b08a7c02c",
   "metadata": {},
   "source": [
    "### 2.2.7 - Enfoque de aprendizaje"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aab5328-3e5a-43de-8e1c-e1aeb6bd5c46",
   "metadata": {},
   "source": [
    "**La política $\\pi$ es la que define el comportamiento del agente de tal forma que sus acciones maximicen la recompensa acumulada**. Podemos verla como el **\"cerebro\" del agente**.\n",
    "\n",
    "Nuestro objetivo es encontrar la política óptima $\\pi^{*}$, la cual **maximiza la el retorno esperado** (i.e., la recompensa acumulada) cuando el agente actúa de acuerdo con ella. **La política óptima $\\pi^{*}$ se encuentra mediante el entrenamiento.**\n",
    "\n",
    "Hay dos posibles acercamientos para entrenar a nuestro agente a encontrar la política óptima $\\pi^{*}$:\n",
    "* **Directamente**, enseñando al agente que acción debe tomar en cada estado: **Policy-based methods**.\n",
    "* **Indirectamente**, enseñando al agente que estado es más valioso, de tal forma que tome acciones que le lleven a estados valiosos: **Value-based methods**.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/two_approaches.png\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beece9f5-485a-4851-9179-43477497f9e6",
   "metadata": {},
   "source": [
    "## 2.3 - Métodos de aprendizaje por refuerzo basados en valor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942ef2e4-7eda-431c-9d99-c6c899286172",
   "metadata": {},
   "source": [
    "Aprendemos una función que asigna a cada estado el valor esperado de estar en él. Dicho valor es el rendimiento descontado esperado que el agente puede obtener si comienza en ese estado y luego actúa de acuerdo con nuestra política.\n",
    "\n",
    "-----\n",
    "\n",
    "<span style=\"color:red\"><b>Pregunta:</b></span> Pero, ¿que significa actuar con respecto a nuestra política? Al fin y al cabo, no tenemos una política en métodos basados en valor dado que lo que aprendemos es una función de valor, no una política en sí misma.\n",
    "\n",
    "<span style=\"color:blue\"><b>Respuesta</b></span> En este caso, la política es \"latente\", no definimos a mano el comportamiento de nuestra política; **es el entrenamiento el que indirectamente la definirá**. Ahora, dado que la política no está entrenada/aprendida, **necesitamos especificar su comportamiento**. Por ejemplo, si queremos una política que, dada la función de valor, realice acciones que siempre conduzcan a la mayor recompensa, crearemos una **política codiciosa**.\n",
    "\n",
    "-----\n",
    "\n",
    "Tenemos dos posibles acercamientos:\n",
    "* Métodos de **estado-valor**.\n",
    "* Métodos de **acción-estado-valor**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5863d2aa-95e2-429e-a8ce-5b4516f3e21a",
   "metadata": {},
   "source": [
    "### 2.3.1 - Métodos de estado-valor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382f373d-1715-45e1-8ea7-b057f2af7459",
   "metadata": {},
   "source": [
    "Para cada estado $s$, la función devuelve el retorno esperado si el agente empieza en ese estado y sigue la política los siguientes instantes de tiempo.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/state-value-function-2.png\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Escribimos la función de valor para un estado $s$ bajo una política $\\pi$ de la siguiente forma:\n",
    "\n",
    "<!-- $$\n",
    "V_{\\pi}(s) = \\mathbf{E}_{\\pi}[G_{t}[S_{t} = s]]\n",
    "$$\n",
    " -->\n",
    " \n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/state-value-function-1.png\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb5ada7-3b1c-4e6d-849f-eb7a133f20e2",
   "metadata": {},
   "source": [
    "### 2.3.2 - Métodos de acción-estado-valor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08533ed9-a638-414a-991d-ccf598d4f99b",
   "metadata": {},
   "source": [
    "En este caso, la función devuelve el valor para cada par de acción-estado. Es decir, el retorno esperado si el agente comenzase en ese estado, tomáse esa acción y siguiese su política hasta terminar el episodio.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/action-state-value-function-2.jpg\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "El valor de tomar la acción $a$ en el estado $s$ bajo la política $\\pi$ es:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/action-state-value-function-1.png\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760efa5a-4d01-4710-a254-8e1a37c56a52",
   "metadata": {},
   "source": [
    "### 2.3.3 - Ecuación de Bellman"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c4b07d-d56a-4f55-9427-e384882cdefe",
   "metadata": {},
   "source": [
    "El problema es que para calcular el valor de cada estado debemos **sumar todas las recompensas que el agente puede obtener si empieza en dicho estado**. Esto puede ser computacionalmente muy caro, especialmente si tenemos un gran número de estados posibles. \n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/bellman2.png\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Para remediarlo, existe la <span style=\"color:blue\"><b>ecuación de Bellman</b></span>.\n",
    "\n",
    "La ecuación de Bellman es una función recursiva inspirada en la [programación dinámica](https://es.wikipedia.org/wiki/Programaci%C3%B3n_din%C3%A1mica) que \"acumula\" la suma local de recompensas para cada estado de tal forma que podamos agilizar el proceso de cómputo. Consideramos cada estado como **la recompensa inmediata $R_{t+1}$ (es decir, la recompensa de pasar al siguiente estado) + el valor descontado del estado siguiente** ($\\gamma * V(S_{t+1})$).\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/bellman4.png\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ee1033-3d70-4ebb-9d70-a4f69564aedb",
   "metadata": {},
   "source": [
    "### 2.3.2 - *Monte Carlo* vs *Temporal Difference*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e59135-792b-4b08-a816-5cb9a2a23c9f",
   "metadata": {},
   "source": [
    "Existen dos estrategias con las que aprender nuestra función de valor:\n",
    "* ***Monte Carlo***. Utiliza la experiencia obtenida de todo un episodio antes de aprender.\n",
    "* ***Temporal Difference***. Utiliza la experiencia obtnida después de cada paso en el episodio para aprender."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7eb47e-4983-4828-baf3-d56300c5d304",
   "metadata": {},
   "source": [
    "#### *Monte Carlo*\n",
    "\n",
    "Monte Carlo espera hasta terminar el episodio, calcula el retorno $G_{t}$ obtenido para cada paso $t$ realizado y utiliza esta información como referencia para actualizar el valor de cada estado $V(S_{t})$:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/monte-carlo-approach.jpg\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Consideremos el juego del ratón y el gato presentado anteriormente como ejemplo:\n",
    "\n",
    "* Ponemos un límite al episodio. Por ejemplo:\n",
    "    * Si el gato se come al ratón\n",
    "    * Si el ratón realiza > 10 pasos\n",
    "* Siempre empezamos el episodio en el **mismo punto de inicio**.\n",
    "* **El agente realizará acciones con respecto a su política especificada**. Por ejemplo, puede usar una política codiciosa $\\epsilon$ (i.e., $\\epsilon$-*Greedy Strategy*), la cual alterna entre exploración y explotación de forma probabilística.\n",
    "* En cada instante de tiempo del episodio, obtenemos la **recompensa para ese estado** y cual es el **siguiente estado al que ir**.\n",
    "* Al final del episodio, **sumamos las recompensas que ha obtenido** el agente (para ver cuan bien lo ha hecho).\n",
    "* **Actualizamos el valor de cada estado** $V(S_{t})$ según la fórmula.\n",
    "* Comenzar un nuevo episodio con este nuevo conocimiento.\n",
    "\n",
    "Consideremos el siguiente episodio de ejemplo para ver como realizamos estos pasos:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/MC-4p.jpg\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Una vez terminado el episodio, tenemos una lista con los diferentes estados en los que ha estado el agente asi como las recompensas correspondientes. A partir de esto, podemos calcular el **retorno total** $G_{t}$ del episodio:\n",
    "\n",
    "* $G_{t} = R_{t+1} + R_{t+2} + R_{t+3} + \\dots$\n",
    "* $G_{t} = 1 + 0 + 0 + 0 + 0 + 0 + 1 + 1 + 0 + 0 = 3$\n",
    "\n",
    "Con esta información y si asumimos que la tasa de aprendizaje ($\\alpha$) es $0.1$ podemos el valor del estado $S_{0}$:\n",
    "\n",
    "* $V(S_{0})^{\\text{new}} = V_{S_{0}} + \\alpha * [G_{t} - V(S_{0})]$\n",
    "* $V(S_{0})^{\\text{new}} = 0 + 0.1 * [3 - 0] = 0.3$\n",
    "\n",
    "Podemos repetir este mismo procedimiento para el resto de estados $S_{1}$, $S_{2}$, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7f454e-c64d-47da-a39d-092100439592",
   "metadata": {},
   "source": [
    "#### *Temporal Difference*\n",
    "\n",
    "*Temporal Difference* (TD) simplemente espera una iteración (un instante de tiempo) $S_{t+1}$ para generar un **retorno parcial** y actualizar $V(S_{t})$ utilizando $R_{t+1}$ y ($\\gamma * V(S_{t+1})$).\n",
    "\n",
    "Dado que no hemos experimento todo el episodio, no tenemos $G_{t}$ (el retorno total). **El retorno parcial aproxima $G_{t}$ mediante la suma de la recomensa inmidiatamente posterior $R_{t+1}$ y el valor descontado del siguiente estado $\\gamma * V(S_{t+1})$**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/TD-1.jpg\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Si tomamos el mismo ejemplo que en el caso de *Monte Carlo* (juego del ratón y el gato):\n",
    "* Consideramos que es el comienzo del entrenamiento, asi que el valor de todo los estados es $0$.\n",
    "* Nuestra tasa de aprendizaje $\\alpha$ es $0.1$ y nuestra tasa de descuento $\\gamma$ es 1 (no se descuenta, recordemos que $0$ implica máximo descuento).\n",
    "* Nuestro ratón **decide explorar el entorno y toma una acción aleatoria**: e.g., va hacia la izquierda.\n",
    "* Recibe una recompensa $R_{t+1}$ ya que come una pieza de queso.\n",
    "\n",
    "Con esta información podemos actualizar el valor del estado $S_{0}$:\n",
    "* $V(S_{0})^{\\text{new}} = V(S_{0}) + \\alpha * [R_{t+1} + \\gamma * V(S_{1}) - V(S_{0})]$\n",
    "* $V(S_{0})^{\\text{new}} = 0 + 0.1 * [1 + 1 * 0 - 0] = 0.1$\n",
    "\n",
    "El agente continuaria actualizando su función de valor mientras interactúa con el entorno. En este caso con la versión actualizada de $V(S_{0})$. Al igual que con Monte Carlo, una vez termine el episodio, podemos repetirlo para seguir aprendiendo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0627fd-e2dd-4294-a57f-d912a327591b",
   "metadata": {},
   "source": [
    "## 2.4 - Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebb8280-32d2-43f6-b35a-c91511899b65",
   "metadata": {},
   "source": [
    "Q-learning es un metodo de aprendizaje por refuerzo basado en valor que utiliza una estrategia *Temporal Difference* para aprender su función de acción-estado-valor. **Q-learning es el algoritmo que utilizamos para aprender nuestra funcíon Q**, la cual es del tipo **acción-estado-valor** y determina el valor de tomar una determinada acción en un estado particular.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/Q-function.png\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Dado un estado y una acción, nuestra función Q devuelve un valor (llamado valor Q).\n",
    "\n",
    "**La \"Q\" se origina de la palabra \"Quality\", que indica la calidad de tomar dicha acción en ese estado**. Recordemos brevemente  la diferencia entre \"valor\" y \"recompensa\":\n",
    "* El **valor de un estado, o de un par estado-acción** es la recompensa acumulada esperada si nuestro agente empieza en dicho estado (o empieza con ese par estado-acción) y después actua siguiendo su política hasta el final.\n",
    "* La **recompensa** es el feedback que el agente recibe del entorno al realizar una acción en dicho estado.\n",
    "\n",
    "**Internamente, nuestra función Q tiene una tabla Q, donde cada celda corresponde con el valor de un par estado-acción.** <span style=\"color:blue\"><b>Podemos pensar en esta tabla como la \"memoria\" de nuestra función Q.</b></span>\n",
    "\n",
    "Tomemos el siguiente \"minijuego\" como ejemplo:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/Maze-3.png\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Como resumen, <span style=\"color:blue\">Q-learning</span> es un algoritmo de aprendizaje por refuerzo que:\n",
    "\n",
    "* Aprende una función Q (del tipo **acción-estado-valor**) que internamente es **una tabla con todos los pares estado-acción posibles**.\n",
    "* Dado un estado y una acción, nuestra función Q **busca en su tabla Q el valor correspondiente**.\n",
    "* **Cuando el entrenamiento ha terminado, tenemos una función Q óptima, lo que significa que tenemos una tabla Q óptima**.\n",
    "* Si tenemos una función Q óptima, **tenemos una política óptima porque sabemos para cada estado cual es la mejor acción a tomar**.\n",
    "\n",
    "Sin embargo, al principio del aprendizaje nuestra tabla Q es \"inútil\" porque contiene valores arbitrarios para cada par estado-acción (la mayor parte del tiempo inicializamos la tabla con 0s). **Según el agente va explorando el entorno y actulizamos la tabla Q, nos devolverá mejores aproximaciones de la política óptima**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/Q-learning-1.png\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6cac5b-bf91-49eb-8ff2-01dfd3e3e949",
   "metadata": {},
   "source": [
    "### 2.4.1 - Algoritmo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1736c4d-82fc-4d15-b80f-560f2fdd06a3",
   "metadata": {},
   "source": [
    "Ahora que tenemos una idea de cómo funciona el algoritmo Q-learning, podemos introducirnos más en los pasos específicos que lo componen:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/Q-learning-2.png\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "#### Paso 1: Inicializar la tabla Q (generalmente con 0s)\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/Q-learning-3.png\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "#### Paso 2: Escoger una acción siguiendo la política codiciosa $\\epsilon$\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/Q-learning-4.png\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "La idea de esta política es que empecemos con un valor inicial de $\\epsilon = 1.0$, de tal forma que:\n",
    "* Con probabilidad $1 - \\epsilon$: **explotamos el entorno** (i.e., nuestro agente selecciona la accion con el máximo valor)\n",
    "* con probabilibilidad $\\epsilon$: **exploramos el entorno** (i.e., nuestro agente realiza una acción al azar)\n",
    "\n",
    "Al comienzo del entrenamiento la probabilidad de que el agente explore es muy alta dado que **$\\epsilon$ tiene un valor muy elevado**. Sin embargo, según avance el entrenamiento y nuestra tabla Q se vuelva mejor y mejor en sus estimaciones, **iremos reduciendo el valor $\\epsilon$** de tal manera que necesitaremos menos exploración y más explotación.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/Q-learning-5.png\" width=\"250\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "#### Paso 3: Realizar una acción $A_{t}$, obtener recompensa $R_{t+1}$ y el siguiente estado $S_{t+1}$\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/Q-learning-6.png\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "#### Paso 4: Actualizar $Q(S_{t}, A_{t})$\n",
    "\n",
    "Recordemos que según la estrategia de *Temporal Difference*, actualizamos nuestra función de valor tras cada interacción que haga el agente con el entorno. Para Q-learning la fórmula toma la siguiente forma:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/Q-learning-8.png\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Esto implica que para actualizar nuestra función $Q(S_{t}, A_{t})$ necesitamos:\n",
    "* Las variables $S_{t}$, $A_{t}$, $R_{t+1}$, $S_{t+1}$\n",
    "* El TD-target\n",
    "\n",
    "**¿Cómo obtenemos el TD-target?**\n",
    "\n",
    "1. Obtenemos la recompensa inmediata $R_{t+1}$ de tomar la acción $A_{t}$.\n",
    "2. Para obtener el siguiente par de estado-acción, utilizamos una política codiciosa. Nótese que no es la política codiciosa $\\epsilon$. En este caso seleccionamos directamente aquella accion con el valor Q más alto.\n",
    "\n",
    "Si nos fijamos, **el algoritmo Q-learning utiliza una política distinta al actuar (inferencia) y al actualizar el valor Q (aprendizaje)**. A este comportamiento se le denaomina ***off-policy***. En el caso de que fuera *on-policy* tendria que utilizar la política codiciosa $\\epsilon$ tanto al actual como al actualizar el valor Q. [Este es el caso de un algoritmo llamado SARSA, muy similar a Q-learning](https://en.wikipedia.org/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action).\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/off-on-4.png\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae36bf1-e4ab-4f32-8f3c-df4ed18c548a",
   "metadata": {},
   "source": [
    "### 2.4.3 - Ejemplo de ejecución"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd92ad5-39c7-49ca-88aa-3a54ae1885b8",
   "metadata": {},
   "source": [
    "Para comprender mejor el funcionamiento del algoritmo Q-learning, consideremos el siguiente ejemplo.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/q-ex-1.jpg\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Las recompensas del \"juego\" son:\n",
    "* +0: ir a un estado que no tiene queso\n",
    "* +1: ir a un estado con queso\n",
    "* +10: ir a un estado con una gran pila de queso\n",
    "* -10: ir al estado con veneno\n",
    "\n",
    "Vamos a presentar un ejemplo con 3 instantes de tiempo (i.e., $t=0$, $t=1$, $t=2$)\n",
    "\n",
    "#### $t=0$:\n",
    "\n",
    "Solo podemos ejecutar el paso 1 del algoritmo (i.e., inicializar)\n",
    "\n",
    "##### **Paso 1: Inicializar tabla Q**\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/Example-1.png\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "#### $t=1$:\n",
    "\n",
    "Ejecutamos los pasos 2, 3 y 4 del algoritmo\n",
    "\n",
    "##### **Paso 2: Escoger una acción mediante la estrategia codiciosa $\\epsilon$**\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/q-ex-3.jpg\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "##### **Paso 3: Realizar acción $A_{t}$, obtener $R_{t+1}$ y $S_{t+1}$\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/q-ex-3.jpg\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "El ratón va a la derecha, obtiene queso (i.e., $R_{t+1} = 1$) y se encuentra en un nuevo estado.\n",
    "\n",
    "##### **Paso 4: Actualizar $Q(S_{t}, A_{t})$**\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/q-ex-5.jpg\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/Example-4.png\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "#### $t=2$:\n",
    "\n",
    "Ejecutamos los pasos 2, 3 y 4 del algoritmo\n",
    "\n",
    "##### **Paso 2: Escoger una acción mediante la estrategia codiciosa $\\epsilon$**\n",
    "\n",
    "Dado que epsilon es todavia muy alto (i.e., $0.99$), el ratón toma una acción aleatoria de nuevo, la cual le lleva al veneno y por tanto terminaria el episodio.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/q-ex-6.jpg\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "##### **Paso 3: Realizar acción $A_{t}$, obtener $R_{t+1}$ y $S_{t+1}$\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/q-ex-7.jpg\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "##### **Paso 4: Actualizar $Q(S_{t}, A_{t})$**\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/q-ex-8.jpg\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "En este momento, terminaria el episodio, pero el entrenamiento no acaba ahi. Empezariamos uno nuevo con la información aprendida de tal forma que el agente es más inteligente ahora que al principio de nuestro entrenamiento (sabe que el queso es bueno y que el veneno no lo se). **Según vayamos explroando y explotando el entorno, la tabla Q nos irá dando mejores y mejores aproximaciones de la política óptima.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ffd42d-9f0a-47f9-8d98-527182349b3a",
   "metadata": {},
   "source": [
    "## 2.5 - Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb910ed-f74f-413e-8e10-3d0dfc5927c8",
   "metadata": {},
   "source": [
    "## 2.6 - Construyendo un agente mediante una red DQNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54613f5-0914-48ad-bd32-f78742f4d45e",
   "metadata": {},
   "source": [
    "### 2.6.1 - Importación de librerías"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215f72c4-46f0-4b9b-9030-02b1a0a9ff95",
   "metadata": {},
   "source": [
    "### 2.6.2 - Construcción del agente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e45b49-5ef2-4c08-bd3c-e5e957af5bec",
   "metadata": {},
   "source": [
    "### 2.6.3 - Construcción del método `main()`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
