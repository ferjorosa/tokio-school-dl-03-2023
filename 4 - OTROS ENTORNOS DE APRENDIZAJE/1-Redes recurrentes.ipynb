{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bad60f56-bf1c-4745-9619-dc3cfd2134d7",
   "metadata": {},
   "source": [
    "# 1 - Redes recurrentes orientadas al procesamiento de lenguaje natural\n",
    "\n",
    "**Sumario**\n",
    "\n",
    "1. Introducción\n",
    "2. Procesamiento de lenguaje natural\n",
    "3. Redes neuronales recurrentes (RNN)\n",
    "4. Encoders - Decoders\n",
    "5. Modelos de atención\n",
    "6. Transformers\n",
    "7. Construcción de una red recurrente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b7064e-f1ce-4b06-8a65-b9cd7f5d1c02",
   "metadata": {},
   "source": [
    "## 1.1 - Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6380cd-4d67-450b-8d3c-480d8136aed3",
   "metadata": {},
   "source": [
    "En los capítulos anteriores hemos estudiado modelos de razonamiento basados en redes neuronales capaces de predecir, por ejemplo,  si un cierto animal se encuentra en una fotografía o si una casa de segunda mano se puede vender por un determinado precio.\n",
    "\n",
    "En ambos casos, nuestros modelos trataban de identificar patrones durante la fase de entrenamiento mediante la asuncion de que las instancias  de entrenamiento eran individuales, independientes e identicamente distribuidas (**i.i.d**). Es decir, que **no había concepto de tiempo**.\n",
    "\n",
    "No obstante, hay determinados procesos donde esta asunción no se cumple, ya que la información de que cada una de ellas representa depende considerablemente de las instancias anteriores y posteriores desde una perspectiva temporal. Por ejemplo, cuando estamos utilizando el lenguaje natural, el significado de cada una de las palabras que usamos depende en gran medida tant de las palabras anteriores, como de las posteriores e, incluso, del contexto o de la entonación de la persona que está hablando.\n",
    "\n",
    "El lenguaje natural no es la única área donde tiene importancia el tiempo, otras tareas de ML incluyen:\n",
    "* Composición musical\n",
    "* Predicción de precio en el stock Market\n",
    "* Conducción automática de vehiculos \n",
    "* etc.\n",
    "\n",
    "Con todo, entre todas las áreas donde las redes neuronales recurrentes han destacado significativamente, sobresale el área del **procesamiento de lenguaje natural** debido a su relación directa el ser humano y el conocimiento. \n",
    "\n",
    "El procesamiento de lenguaje natural es un área relativamente moderna dada la dificulta que supone interpretar un texto complejo o una conversación. A pesar de esto, ha evolucionado a una velocidad pasmosa, sobretodo gracias a las redes neuronales.\n",
    "\n",
    "Durante los primeros años, los diferentes métodos que se utilizaban estaban totalmente orientados al análisis de textos mediante **\"bolsas de palabras\"** (i.e., *Bag of words*, BOW por sus singlas en inglés). Así, cada documento equivalía a una bolsa y se utilizaba como entrada del algoritmo de aprendizaje con el objetivo de construir modelos de identificación. En aquel momento, las tareas principales de investigación eran:\n",
    "* Búsqueda de textos en documentos.\n",
    "* Agrupación de documentos similares.\n",
    "* Clasificación de textos (e.g., noticias, historia, positivo-negativo, etc.).\n",
    "\n",
    "Con la aparición de las redes neuronales recurrentes y de las capas tipo *embedding* (representaciones númericas del texto en forma matricial), se expandió enormemente el campo, permitiendo no solo mejores resultados en texto escrito, sino también el procesamiento de lenguaje natural hablado.\n",
    "\n",
    "Ya en 2017, surge una nueva arquitectura de red neuronal que pone el campo patas arriba denominada ***transformer***. Donde se sustituyen las capas de tipo recurrente por un nuevo tipo denominado **capa de atención**. Esta capa permite codificar cada palabra en función del resto de la frase o secuencia **sin incurrir en los problemas de \"olvido\"** que muestran las redes recurrentes. Para ello, uso embeddings contextuales, que introducen el contexto en la representación matemática del texto dentro de la red.\n",
    "\n",
    "A lo largo de este capítulo, describiremos aquellos conceptos básicos asociados al procesamiento del lenguaje natural que son necesarios para la construcción de redes de neuronas profundas de tipo recurrente. A continuación, nos ocuparemos del concepto de redes recurrentes e identificaremos los diferentes tipos de capas (recurrentes, LTSM, GRU, atención), así como las distintas arquitecturas destinadas a la construcción de modelos que permitan procesar series temporales y texto tanto hablado como escrito."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da7eb49-ff34-4b9f-9a75-470eb48c5fb9",
   "metadata": {},
   "source": [
    "## 1.2 - Procesamiento de lenguaje natural"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b05a0f7-6d41-4491-8ca4-e22981cbfcb7",
   "metadata": {},
   "source": [
    "El procesamiento del lenguaje natural (*Natural Language Processing*, NLP por sus siglas en inglés) es un área de la inteligencia artificial centrada en crear sistemas capaces de interacturar con seres humanos en su mismo lenguaje, tanto de manera escrita como hablada (en este sentido, el lenguaje hablado debe ser primero transformado a lenguaje escrito mediante técnicas de reconocimiento de sonido).\n",
    "\n",
    "Por tanto el área de NLP surge como la combinación de la lingüística y la inteligencia artificial, dando lugar a dos linear de trabajo:\n",
    "\n",
    "* **Generación de lenguaje natural** (*Natural Language Generation*, NLG por sus siglas en inglés).\n",
    "* **Entendimiento de lenguaje natural** (*Natural Language Understanding*, NLU por sus siglas en inglés). \n",
    "\n",
    "El procesamiento de lenguaje natural se aplica en multitud de aplicaciones:\n",
    "* Elaboración de resúmenes de texto.\n",
    "* Sugerencias de palabras en los procesos de escritura para mensajes o emails.\n",
    "* Traducción de textos.\n",
    "* Reconocimiento de entidades como marcas, lugares, empresas, celebridades, etc.\n",
    "* Análisis de sentimientos con el objetivo de identificar el \"tono\" de un texto (e.g., positivo o negativo)\n",
    "* etc.\n",
    "\n",
    "El procesamiento del lenguaje nautural comenzó a aplicarse mediante enfoques basados en reglas definidas por expertos en lingüística. Pero como hemos visto en capítulos anteriores, las redes neuronales necesitan representaciones numéricas. De manera que se desarollaron dos enfoques capaces de representar la información textual de manera numérica.\n",
    "\n",
    "**Enfoque léxico**, donde la información era modelada según su frecuencia de aparición en el texto. Esto puede ser de manera individual o agrupando grupos de palabras (i.e., *bag of words*).  En tal caso, se hace algo similar a una **codificación one-hot que asocia el grupo con frecuencia**.\n",
    "\n",
    "**Enfoque semántico**, donde la información se modela en base a un **vector numérico multidimensional** que describe cada palabra a partir de unas \"características\" identificadas por una red neuronal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fe4bba-e31d-45af-a09d-9e83aabd15ca",
   "metadata": {},
   "source": [
    "### 1.2.1 - Bags of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b1e4a5-5fc8-419b-9741-f0a58d8893bd",
   "metadata": {},
   "source": [
    "El proceso de generación de una bolsa de palabras, se fundamente en el siguiente proceso:\n",
    "\n",
    "1. Se realiza una **\"tokenización\" del documento**, es decir, se extraen todas las palabras del documento (o subpalabras).\n",
    "\n",
    "2. Se aplica un método de **conteo de palabras**. Dicho resultado puede ser normalizado. Por ejemplo mediante el uso de la técnica TF-IDF (Term Frequency - Inverse Document Frequency), que ajusta la frecuencia de un término (TF) mediante su frecuencia en **todos** los documentos. De tal forma que aquellas palabras que aparecen mucho en un documento pero también aparecen en otros documentos no tienen tanto importancia (e.g., \"el\", \"la\", \"yo\", etc.)\n",
    "\n",
    "Para explicar brevemente esta técnica, consideremos que tenemos 3 \"documentos\" que representan reviews sobre películas:\n",
    "\n",
    "* **Review 1:** This movie is very scary and long\n",
    "* **Review 2:** This movie is not scary and is slow\n",
    "* **Review 3:** This movie is spooky and good\n",
    "\n",
    "<img src=\"images_1/tf_idf_example.png\" width=\"500\" data-align=\"center\">\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{TF}_{d}(t) &= \\frac{N_{d}(t)}{N_{d}} \\\\\n",
    "\\text{IDF}(t) &= \\log \\frac{D}{D(t)} \\\\\n",
    "\\text{TF-IDF}(t,d) &= \\text{TF}_{d}(t) * \\text{IDF}(t)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "donde:\n",
    "* $N_{d}(t)$ representa el número de apariciones del término $t$ en el documento $d$.\n",
    "* $N_{d}$ representa el número total de términos en el documento $d$.\n",
    "* $D$ representa el número total de documentos.\n",
    "* $D(t)$ representa el número de documentos con el término $t$.\n",
    "\n",
    "Inicialmente, este tipo de técnica se convirtió en una buena solución, pero tiene dos principales limitaciones:\n",
    "* No se recoge ningún tipo de información relativa al orden o al **contexto de las palabras** dentro del documento.\n",
    "* La **representación *sparse*** resultante aumenta cuadráticamente con respecto al tamaño del vocabulario y el número de documentos, lo que lo hace **computacionalmente problemático**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90311c11-76b3-4d53-9c9c-8a5ac1e8f267",
   "metadata": {},
   "source": [
    "### 1.2.2 - Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2911e27-c97d-456f-a521-1f5e8dc1fe7b",
   "metadata": {},
   "source": [
    "Con el objetivo de eliminar las limitaciones que presentaba el sistema basado en bolsas de palabras, tanto en lo que se refiere a la calidad de la información como a su tamaño de representación se desarrolló el sistema de *embeddings*.\n",
    "\n",
    "Esta técnica busca construir una representación más compacta (i.e., densa) que nos permita reflejar mejor la información, así como las relaciones entre las diferentes palabras.\n",
    "\n",
    "De esta forma, se asigna a cada palabra (independientemente a que documento pertenezca) un vector continuo de $n$ dimensiones. La distancia existente en este espacio vectorial entre palabras denota similitud semántica.\n",
    "\n",
    "Por ejemplo, imaginemos que tenemos un embedding de 50 dimensiones con un vocabulario de 20.000 palabras en inglés. Entre ellas podria estar la palabra *king* con el siguiente vector de valores que oscilan entre $1.6$ y $-1.6$:\n",
    "\n",
    "<img src=\"images_1/embedding_king.jpg\" width=\"700\" data-align=\"center\">\n",
    "\n",
    "Existen diversas técnicas para la construcción de embeddings. La mayor parte de ellas utilizan aprendizaje automático con redes neuronales. En este sentido, **el objetivo es \"rellenar\" la matriz de valores de tamaño $V * n$**, donde $V$ es el tamaño del vocabulario y $n$ el número de dimensiones a considerar.\n",
    "\n",
    "Una técnica muy conocida es ***Word2Vec***, la cual consiste en una red neuronal con dos capas:\n",
    "* Una capa de proyección (i.e., el *embedding*) que representa la matriz anterior. Inicialmente con valores aleatorios.\n",
    "* Una capa densa con una función de activación *softmax*. El número de conexiones es igual al número de dimensiones en nuestra capa de embedding, es decir, $n$.\n",
    "\n",
    "La manera en la cual se aprende esta red neuronal es mediante una tecnica de predicción de palabras en el corpus. Es decir, partimos de una o varias palabras del texto y queremos que el modelo prediga una o varias palabras cercanas.\n",
    "\n",
    "Existen dos arquitecturas específicas del tipo ***Word2Vec***, dependiendo de la entrada y del tipo específico de salida que queramos generar:\n",
    "\n",
    "* [**Bolsa de palabras continua (CBOW)**](https://www.kdnuggets.com/2018/04/implementing-deep-learning-methods-feature-engineering-text-data-cbow.html). Se predice una palabra a partir del contexto. Queremos que el modelo modifique los valores del embedding de tal manera que pueda predecir correctamente cual es la palabra del vocabulario a partir de sus palabras anteriores y posteriores.\n",
    "\n",
    "* [***Skip-gram***](https://www.kdnuggets.com/2018/04/implementing-deep-learning-methods-feature-engineering-text-data-skip-gram.html). Funciona de manera opuesta a CBOW, ya que nos permite predecir el contexto a partir de un determinado término.\n",
    "\n",
    "<img src=\"images_1/word2vec_architectures.png\" width=\"500\" data-align=\"center\">\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Ejemplo CBOW</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images_1/cbow.jpg\" width=\"700\" data-align=\"center\">></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Ejemplo Skip-gram</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images_1/skipgram_example.png\" width=\"300\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Presentar un ejemplo de skip-gram es más complicado ya que es menos visual. En este caso, hemos definido un tamaño de contexto de 1, de manera que dada una palabra, predecimos si otra tiene sentido como \"contexto\" de la primera.\n",
    "\n",
    "----\n",
    "\n",
    "**Nota:** [Google ofrece modelos ya entrenados de Word2Vec que contienen embeddings de 300 dimensiones para tres millones palabras y frases](https://code.google.com/archive/p/word2vec/).\n",
    "\n",
    "----\n",
    "\n",
    "Además de Word2Vec existen otros modelos similares:\n",
    "\n",
    "* **GloVe**. Se basa en el conteo. De tal manera que se determina cuántas veces aparece una palabra en un contexto específico.\n",
    "* **LexVec**. Combina GloVe y Word2Vec. Existen diferentes versiones ya que ha ido evolucionando con el tiempo (a diferencia de los anteriores).\n",
    "\n",
    "Poco a poco, los sistemas de embedding comenzaron a utilizarse no solo por su cuenta (para medir la distancia entre palabras) sino también como capas de redes neuronales más complejas, lo que supuso una mejora en las tareas de NLP, especialmente cuando dichos embeddings ya estaban pre-entrenados (como el caso propuesto por Google)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939da059-e2e8-456f-a3c4-e3f23a6579e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.3 - Redes neuronales recurrentes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166bf3bd-e4d9-4cd1-a196-b73841a65bb3",
   "metadata": {},
   "source": [
    "Las redes neuronales recurrentes (*Recurrent Neural Networks*, RNNs por sus siglas en inglés) son un tipo de redes de neuronas que contienen, al menos, un ciclo dentro de sus conexiones de red. Es decir, **algunas de las neuronas de la red utilizan su propia salida o la salida de neuronas anteriores a ellas en la estructura de la red**.\n",
    "\n",
    "De esta forma, **la información ya no fluye en una única dirección**, como observábamos en las redes de tipo *feed-forward*, sino en ambos sentidos, hacia delante y parcialmente hacia atrás.\n",
    "\n",
    "La utilización de los procesos de retroalimentación de la salida permite a este tipo de redes contar con **ciertas características de \"memoria\"** en tareas de NLP.\n",
    "\n",
    "En base a su complejidad, podemos dividir las RNNs en dos grandes categorías:\n",
    "\n",
    "* RNNs simples.\n",
    "* RNNs complejas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc36f381-aad2-4632-a290-422958a4ebaf",
   "metadata": {},
   "source": [
    "### 1.3.1 - Redes recurrentes simples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a55672-7f00-43d1-bd86-be5aad69e542",
   "metadata": {},
   "source": [
    "Las redes recurrentes simples poseen una retroalimenación sencilla ya que solo incluyen el resultado de la neurona inmediatamente anterior.\n",
    "\n",
    "En la siguiente figura se muestra el funcionamiento de una red recurrente representada de manera compacta y desenrollada para cada instante de tiempo $t$:\n",
    "\n",
    "<img src=\"images_1/rnn_simple.png\" width=\"500\" data-align=\"center\">\n",
    "\n",
    "Por tanto, para el instante de tiempo $t=0$, la neurona solo utiliza como entrada la información externa, pero a partir de instante de tiempo $t=1$, la neurona utiliza como entrada la información externa como la salida para el instante de tiempo $t-1$.\n",
    "\n",
    "A continuación se presenta una versión esquematizada de una neurona recurrente simple:\n",
    "\n",
    "<img src=\"images_1/neurona_simple.png\" width=\"300\" data-align=\"center\">\n",
    "\n",
    "Como se puede observar en la siguiente figura, la neurona ya no solo usa la información de entrada, sino tambien el resultado obtenido para la entrada anterior, definido como $h_{t-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d509a8-2bf9-47df-b8b9-66640ba95801",
   "metadata": {},
   "source": [
    "#### Funcionamiento de una red recurrente simple\n",
    "\n",
    "Para explicar de manera sencilla su funcionamiento, vamos a desenrollar una red recurrente sencilla formada por una única neurona en una secuencia temporal de $T$ pasos, comenzando con el paso anterior al actual (i.e., $t-1$).\n",
    "\n",
    "<img src=\"images_1/rnn_simple_explained.png\" width=\"500\" data-align=\"center\">\n",
    "\n",
    "En este ejemplo, se pueden observar tres capas de neuronas:\n",
    "\n",
    "* **Capa de entrada**, que se corresponde con la información de entrada a la red y que, en el caso de NLP suele ser una palabra, la cual suele ser traducida a un vector numérico mediante un *embedding*. Se indica como $x(t)$.\n",
    "\n",
    "* **Un número finito de capas densas ocultas**. En este caso mostramos una única capa oculta, pero la información de entrada y el estado anterior podrian ser combinados y pasar por múltiples de ellas antes de ser expulsados. La salida de esta capa será utilizada como entrada de la capa siguiente, representa el \"estado\" de la red o \"memoria\". Se indica como $h(t)$.\n",
    "\n",
    "* **Capa de salida**. La salida de la red para el instante $t$. Por ejemplo, si utilizamos una RNN para predecir el token más probable a partir de una frase tendriamos en cada momento algo de este estilo:\n",
    "\n",
    "<img src=\"images_1/rnn_simple_example.jpg\" width=\"700\" data-align=\"center\">\n",
    "\n",
    "Los diferentes elementos matemáticos que se presentan son:\n",
    "* $x(t)$, $h(t)$ e $y(t)$ se corresponden con la entrada, el estado oculto y la salida de la red, respectivamente.\n",
    "* $W_{hh}$ es la matriz de peso que conecta las neuronas de la capa oculta consigo mismas simulando el proceso de memoria a corto plazo.\n",
    "* $W_{xh}$ es la matriz de peso que conecta las neuronas de la capa de entrada con las neuronas de la capa oculta.\n",
    "* $W_{hy}$ es la matriz de peso que conecta las neuronas de la capa oculta con las neuronas de la capa de salida."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524ae447-b270-40b0-8376-db1177c895de",
   "metadata": {},
   "source": [
    "### 1.3.2 - Redes recurrentes complejas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1fc45e-5986-463e-a29a-5cc97443790e",
   "metadata": {},
   "source": [
    "El problema de las RNN simples es que pueden sufrir el problem del \"desvanecimiento de gradiente\". Esto ocurre porque cuanto mayor sea el número de instantes de tiempo que tengamos, mas \"larga\" será la red.\n",
    "\n",
    "El problema del desvanecimiento de gradiente se muestra en el hecho de que las RNNs simples se \"olvidan\" de información a largo plazo. Por ejemplo, en el ámbito de NLP, **la información del inicio de la frase/párrafo tiende a ser \"olvidada\"** y solo se centran en las palabras más cercanas a la que esta siendo evaluada en ese momento.\n",
    "\n",
    "Para paliar este problema, se desarrollaron dos neuronas más complejas:\n",
    "\n",
    "* *Long Short-Term Memory* (LSTM).\n",
    "* *Gated Recurrent Unit* (GRU).\n",
    "\n",
    "----\n",
    "\n",
    "**Nota:** En esta sección solo vamos comentar brevemente ambas arquitecturas. [**Recomiendo leer el capítulo 12 del libro de FastAI (gratuito)**](https://github.com/fastai/fastbook/blob/master/12_nlp_dive.ipynb)\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659fbf08-5824-4d0c-add3-1dd060f3b71a",
   "metadata": {},
   "source": [
    "#### Long Short-Term Memory (LSTM)\n",
    "\n",
    "Las neuronas de tipo ***Long Short-Term Memory*** (LSTM) fueron introducidas por Hochreiter y Schmidhuber en 1997 con el propósito de resolver estos problemas de dependencias a largo plazo.\n",
    "\n",
    "Este nuevo tipo de neurona cuenta con no uno sino dos estados ocultos:\n",
    "* El estado oculto $h$ que se encarga de combinar la información de \"corto plazo\" con el input del token actual.\n",
    "* El estado oculto $c$ (i.e., *cell state*) que se encarga de almacenar la información de \"largo plazo\".\n",
    "\n",
    "<img src=\"images_1/lstm_diagram.png\" width=\"450\" data-align=\"center\">\n",
    "\n",
    "La primera puerta se llama **la puerta del olvido** (*forget gate*). Dado que es una capa lineal seguida de un sigmoide, su salida consistirá en escalares entre 0 y 1. Multiplicamos este resultado por el estado de la celda para determinar qué información conservar y cuál descartar: los valores más cercanos a 0 se \"descartan\". Esto le da al LSTM la capacidad de \"olvidar\" cosas sobre su estado a largo plazo.\n",
    "\n",
    "La segunda puerta se llama **puerta de entrada** (*input gate*). Cuenta con una \"puerta interna\", que a veces se denomina **puerta de celda** (*cell gate*), para actualizar el estado de la celda. Similar a la puerta de olvido, la puerta de entrada decide qué elementos del estado de la celda actualizar (valores cercanos a 1) y la puerta de la celda determina cuáles son esos valores de actualización, en el rango de -1 a 1 (tanh). Por ejemplo, es posible que veamos un nuevo pronombre de género, en cuyo caso necesitaremos reemplazar la información sobre el género que la puerta de olvido \"eliminó\" (poner cerca de 0).\n",
    "\n",
    "La última puerta es la **puerta de salida** (*output gate*). Determina qué información del estado de la celda usar para generar la salida (para la siguiente palabra). el estado de la celda pasa por un tanh antes de combinarse con la salida sigmoidea de la puerta de salida, y el resultado es el nuevo estado oculto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fc9470-7208-4acc-bf50-0ad35a2dfd2c",
   "metadata": {},
   "source": [
    "#### Gated recurrent units\n",
    "\n",
    "Las neuronas de tipo LSTM ofrecían grandes resultados, pero presentaban una elevada complejidad. Para simplificar\n",
    "su funcionamiento, se introdujeron las neuronas ***Gated Recurrent Units*** (GRU), que proporcionaban resultados similares a las neuronas LSTM, aunque con una estructura mucho más sencilla conformada únicamente por dos puertas y sin la necesidad de utilizar dos estados ocultos.\n",
    "\n",
    "<img src=\"images_1/gru_diagram.png\" width=\"450\" data-align=\"center\">\n",
    "\n",
    "La **puerta de reinicio** (*reset gate*) funciona de forma similar a la de olvido de la LSTM pero usa el estado oculto del instante anterior $h_{t-1}$.\n",
    "\n",
    "La **puerta nueva** (*new gate*) funciona de forma similar a la **puerta de entrada** de la LSTM. Se combina la información de entrada $x_{t}$ con el estado oculto del instante anterior $h_{t-1}$ (tras el reinicio).\n",
    "\n",
    "La **puerta de actualización** (*update gate*) selecciona el \"grado de importancia\" del estado oculto anterior $h_{t-1}$. Tiene ciertas similitudes con la puerta de reinicio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9349e7e8-67e9-41c5-8896-9de1505b436a",
   "metadata": {},
   "source": [
    "## 1.4 - Encoders-Decoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50f15d6-b0b6-48f6-945e-c5f35d0051b0",
   "metadata": {},
   "source": [
    "## 1.5 - Modelos de atención"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302050c3-18f8-4d92-b6f8-1d9813fba0cd",
   "metadata": {},
   "source": [
    "## 1.6 - Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ee8923-ac86-4452-96b6-f5eb33145a9f",
   "metadata": {},
   "source": [
    "## 1.7 - Construcción de una red recurrente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaa3c7e-7d47-4329-b77c-32142b7021bf",
   "metadata": {},
   "source": [
    "### 1.7.1 - Importación de la librería"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe53283-d891-44ef-aa68-3bab6d5fad3d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 1.7.2 - Carga de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e5811f-3ecc-4bf9-a5d8-65038b8d55ed",
   "metadata": {},
   "source": [
    "### 1.7.3 - Preparación del sistema de codificación de palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caee69c8-02a4-4fff-9e68-0f31f245c372",
   "metadata": {},
   "source": [
    "### 1.7.4 - Creación de la red recurrente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa390f29-f39a-460e-976f-cd1fcd75af00",
   "metadata": {},
   "source": [
    "### 1.7.5 - Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c59f62-e68f-4a7d-883e-1a18a9d10ad8",
   "metadata": {},
   "source": [
    "### 1.7.6 - Prueba del modelo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
