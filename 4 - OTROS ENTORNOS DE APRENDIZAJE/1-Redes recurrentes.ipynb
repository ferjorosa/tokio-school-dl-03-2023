{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bad60f56-bf1c-4745-9619-dc3cfd2134d7",
   "metadata": {},
   "source": [
    "# 1 - Redes recurrentes orientadas al procesamiento de lenguaje natural\n",
    "\n",
    "**Sumario**\n",
    "\n",
    "1. Introducción\n",
    "2. Procesamiento de lenguaje natural\n",
    "3. Redes neuronales recurrentes (RNN)\n",
    "4. Encoders - Decoders\n",
    "5. Modelos de atención\n",
    "6. Transformers\n",
    "7. Construcción de una red recurrente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b7064e-f1ce-4b06-8a65-b9cd7f5d1c02",
   "metadata": {},
   "source": [
    "## 1.1 - Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6380cd-4d67-450b-8d3c-480d8136aed3",
   "metadata": {},
   "source": [
    "En los capítulos anteriores hemos estudiado modelos de razonamiento basados en redes neuronales capaces de predecir, por ejemplo,  si un cierto animal se encuentra en una fotografía o si una casa de segunda mano se puede vender por un determinado precio.\n",
    "\n",
    "En ambos casos, nuestros modelos trataban de identificar patrones durante la fase de entrenamiento mediante la asuncion de que las instancias  de entrenamiento eran individuales, independientes e identicamente distribuidas (**i.i.d**). Es decir, que **no había concepto de tiempo**.\n",
    "\n",
    "No obstante, hay determinados procesos donde esta asunción no se cumple, ya que la información de que cada una de ellas representa depende considerablemente de las instancias anteriores y posteriores desde una perspectiva temporal. Por ejemplo, cuando estamos utilizando el lenguaje natural, el significado de cada una de las palabras que usamos depende en gran medida tant de las palabras anteriores, como de las posteriores e, incluso, del contexto o de la entonación de la persona que está hablando.\n",
    "\n",
    "El lenguaje natural no es la única área donde tiene importancia el tiempo, otras tareas de ML incluyen:\n",
    "* Composición musical\n",
    "* Predicción de precio en el stock Market\n",
    "* Conducción automática de vehiculos \n",
    "* etc.\n",
    "\n",
    "Con todo, entre todas las áreas donde las redes neuronales recurrentes han destacado significativamente, sobresale el área del **procesamiento de lenguaje natural** debido a su relación directa el ser humano y el conocimiento. \n",
    "\n",
    "El procesamiento de lenguaje natural es un área relativamente moderna dada la dificulta que supone interpretar un texto complejo o una conversación. A pesar de esto, ha evolucionado a una velocidad pasmosa, sobretodo gracias a las redes neuronales.\n",
    "\n",
    "Durante los primeros años, los diferentes métodos que se utilizaban estaban totalmente orientados al análisis de textos mediante **\"bolsas de palabras\"** (i.e., *Bag of words*, BOW por sus singlas en inglés). Así, cada documento equivalía a una bolsa y se utilizaba como entrada del algoritmo de aprendizaje con el objetivo de construir modelos de identificación. En aquel momento, las tareas principales de investigación eran:\n",
    "* Búsqueda de textos en documentos.\n",
    "* Agrupación de documentos similares.\n",
    "* Clasificación de textos (e.g., noticias, historia, positivo-negativo, etc.).\n",
    "\n",
    "Con la aparición de las redes neuronales recurrentes y de las capas tipo *embedding* (representaciones númericas del texto en forma matricial), se expandió enormemente el campo, permitiendo no solo mejores resultados en texto escrito, sino también el procesamiento de lenguaje natural hablado.\n",
    "\n",
    "Ya en 2017, surge una nueva arquitectura de red neuronal que pone el campo patas arriba denominada ***transformer***. Donde se sustituyen las capas de tipo recurrente por un nuevo tipo denominado **capa de atención**. Esta capa permite codificar cada palabra en función del resto de la frase o secuencia **sin incurrir en los problemas de \"olvido\"** que muestran las redes recurrentes. Para ello, uso embeddings contextuales, que introducen el contexto en la representación matemática del texto dentro de la red.\n",
    "\n",
    "A lo largo de este capítulo, describiremos aquellos conceptos básicos asociados al procesamiento del lenguaje natural que son necesarios para la construcción de redes de neuronas profundas de tipo recurrente. A continuación, nos ocuparemos del concepto de redes recurrentes e identificaremos los diferentes tipos de capas (recurrentes, LTSM, GRU, atención), así como las distintas arquitecturas destinadas a la construcción de modelos que permitan procesar series temporales y texto tanto hablado como escrito."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da7eb49-ff34-4b9f-9a75-470eb48c5fb9",
   "metadata": {},
   "source": [
    "## 1.2 - Procesamiento de lenguaje natural"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b05a0f7-6d41-4491-8ca4-e22981cbfcb7",
   "metadata": {},
   "source": [
    "El procesamiento del lenguaje natural (*Natural Language Processing*, NLP por sus siglas en inglés) es un área de la inteligencia artificial centrada en crear sistemas capaces de interacturar con seres humanos en su mismo lenguaje, tanto de manera escrita como hablada (en este sentido, el lenguaje hablado debe ser primero transformado a lenguaje escrito mediante técnicas de reconocimiento de sonido).\n",
    "\n",
    "Por tanto el área de NLP surge como la combinación de la lingüística y la inteligencia artificial, dando lugar a dos linear de trabajo:\n",
    "\n",
    "* **Generación de lenguaje natural** (*Natural Language Generation*, NLG por sus siglas en inglés).\n",
    "* **Entendimiento de lenguaje natural** (*Natural Language Understanding*, NLU por sus siglas en inglés). \n",
    "\n",
    "El procesamiento de lenguaje natural se aplica en multitud de aplicaciones:\n",
    "* Elaboración de resúmenes de texto.\n",
    "* Sugerencias de palabras en los procesos de escritura para mensajes o emails.\n",
    "* Traducción de textos.\n",
    "* Reconocimiento de entidades como marcas, lugares, empresas, celebridades, etc.\n",
    "* Análisis de sentimientos con el objetivo de identificar el \"tono\" de un texto (e.g., positivo o negativo)\n",
    "* etc.\n",
    "\n",
    "El procesamiento del lenguaje nautural comenzó a aplicarse mediante enfoques basados en reglas definidas por expertos en lingüística. Pero como hemos visto en capítulos anteriores, las redes neuronales necesitan representaciones numéricas. De manera que se desarollaron dos enfoques capaces de representar la información textual de manera numérica.\n",
    "\n",
    "**Enfoque léxico**, donde la información era modelada según su frecuencia de aparición en el texto. Esto puede ser de manera individual o agrupando grupos de palabras (i.e., *bag of words*).  En tal caso, se hace algo similar a una **codificación one-hot que asocia el grupo con frecuencia**.\n",
    "\n",
    "**Enfoque semántico**, donde la información se modela en base a un **vector numérico multidimensional** que describe cada palabra a partir de unas \"características\" identificadas por una red neuronal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fe4bba-e31d-45af-a09d-9e83aabd15ca",
   "metadata": {},
   "source": [
    "### 1.2.1 - Bags of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b1e4a5-5fc8-419b-9741-f0a58d8893bd",
   "metadata": {},
   "source": [
    "El proceso de generación de una bolsa de palabras, se fundamente en el siguiente proceso:\n",
    "\n",
    "1. Se realiza una **\"tokenización\" del documento**, es decir, se extraen todas las palabras del documento (o subpalabras).\n",
    "\n",
    "2. Se aplica un método de **conteo de palabras**. Dicho resultado puede ser normalizado. Por ejemplo mediante el uso de la técnica TF-IDF (Term Frequency - Inverse Document Frequency), que ajusta la frecuencia de un término (TF) mediante su frecuencia en **todos** los documentos. De tal forma que aquellas palabras que aparecen mucho en un documento pero también aparecen en otros documentos no tienen tanto importancia (e.g., \"el\", \"la\", \"yo\", etc.)\n",
    "\n",
    "Para explicar brevemente esta técnica, consideremos que tenemos 3 \"documentos\" que representan reviews sobre películas:\n",
    "\n",
    "* **Review 1:** This movie is very scary and long\n",
    "* **Review 2:** This movie is not scary and is slow\n",
    "* **Review 3:** This movie is spooky and good\n",
    "\n",
    "<img src=\"images_1/tf_idf_example.png\" width=\"500\" data-align=\"center\">\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{TF}_{d}(t) &= \\frac{N_{d}(t)}{N_{d}} \\\\\n",
    "\\text{IDF}(t) &= \\log \\frac{D}{D(t)} \\\\\n",
    "\\text{TF-IDF}(t,d) &= \\text{TF}_{d}(t) * \\text{IDF}(t)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "donde:\n",
    "* $N_{d}(t)$ representa el número de apariciones del término $t$ en el documento $d$.\n",
    "* $N_{d}$ representa el número total de términos en el documento $d$.\n",
    "* $D$ representa el número total de documentos.\n",
    "* $D(t)$ representa el número de documentos con el término $t$.\n",
    "\n",
    "Inicialmente, este tipo de técnica se convirtió en una buena solución, pero tiene dos principales limitaciones:\n",
    "* No se recoge ningún tipo de información relativa al orden o al **contexto de las palabras** dentro del documento.\n",
    "* La **representación *sparse*** resultante aumenta cuadráticamente con respecto al tamaño del vocabulario y el número de documentos, lo que lo hace **computacionalmente problemático**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90311c11-76b3-4d53-9c9c-8a5ac1e8f267",
   "metadata": {},
   "source": [
    "### 1.2.2 - Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2911e27-c97d-456f-a521-1f5e8dc1fe7b",
   "metadata": {},
   "source": [
    "Con el objetivo de eliminar las limitaciones que presentaba el sistema basado en bolsas de palabras, tanto en lo que se refiere a la calidad de la información como a su tamaño de representación se desarrolló el sistema de *embeddings*.\n",
    "\n",
    "Esta técnica busca construir una representación más compacta (i.e., densa) que nos permita reflejar mejor la información, así como las relaciones entre las diferentes palabras.\n",
    "\n",
    "De esta forma, se asigna a cada palabra (independientemente a que documento pertenezca) un vector continuo de $n$ dimensiones. La distancia existente en este espacio vectorial entre palabras denota similitud semántica.\n",
    "\n",
    "Por ejemplo, imaginemos que tenemos un embedding de 50 dimensiones con un vocabulario de 20.000 palabras en inglés. Entre ellas podria estar la palabra *king* con el siguiente vector de valores que oscilan entre $1.6$ y $-1.6$:\n",
    "\n",
    "<img src=\"images_1/embedding_king.jpg\" width=\"700\" data-align=\"center\">\n",
    "\n",
    "Existen diversas técnicas para la construcción de embeddings. La mayor parte de ellas utilizan aprendizaje automático con redes neuronales. En este sentido, **el objetivo es \"rellenar\" la matriz de valores de tamaño $V * n$**, donde $V$ es el tamaño del vocabulario y $n$ el número de dimensiones a considerar.\n",
    "\n",
    "Una técnica muy conocida es ***Word2Vec***, la cual consiste en una red neuronal con dos capas:\n",
    "* Una capa de proyección (i.e., el *embedding*) que representa la matriz anterior. Inicialmente con valores aleatorios.\n",
    "* Una capa densa con una función de activación *softmax*. El número de conexiones es igual al número de dimensiones en nuestra capa de embedding, es decir, $n$.\n",
    "\n",
    "La manera en la cual se aprende esta red neuronal es mediante una tecnica de predicción de palabras en el corpus. Es decir, partimos de una o varias palabras del texto y queremos que el modelo prediga una o varias palabras cercanas.\n",
    "\n",
    "Existen dos arquitecturas específicas del tipo ***Word2Vec***, dependiendo de la entrada y del tipo específico de salida que queramos generar:\n",
    "\n",
    "* [**Bolsa de palabras continua (CBOW)**](https://www.kdnuggets.com/2018/04/implementing-deep-learning-methods-feature-engineering-text-data-cbow.html). Se predice una palabra a partir del contexto. Queremos que el modelo modifique los valores del embedding de tal manera que pueda predecir correctamente cual es la palabra del vocabulario a partir de sus palabras anteriores y posteriores.\n",
    "\n",
    "* [***Skip-gram***](https://www.kdnuggets.com/2018/04/implementing-deep-learning-methods-feature-engineering-text-data-skip-gram.html). Funciona de manera opuesta a CBOW, ya que nos permite predecir el contexto a partir de un determinado término.\n",
    "\n",
    "<img src=\"images_1/word2vec_architectures.png\" width=\"500\" data-align=\"center\">\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Ejemplo CBOW</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images_1/cbow.jpg\" width=\"700\" data-align=\"center\">></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Ejemplo Skip-gram</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images_1/skipgram_example.png\" width=\"300\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Presentar un ejemplo de skip-gram es más complicado ya que es menos visual. En este caso, hemos definido un tamaño de contexto de 1, de manera que dada una palabra, predecimos si otra tiene sentido como \"contexto\" de la primera.\n",
    "\n",
    "----\n",
    "\n",
    "**Nota:** [Google ofrece modelos ya entrenados de Word2Vec que contienen embeddings de 300 dimensiones para tres millones palabras y frases](https://code.google.com/archive/p/word2vec/).\n",
    "\n",
    "----\n",
    "\n",
    "Además de Word2Vec existen otros modelos similares:\n",
    "\n",
    "* **GloVe**. Se basa en el conteo. De tal manera que se determina cuántas veces aparece una palabra en un contexto específico.\n",
    "* **LexVec**. Combina GloVe y Word2Vec. Existen diferentes versiones ya que ha ido evolucionando con el tiempo (a diferencia de los anteriores).\n",
    "\n",
    "Poco a poco, los sistemas de embedding comenzaron a utilizarse no solo por su cuenta (para medir la distancia entre palabras) sino también como capas de redes neuronales más complejas, lo que supuso una mejora en las tareas de NLP, especialmente cuando dichos embeddings ya estaban pre-entrenados (como el caso propuesto por Google)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939da059-e2e8-456f-a3c4-e3f23a6579e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.3 - Redes neuronales recurrentes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166bf3bd-e4d9-4cd1-a196-b73841a65bb3",
   "metadata": {},
   "source": [
    "Las redes neuronales recurrentes (*Recurrent Neural Networks*, RNNs por sus siglas en inglés) son un tipo de redes de neuronas que contienen, al menos, un ciclo dentro de sus conexiones de red. Es decir, **algunas de las neuronas de la red utilizan su propia salida o la salida de neuronas anteriores a ellas en la estructura de la red**.\n",
    "\n",
    "De esta forma, **la información ya no fluye en una única dirección**, como observábamos en las redes de tipo *feed-forward*, sino en ambos sentidos, hacia delante y parcialmente hacia atrás.\n",
    "\n",
    "La utilización de los procesos de retroalimentación de la salida permite a este tipo de redes contar con **ciertas características de \"memoria\"** en tareas de NLP.\n",
    "\n",
    "En base a su complejidad, podemos dividir las RNNs en dos grandes categorías:\n",
    "\n",
    "* RNNs simples.\n",
    "* RNNs complejas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc36f381-aad2-4632-a290-422958a4ebaf",
   "metadata": {},
   "source": [
    "### 1.3.1 - Redes recurrentes simples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a55672-7f00-43d1-bd86-be5aad69e542",
   "metadata": {},
   "source": [
    "Las redes recurrentes simples poseen una retroalimenación sencilla ya que solo incluyen el resultado de la neurona inmediatamente anterior.\n",
    "\n",
    "En la siguiente figura se muestra el funcionamiento de una red recurrente representada de manera compacta y desenrollada para cada instante de tiempo $t$:\n",
    "\n",
    "<img src=\"images_1/rnn_simple.png\" width=\"500\" data-align=\"center\">\n",
    "\n",
    "Por tanto, para el instante de tiempo $t=0$, la neurona solo utiliza como entrada la información externa, pero a partir de instante de tiempo $t=1$, la neurona utiliza como entrada la información externa como la salida para el instante de tiempo $t-1$.\n",
    "\n",
    "A continuación se presenta una versión esquematizada de una neurona recurrente simple:\n",
    "\n",
    "<img src=\"images_1/neurona_simple.png\" width=\"300\" data-align=\"center\">\n",
    "\n",
    "Como se puede observar en la siguiente figura, la neurona ya no solo usa la información de entrada, sino tambien el resultado obtenido para la entrada anterior, definido como $h_{t-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d509a8-2bf9-47df-b8b9-66640ba95801",
   "metadata": {},
   "source": [
    "#### Funcionamiento de una red recurrente simple\n",
    "\n",
    "Para explicar de manera sencilla su funcionamiento, vamos a desenrollar una red recurrente sencilla formada por una única neurona en una secuencia temporal de $T$ pasos, comenzando con el paso anterior al actual (i.e., $t-1$).\n",
    "\n",
    "<img src=\"images_1/rnn_simple_explained.png\" width=\"500\" data-align=\"center\">\n",
    "\n",
    "En este ejemplo, se pueden observar tres capas de neuronas:\n",
    "\n",
    "* **Capa de entrada**, que se corresponde con la información de entrada a la red y que, en el caso de NLP suele ser una palabra, la cual suele ser traducida a un vector numérico mediante un *embedding*. Se indica como $x(t)$.\n",
    "\n",
    "* **Un número finito de capas densas ocultas**. En este caso mostramos una única capa oculta, pero la información de entrada y el estado anterior podrian ser combinados y pasar por múltiples de ellas antes de ser expulsados. La salida de esta capa será utilizada como entrada de la capa siguiente, representa el \"estado\" de la red o \"memoria\". Se indica como $h(t)$.\n",
    "\n",
    "* **Capa de salida**. La salida de la red para el instante $t$. Por ejemplo, si utilizamos una RNN para predecir el token más probable a partir de una frase tendriamos en cada momento algo de este estilo:\n",
    "\n",
    "<img src=\"images_1/rnn_simple_example.jpg\" width=\"700\" data-align=\"center\">\n",
    "\n",
    "Los diferentes elementos matemáticos que se presentan son:\n",
    "* $x(t)$, $h(t)$ e $y(t)$ se corresponden con la entrada, el estado oculto y la salida de la red, respectivamente.\n",
    "* $W_{hh}$ es la matriz de peso que conecta las neuronas de la capa oculta consigo mismas simulando el proceso de memoria a corto plazo.\n",
    "* $W_{xh}$ es la matriz de peso que conecta las neuronas de la capa de entrada con las neuronas de la capa oculta.\n",
    "* $W_{hy}$ es la matriz de peso que conecta las neuronas de la capa oculta con las neuronas de la capa de salida."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524ae447-b270-40b0-8376-db1177c895de",
   "metadata": {},
   "source": [
    "### 1.3.2 - Redes recurrentes complejas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1fc45e-5986-463e-a29a-5cc97443790e",
   "metadata": {},
   "source": [
    "El problema de las RNN simples es que pueden sufrir el problem del \"desvanecimiento de gradiente\". Esto ocurre porque cuanto mayor sea el número de instantes de tiempo que tengamos, mas \"larga\" será la red.\n",
    "\n",
    "El problema del desvanecimiento de gradiente se muestra en el hecho de que las RNNs simples se \"olvidan\" de información a largo plazo. Por ejemplo, en el ámbito de NLP, **la información del inicio de la frase/párrafo tiende a ser \"olvidada\"** y solo se centran en las palabras más cercanas a la que esta siendo evaluada en ese momento.\n",
    "\n",
    "Para paliar este problema, se desarrollaron dos neuronas más complejas:\n",
    "\n",
    "* *Long Short-Term Memory* (LSTM).\n",
    "* *Gated Recurrent Unit* (GRU).\n",
    "\n",
    "----\n",
    "\n",
    "**Nota:** En esta sección solo vamos comentar brevemente ambas arquitecturas. [**Recomiendo leer el capítulo 12 del libro de FastAI (gratuito)**](https://github.com/fastai/fastbook/blob/master/12_nlp_dive.ipynb)\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659fbf08-5824-4d0c-add3-1dd060f3b71a",
   "metadata": {},
   "source": [
    "#### Long Short-Term Memory (LSTM)\n",
    "\n",
    "Las neuronas de tipo ***Long Short-Term Memory*** (LSTM) fueron introducidas por Hochreiter y Schmidhuber en 1997 con el propósito de resolver estos problemas de dependencias a largo plazo.\n",
    "\n",
    "Este nuevo tipo de neurona cuenta con no uno sino dos estados ocultos:\n",
    "* El estado oculto $h$ que se encarga de combinar la información de \"corto plazo\" con el input del token actual.\n",
    "* El estado oculto $c$ (i.e., *cell state*) que se encarga de almacenar la información de \"largo plazo\".\n",
    "\n",
    "<img src=\"images_1/lstm_diagram.png\" width=\"450\" data-align=\"center\">\n",
    "\n",
    "La primera puerta se llama **la puerta del olvido** (*forget gate*). Dado que es una capa lineal seguida de un sigmoide, su salida consistirá en escalares entre 0 y 1. Multiplicamos este resultado por el estado de la celda para determinar qué información conservar y cuál descartar: los valores más cercanos a 0 se \"descartan\". Esto le da al LSTM la capacidad de \"olvidar\" cosas sobre su estado a largo plazo.\n",
    "\n",
    "La segunda puerta se llama **puerta de entrada** (*input gate*). Cuenta con una \"puerta interna\", que a veces se denomina **puerta de celda** (*cell gate*), para actualizar el estado de la celda. Similar a la puerta de olvido, la puerta de entrada decide qué elementos del estado de la celda actualizar (valores cercanos a 1) y la puerta de la celda determina cuáles son esos valores de actualización, en el rango de -1 a 1 (tanh). Por ejemplo, es posible que veamos un nuevo pronombre de género, en cuyo caso necesitaremos reemplazar la información sobre el género que la puerta de olvido \"eliminó\" (poner cerca de 0).\n",
    "\n",
    "La última puerta es la **puerta de salida** (*output gate*). Determina qué información del estado de la celda usar para generar la salida (para la siguiente palabra). el estado de la celda pasa por un tanh antes de combinarse con la salida sigmoidea de la puerta de salida, y el resultado es el nuevo estado oculto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fc9470-7208-4acc-bf50-0ad35a2dfd2c",
   "metadata": {},
   "source": [
    "#### Gated recurrent units\n",
    "\n",
    "Las neuronas de tipo LSTM ofrecían grandes resultados, pero presentaban una elevada complejidad. Para simplificar\n",
    "su funcionamiento, se introdujeron las neuronas ***Gated Recurrent Units*** (GRU), que proporcionaban resultados similares a las neuronas LSTM, aunque con una estructura mucho más sencilla conformada únicamente por dos puertas y sin la necesidad de utilizar dos estados ocultos.\n",
    "\n",
    "<img src=\"images_1/gru_diagram.png\" width=\"450\" data-align=\"center\">\n",
    "\n",
    "La **puerta de reinicio** (*reset gate*) funciona de forma similar a la de olvido de la LSTM pero usa el estado oculto del instante anterior $h_{t-1}$.\n",
    "\n",
    "La **puerta nueva** (*new gate*) funciona de forma similar a la **puerta de entrada** de la LSTM. Se combina la información de entrada $x_{t}$ con el estado oculto del instante anterior $h_{t-1}$ (tras el reinicio).\n",
    "\n",
    "La **puerta de actualización** (*update gate*) selecciona el \"grado de importancia\" del estado oculto anterior $h_{t-1}$. Tiene ciertas similitudes con la puerta de reinicio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9349e7e8-67e9-41c5-8896-9de1505b436a",
   "metadata": {},
   "source": [
    "## 1.4 - Encoders-Decoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a6ad79-9b40-4f4a-96d8-f744546f05b6",
   "metadata": {},
   "source": [
    "Uno de los principales avances logrados en el área de NLP por las redes neuronales recurrentes fue la realización de traducciones de texto entre múltiples lenguajes con una alta veracidad. [El área de traducción automática comenzó en los años 50 con métodos basados en reglas, fue seguido por un periodo de métodos estadísticos y finalmente las redes neuronales (con su rotundo éxito)](https://medium.com/@kalyanks/overview-and-evolution-of-neural-machine-translation-444506f83600):\n",
    "\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_1/machine_translation_evolution.png\" width=\"500\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "En la siguiente figura se muestra la arquitectura básica de un sistema de traducción, que puede definirse como una metared formada por dos redes neuronales:\n",
    "* Red codificadora (*encoder*)\n",
    "* Red decodificadora (*decoder*)\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_1/enc_dec.png\" width=\"500\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "De manera formal, un sistema de traducción está formado por tres elementos básicos:\n",
    "* La **red codificadora** (*encoder*) procesa las palabras que forman la frase de entrada y propaga hacia adelante la información.\n",
    "* El **vector intermedio** (*encoder state*) se corresponde con la salida del codificador. Dicho estado constituye una representación comprimida de la información de entrada.\n",
    "* La **red decodificadora** (*decoder*) utiliza como entrada el estado intermedio y genera una serie de palabras que se corresponde con la traducción.\n",
    "\n",
    "En este sentido, es importante destacar que, cuando la secuencia de entrada es considerable, no es conveniente utilizar capas recurrentes simples pues pueden surgir problemas de desvanecimiento o explosión del gradiente. Por ello **es recomendable utilizar capas complejas como los LSTM y los GRU**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50f15d6-b0b6-48f6-945e-c5f35d0051b0",
   "metadata": {},
   "source": [
    "## 1.5 - Modelos de atención"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da5b818-b826-4c19-b99f-c5cca37b135d",
   "metadata": {},
   "source": [
    "Si bien las redes recurrentes complejas funcionan bastante bien dentro de una metared *encoder-decoder*. Muestran ciertos \"sesgos\" o limitaciones para casos de complejos de generación (e.g., traducción, preguntas y respuestas, etc.). Una de estas limitaciones se observa en la **falta de consideración por la posición de las palabras en la frase**.\n",
    "\n",
    "Los sistemas de traducción iniciales no tenían en cuenta la importancia de las palabras en lo que se refiere a su posición, lo que daba lugar a traducciones de peor calidad o no del todo precisas. Consideremos el siguiente ejemplo de traducción\n",
    "* Español: *Se ha alcanzado un acuerdo*\n",
    "* Inglés: *An agreement has been reached*\n",
    "\n",
    "La dificultad de este ejemplo es que la palabra \"acuerdo\" se encuentra al final de la frase en español y al inicio de la frase en inglés. Para poder resolver este problema, en 2015, los investigadores de NLP se inspiraron en el **mecanismo humano de atención** que nos permite centrarnos en un conjunto determinado de información e ignorar aquella que, aparentemente, no es útil. Desde el punto de vista de un sistema de traducción automático, esto supone que el sistema se centrará en la parte relevante de la frase para generar la traducción correspondiente.\n",
    "\n",
    "El mecanismo de atención original consistía en **incluir un conjunto de pesos y asignarlos a los estados ocultos generados por la red codificadora en cada uno de los instantes de tiempo**. Así cada paso de la red decodificadora pasaba a recibir no 2, sino 3 entradas:\n",
    "* La palabra generada en el paso anterior\n",
    "* El estado oculto del paso anterior\n",
    "* El vector de contexto correspondiente a la atención, el cual se calculaba mediante una suma ponderada de los estados ocultos de la red codificadora:\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_1/attention_computation.png\" width=\"500\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Para calcular el *score*, podemos utilizar múltiples técnicas, la más simple es el **producto escalar** (i.e., *dot product*). Otras opciones son la **función bilineal** (i.e., *bilinear function*) o el **perceptrón multicapa** (i.e., *multi-layer perceptron*):\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_1/score_functions.png\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "----\n",
    "\n",
    "**Nota:**[ Aconsejo echar un ojo al trabajo de Lena Voita, explica muy bien el concepto de atención](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html#attention_intro)\n",
    "\n",
    "----\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><i>Encoder-decoder tradicional</i></th>\n",
    "        <th><i>Encoder-decoder con atención (global)</i></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images_1/attention_1.png\" width=\"500\" data-align=\"center\"></td>\n",
    "        <td><img src=\"images_1/attention_2.png\" width=\"500\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "Este modelo de atención se denomina **global** ya que estamos teniendo en cuenta los estados ocultos de todas las palabras de entrada. Si bien es efectivo, es complejo a nivel computacional. Por ello, autores posteriores proposieron el uso de atención **local**, donde solo se tienen en cuenta los estados ocultos de un subset de las palabras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302050c3-18f8-4d92-b6f8-1d9813fba0cd",
   "metadata": {},
   "source": [
    "## 1.6 - Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5160086f-2a0e-4c96-acf6-70a3cf05a0dc",
   "metadata": {},
   "source": [
    "La atención demostró ser una técnica muy poderosa a la hora de mejorar los sistemas de traducción o generación de respuestas. Sin embargo, estaba limitada al uso de redes recurrentes. Esto se mantuvo hasta que [en 2017 se publicó el trabajo de Vaswani (*Attention is all you need*)](https://arxiv.org/abs/1706.03762), donde se redefinía el concepto de atención.\n",
    "\n",
    "En dicho trabajo se propuso la arquitectura **Transformer**, la cual era capaz de operar **sin necesidad de recurrencia o convoluciones**, simplemente con atención. Esta nueva arquitectura fue un triunfo casi inmediato. No solo mostraba mejoras considerables en la calidad de traducción, sino que su aprendizaje era un grado de magnitud más rapido que las RNNs. Tal ha sido su éxito que los Transformers (con sus múltiples variaciones) son la arquitectura por defecto en la mayor parte de tareas de NLP.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_1/rnn_vs_transformer_comic.png\" width=\"500\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732bba8d-d5aa-4c56-a46e-8af10cab4ac7",
   "metadata": {},
   "source": [
    "#### Atención propia\n",
    "\n",
    "La razón principal de su éxito se encuentra un nuevo sistema de atención llamado **atención propia** (i.e., *self-attention*). Mediante la atención propia, las palabras de una frase se \"miran\" entre sí para comprender mejor el contexto. \n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_1/self_attention_visualization.png\" width=\"350\" data-align=\"center\"></td>\n",
    "        <td><img src=\"images_1/encoder_self_attention.gif\" width=\"400\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "* De forma intuitiva podemos entender el la red codificadora (*encoder*) como un conjunto de pasos, donde en cada paso un token mira al resto (y el resto a él) de forma que se \"entiendan\" mejor, lo que aumenta la calidad de codificación.\n",
    "\n",
    "* También podemos aplicar el concepto de *self-attention* a la red decodificadora (*decoder*). En este caso **tendriamos dos tipos de atención**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_1/two_types_attention.png\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b124d8-5bc2-49fb-951d-05b00deeff6b",
   "metadata": {},
   "source": [
    "#### Clave, consulta y valor en la atención propia\n",
    "\n",
    "Formalmente, el concepto de atención propia se implementa mediante tres tensores. Podemos entenderlocomo una especie de \"conversación\" entre las diferentes palabras de la frase:\n",
    "* Clave (*query*). Se utiliza cuando una palabra/token mira al resto para **entenderse mejor a sí mismo (su propio contexto)**.\n",
    "* Consulta (*key*). Se utiliza por el resto de tokens para **indicar que tienen la información pedida**. Sirve para computar los pesos (alto peso si la información que \"dicen\" tener es relevante).\n",
    "* Valor (*value*). **La información que los tokens tienen** y que \"dicen\" que es relevante.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_1/qkv_explained.png\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_1/qkv_attention_formula.png\" width=\"300\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fbd19f-5c40-479d-9fc2-3bcaccce8131",
   "metadata": {},
   "source": [
    "#### Atención propia enmascarada para el decodificador\n",
    "\n",
    "Como hemos comentado antes, la red decodificadora también tiene un mecanismo de atención propia, pero ésta difiera un poco del de la red codificadora. Si bien el codificador recibe todos las palabras a la vez y éstas se pueden mirar entre sí, en el decodificador generamos las palabras una a una (como una RNN) y los palabras solo pueden \"mirar\" a las anteriores ya que no saben que palabras van a venir después.\n",
    "\n",
    "Durante inferencia este proceso ocurre de forma natural porque la red decodificadora no sabe lo que va a generar en el futuro. Sin embargo, durante entrenamiento es más complicado porque recordemos que **el Transformer no tiene recurrencia**, es decir, todas las palabras son procesadas a la vez, por lo que si no limitamos al decodificador, éste haría trampas (podría ver el futuro) lo cual no es lo que queremos ya que no aprendería correctamente.\n",
    "\n",
    "Para **evitar que el decodificador \"vea el futuro\" durante el entrenamiento**, \"enmascaramos\" las palabras futuras de tal forma que no pueda considerarlas al calcular la atención:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_1/masked_self_attn.gif\" width=\"350\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae14b86-719e-4c91-ac34-88c1eeb1dec8",
   "metadata": {},
   "source": [
    "#### Multiples \"cabezas\" de atención \n",
    "\n",
    "Por lo general, comprender el papel de una palabra en una frase requiere comprender cómo se relaciona con las diferentes partes de la misma. Esto es importante no solo en el procesamiento de la frase en sí sino también en la generación del objetivo.\n",
    "\n",
    "Por ello, es necesario que permitamos a las palabras que se centren en múltiples partes de la frase para representar mejor su contexto. Este gan enorme importancia **según aumenta el tamaño del texto de entrada**. \n",
    "\n",
    "Se puede implementar formalmente mediante el uso de múltiples mecanismos de atención  (cada uno centrandose en secciones diferentes) cuyos resultados son combinados:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_1/multi_head.gif\" width=\"350\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341ca882-7600-4355-911e-6c97659e0e0a",
   "metadata": {},
   "source": [
    "#### Arquitectura de la red *Transformer*\n",
    "\n",
    "Ahora que comprendemos los principales componentes del modelo podemos ver como se relacionan entre sí en la arquitectura:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_1/transformer_architecture.png\" width=\"700\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Comentar que estamos mostrando la arquitectura \"original\" del Transformer el cual contenía tanto una red codificadora como una red decodificadora. Sin embargo, con el paso de los años han surgido múltiples variantes que solo utilizan o la red \n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_1/transformers_tree.png\" width=\"500\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f781c7-a207-4324-b374-050c7b3930ce",
   "metadata": {},
   "source": [
    "#### Lecturas recomendadas\n",
    "\n",
    "* [Implementación en Tensorflow de la arquitectura Transformer](https://www.tensorflow.org/text/tutorials/transformer)\n",
    "* [Blog post de Jalamar (\"Illustrated Transformer\")](https://jalammar.github.io/illustrated-transformer/)\n",
    "* [Blog de Lena Voita (\"NLP course for you\")](https://lena-voita.github.io/nlp_course.html)\n",
    "* [Capítulo 3 del libro de Lewis Tunstall (\"Natural Language Processing with Transformers: Building Language Applications with Hugging Face\")](https://transformersbook.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab4946b-217d-487b-a86f-dea808156e90",
   "metadata": {},
   "source": [
    "## 1.7 - Construcción de una red recurrente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999be5e2-faf8-4c9a-9060-5f7f8bf793af",
   "metadata": {},
   "source": [
    "A continuación veremos como implementar una red neuronal recurrente para predecir el \"sentimiento\" (e.g., positivo/negativo) de un conjunto de texto sobre valoraciones de películas cinematográficas.\n",
    "\n",
    "https://www.kaggle.com/code/sumishog/imdb-sentiment-analysis-keras-lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792e7556-898f-457d-87e0-13624a338634",
   "metadata": {},
   "source": [
    "### 1.7.1 - Importación de la librería"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "539634d1-f50b-47c0-9656-f73e643a29d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow.keras as keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96520024-746b-4dd9-bc07-48e8b57d1024",
   "metadata": {},
   "source": [
    "### 1.7.2 - Carga de datos\n",
    "\n",
    "En este caso vamos a utilizar uno de los *datasets* de ejemplo almacenados en Keras, `imdb_reviews`, donde se incluye un conjunto de críticas cinematográficas procedentes de la página web Imdb que han sido etiquetadas de manera positiva ($1$) o negativa ($0$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2421eb65-6a6d-46af-b27e-082237826465",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 25000\n",
      "Test size: 25000\n"
     ]
    }
   ],
   "source": [
    "dataset, info = tfds.load(\n",
    "    \"imdb_reviews\",\n",
    "    with_info=True,\n",
    "    as_supervised=True\n",
    ")\n",
    "\n",
    "df_train = tfds.as_dataframe(dataset[\"train\"], info)\n",
    "df_train[\"text\"] = df_train[\"text\"].astype(str)\n",
    "df_test = tfds.as_dataframe(dataset[\"test\"], info)\n",
    "df_test[\"text\"] = df_test[\"text\"].astype(str)\n",
    "\n",
    "print(f\"Train size: {df_train.shape[0]}\")\n",
    "print(f\"Test size: {df_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617b6bf8-872a-42d0-bf52-cc963ceda457",
   "metadata": {},
   "source": [
    "Es un dataset bastante grande (50000 instancias), así que para disminuir el tiempo de entrenamiento nos vamos a utilizar unicamente un subset de la partición \"train\" original con 500 instancias. Una vez que veamos que el modelo funciona correctamente y que nos hagamos una idea del tiempo de ejecución, podriamos aumentar el número de instancias consideradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f1d7e4d-109f-43e7-a9af-4bb42a6c7872",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_train.iloc[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2356560e-d1c6-4a18-b034-a8db1367d939",
   "metadata": {},
   "source": [
    "### 1.7.3 - Tokenización y *padding* del texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6da1013-5f2d-47f5-ad0c-74f9b8d22c46",
   "metadata": {},
   "source": [
    "Nuestro modelo de red neuronal **no puede recibir texto \"crudo\" como entrada**. Este tipo de modelos asumen que el texto ha sido tokenizado y codificado como vectores numéricos.\n",
    "\n",
    "La **tokenización** es el paso de preprocesamiento de textos que divide un string en \"unidades atómicas\" (e.g., palabras, sílabas, letras, etc.). Para ello vamos a utilizar la clase [`tf.keras.preprocessing.text.Tokenizer`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b9d4a8a-f0ee-4a32-809f-439b693a417c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(df[\"text\"])\n",
    "seq = tokenizer.texts_to_sequences(df[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575f0589-c424-4411-83de-eb957fd8c5de",
   "metadata": {},
   "source": [
    "Podemos acceder al diccionario mediante el atributo `word_index`. En este caso contamos con 12593 palabras en el diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01a61fa5-0b6d-4e18-9280-732a732c7ea1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12593\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[11, 6, 3, 2357]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(tokenizer.word_index))\n",
    "\n",
    "# Probamos una frase para ver que índices nos devuelve\n",
    "tokenizer.texts_to_sequences([\"This is a test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a54f37-afb4-4fd8-b98d-17c4ec364691",
   "metadata": {},
   "source": [
    "Una vez hemos generado nuestro diccionario de palabras y hemos tokenizados los textos del dataset. Pasamos  homogenizar la longitud de las diferentes reviews que componen el dataset. Este paso se denomina en inglés *padding*. \n",
    "* En caso de que el texto sea más corto que el valor introducido, la expande con valores por defecto que son ignorados durante el entrenamiento.\n",
    "* En caso de que el texto sea más larga que el valor introducido, lo acorta con respecto al tamaño indicado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91b5ee8e-6045-4bfe-b4ac-fdca8e5da478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_shape: (500, 80), X_min: 0, X_max: 12567\n"
     ]
    }
   ],
   "source": [
    "X = pad_sequences(seq, maxlen=80, padding=\"post\", truncating=\"post\") # \"post\" indica que queremos que corte o expanda por la parte final del texto\n",
    "print(f'X_shape: {X.shape}, X_min: {np.min(X)}, X_max: {np.max(X)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef62f416-3b16-495a-a239-4bb4cb6b6ed4",
   "metadata": {},
   "source": [
    "Tenemos 500 ejemplos con una longitud total de 80 palabras y vemos que el número de valores posibles va desde 0 a 12567, un poco menor que nuestro vocabulario por el corte que se ha realizado para ajustar.\n",
    "\n",
    "Ahora modificamos la variable objetivo para que sea más entendible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4102b005-6317-4a1e-8f39-8050fa3cc839",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = df[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8e29d6-612e-4568-9ed7-c4a4a5567c40",
   "metadata": {},
   "source": [
    "### 1.7.4 - Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e2c6d01-9656-4fc1-864e-e19b7d99b77e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 80) (400,) (100, 80) (100,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2484b43-986b-44aa-b78a-cd77ee741c83",
   "metadata": {},
   "source": [
    "### 1.7.5 - Creación de la red recurrente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94fbd05-7784-4c61-a65a-d709ffc6eda5",
   "metadata": {},
   "source": [
    "Tras la construcción del conjunto de entrenamiento y el codificador de palabras, debemos crear nuestra red neuronal recurrente. En este caso vamos a crear una muy sencilla, pues estará conformada por 6 capas:\n",
    "\n",
    "* Una primera **capa de codificación de palabras** que utilizara la capa del tipo `TextVectorization` que hemos creado previamente para transformar los textos en vectores con lo índices correspondientes a las palabras.\n",
    "* Una **capa de *embeddings*** de palabras a partir de los índices. Es decir, tendremos un vector numérico flotante para cada una de las palabras de nuestro vocabulario. Esta capa se puede entender a partir de sus dos atributos:\n",
    "    * `input_dim`. El número de palabras en nuestro vocabulario\n",
    "    * `output_dim`. El número de dimensiones de los vectores numéricos asociados a cada palabra del vocabulario\n",
    "* Una **capa bidireccional**, que nos permite definir conexiones hacia atrás de forma que nuestra red recurrente sea capaz de representar mejor el contexto.\n",
    "* Una **capa de tipo LSTM** formada por 64 unidades (el `output_dim` de nuestro embedding).\n",
    "* Dos **capas densas**:\n",
    "    1. La primera con 64 neuronas y una función de activación ReLU.\n",
    "    2. La segunda con 1 neurona y una función de activación sigmoide, destinada a generar el valor de las clasificación binaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b92362-2f46-44dc-a218-f518dd3d688d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3636738-dce2-41db-9b09-41f1c6b1894e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0599170b-2adf-4791-ae0e-c319180d5e37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b1b0fc-0aa8-43f1-bc03-957c2dc9ff97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcade0b3-454d-4d73-af4e-83431b2cc882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e245e1ae-cbfb-4d40-95e2-88e3609411da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551431a3-abe6-4ab2-a625-f111e84d6441",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32ee8923-ac86-4452-96b6-f5eb33145a9f",
   "metadata": {},
   "source": [
    "## 1.7 - Construcción de una red recurrente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bcd483-5322-4012-8812-c5eef146791e",
   "metadata": {},
   "source": [
    "A continuación veremos como implementar una red neuronal recurrente para predecir el \"sentimiento\" (e.g., positivo/negativo) de un conjunto de texto sobre valoraciones de películas cinematográficas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaa3c7e-7d47-4329-b77c-32142b7021bf",
   "metadata": {},
   "source": [
    "### 1.7.1 - Importación de la librería"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f6a984-ce30-4d43-a5c3-cd27f2a56f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe53283-d891-44ef-aa68-3bab6d5fad3d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.7.2 - Carga de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022a9747-ca03-44ae-af91-afbb5a975aa2",
   "metadata": {},
   "source": [
    "En este caso vamos a utilizar uno de los *datasets* de ejemplo almacenados en Keras, `imdb_reviews`, donde se incluye un conjunto de críticas cinematográficas procedentes de la página web Imdb que han sido etiquetadas de manera positiva o negativa.\n",
    "\n",
    "Para la implementación del proceso de carga y preparación de los datos, vamos a crear una función que utilizará dos parámetros:\n",
    "\n",
    "* ***batch_size* (int)**, que se corresponde con el tamaño del *batch* en el que será dividido el dataset.\n",
    "* ***buffer_size* (int)**, que se corresponde con el número de ejemplos seleccionados para crear tanto el conjunto de entrenamiento como el de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "96870487-c842-476f-a83e-b006892b0f92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_data_set(\n",
    "    batch_size,\n",
    "    buffer_size,\n",
    "    verbose=False\n",
    "):\n",
    "    dataset, info = tfds.load(\n",
    "        \"imdb_reviews\",\n",
    "        with_info=True,\n",
    "        as_supervised=True\n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        print(info)\n",
    "    \n",
    "    train_dataset = dataset[\"train\"]\n",
    "    test_dataset = dataset[\"test\"]\n",
    "    \n",
    "    train = train_dataset.shuffle(buffer_size).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    test = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e7e1f9-ca92-4954-98db-d4964e6805b9",
   "metadata": {},
   "source": [
    "Asi, cargamos los datos del dataset de Imdb y mostramos tres elementos junto (con sus respectivas etiquetas) del primer batch de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "635a6a41-a3ff-4fb4-899d-197aaf2cf2f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texts: [b'You know the story..Pretty kids alone in the woods,when BAM!something starts cutting them up.<br /><br />Well this crap is no different.A bunch of kids return to a cabin where the male leads twin brother disappeared for years before.Suddenly an \"UNKOWN CREATURE\" stars cutting them up,and their only help is a doctor/biker.<br /><br />To say this film was bad is an understatement,it\\'s smut! The acting was horrible.<br /><br />The creature looked very cheesy. And as all films do these days they try to get you with a twist ending,which they do not!<br /><br />There is one bright spot to this film- LOST star Maggie Grace as the female lead.'\n",
      " b'Five-year-old Michael sees his mother getting axed to death by his serial killer father \"The Highwayman,\" who later commits suicide. \"20 years later\" grown Mike (Gordon Currie, from PUPPET MASTER 4 and 5) invites seven of his friends to his secluded grandparents home to \"master their own fears\" at a Halloween night costume party. Morty, a life-size wooden doll kept in the attic by the Indian handyman, becomes possessed by the dead father\\'s spirit and kills them off using their phobias. Characters are thrown out a window, drowned in a toilet, eaten by rats, blown up, etc. Morty morphs into the dad and a tree, walks around and makes stupid wisecracks. After finding a girl chopped up and stuffed in a cardboard box, the characters remain in the house, act cheerful, crack jokes and have sex.<br /><br />The Morty design is good and Betsy Palmer (Mrs. Voorhees from the original Friday THE 13TH) is surprisingly delightful as the grandmother, but this thing is even more senseless and confusing than the original and is full of false scares, bad acting, brain-dead characters, repeat flashback footage and annoying distorted camera-work. Plus the only two minority characters (the Indian and a half-black girl) are the first to die. BLAH!'\n",
      " b\"Some kids are hiking in the mountains, and one of them goes into a large tunnel and discovers some old mummified gladiator. He puts on the gladiator's helmet and spends the rest of the movie killing all the other hikers.<br /><br />This thing is just so utterly senseless it's maddening. Here's a short list of things that don't make any sense:<br /><br />1) A guy and a girl are in their tent and they think they hear something outside. The guy goes out to investigate and finds another hiker outside. Then he hears his girlfriend scream so they head back to the tent - arriving the next morning?!? He was only 50 feet away!<br /><br />2) These two dunderheads then hear another girl scream (What, 100 feet away?), but don't investigate because they're afraid they'll get lost.<br /><br />3) Another guy and a girl are walking around, and in about their 10th scene together the girl informs the guy that due to the circumstances, protocol no longer requires him to address her as professor. I mean, what the...? First off, that's just a really stupid thing to say, secondly he never called her professor in the first nine scenes they were in together.<br /><br />4) A wounded girl attacks Demonicus and he stops her, telling her that part of his gladiator training taught him how to wound without killing. Um, yeah, we kinda noticed she's wounded and not dead because she's up and walking around. But, thanks for that tidbit of information.<br /><br />5) One girl is tied up in Demonicus' lair, and when someone attempts to free her, she instead instructs this person to go and get help. Um, look, idiot, if she set you free, which would take about 5 seconds, there would be no need to get help.<br /><br />And it just goes on and on. The whole middle part of the movie is spent with the two idiots getting lost in the woods, then they fight, then they pitch a tent and ignore the screams of their friends, then they wander around some more. It's just so damned boring and pointless that I turned the DVD off halfway through. <br /><br />None of these characters are sympathetic, especially the ones that get the majority of the screen time. Demonicus himself made me laugh out loud every time I saw him - he looks like a kid in a Halloween costume, scrunching his face up to look evil. He runs, or should I say scampers around like he's gay. The special effects are comedic, the acting is for the most part awful, and nothing makes any sense.<br /><br />Overall, maybe this concept could have produced an enjoyably campy film if they put some more time and effort into it, getting rid of the ludicrous dialogue, creating characters with actual likable personalities, having some sort of logical flow to the action, and maybe even making Demonicus a female character in a sexy gladiator outfit. But no, instead we get this senseless pile of nonsense that will bore you to death.\"] \n",
      "\n",
      "labels: [0 0 0]\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train, test = generate_data_set(BATCH_SIZE, BUFFER_SIZE)\n",
    "\n",
    "for example, label in train.take(1):\n",
    "    print(f\"texts: {example.numpy()[:3]} \\n\")\n",
    "    print(f\"labels: {label.numpy()[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e5811f-3ecc-4bf9-a5d8-65038b8d55ed",
   "metadata": {},
   "source": [
    "### 1.7.3 - Preparación del sistema de codificación de palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadd2f29-f60f-4aaa-9387-82c424a00e8c",
   "metadata": {},
   "source": [
    "Una vez cargado nuestro conjunto de datos, podremos crear el diccionario de palabras que nos servirá para definir la estructura de entrada de nuestra red neuronal. Es decir, la codificación nos permite crear un vector con un tamaño fijo por cada instancia y, en cada posición de éste, se incluirá un valor numérico con el identificador de la palabra correspondiente en el diccionario. \n",
    "\n",
    "En este diccionario estarán todas las palabras que podemos encontrarnos en el texto de entrenamiento. Además de éstas, podemos términos especiales como \"desconocido\" (`UNK`), \"end of sentence\" (`EOS`), \"máscara\" (`MASK`), etc.\n",
    "\n",
    "A continuación creamos nuestro \"codificador\" o \"generador de vector numérico\" y le pasamos el vocabulario de nuestro dataset de entrenamiento mediante la función `adapt()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa256204-6b6a-464e-96f2-9f488915b6a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 500\n",
    "\n",
    "word_encoder = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE\n",
    ")\n",
    "\n",
    "# Mediante el método adapt, extraemos el vocabulario a partir del texto de nuestro dataset\n",
    "word_encoder.adapt(train.map(lambda text, label: text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fc8d25-966f-4f7b-bbc7-a1d7638671cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mediante esta función podemos ver que palabras forman nuestro vocabulario\n",
    "# word_encoder.get_vocabulary()\n",
    "\n",
    "# Si pasamos el texto por el codificador de palabras, veremos los índices a los que se corresponde\n",
    "def encode_text(words):\n",
    "    return word_encoder(words)\n",
    "\n",
    "# Si hacemos el movimiento inverso deberiamos obtener las palabras originales\n",
    "# En este caso no va a ser asi porque hemos limitado el tamaño del vocabulario mucho\n",
    "# i.e., hay mas de 500 palabras únicas\n",
    "def decode_text_indices(indices):\n",
    "    return [word_encoder.get_vocabulary()[index] for index in indices]\n",
    "\n",
    "orginal_text = example.numpy()[0]\n",
    "encoded_text = encode_text(orginal_text)\n",
    "decoded_text = decode_text_indices(encoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caee69c8-02a4-4fff-9e68-0f31f245c372",
   "metadata": {},
   "source": [
    "### 1.7.4 - Creación de la red recurrente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5216b4e6-c8f1-40da-93c3-643eb272cadf",
   "metadata": {},
   "source": [
    "Tras la construcción del conjunto de entrenamiento y el codificador de palabras, debemos crear nuestra red neuronal recurrente. En este caso vamos a crear una muy sencilla, pues estará conformada por 6 capas:\n",
    "\n",
    "* Una primera **capa de codificación de palabras** que utilizara la capa del tipo `TextVectorization` que hemos creado previamente para transformar los textos en vectores con lo índices correspondientes a las palabras.\n",
    "* Una **capa de *embeddings*** de palabras a partir de los índices. Es decir, tendremos un vector numérico flotante para cada una de las palabras de nuestro vocabulario. Esta capa se puede entender a partir de sus dos atributos:\n",
    "    * `input_dim`. El número de palabras en nuestro vocabulario\n",
    "    * `output_dim`. El número de dimensiones de los vectores numéricos asociados a cada palabra del vocabulario\n",
    "* Una **capa bidireccional**, que nos permite definir conexiones hacia atrás de forma que nuestra red recurrente sea capaz de representar mejor el contexto.\n",
    "* Una **capa de tipo LSTM** formada por 64 unidades (el `output_dim` de nuestro embedding).\n",
    "* Dos **capas densas**:\n",
    "    1. La primera con 64 neuronas y una función de activación ReLU.\n",
    "    2. La segunda con 1 neurona y una función de activación sigmoide, destinada a generar el valor de las clasificación binaria.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c8e160-24d0-4c93-b5ab-ee043ecc5085",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layers = [\n",
    "    # Capa de codificación de palabras\n",
    "    word_encoder,\n",
    "    \n",
    "    # Capa de embeddings\n",
    "    keras.layers.Embedding(\n",
    "        input_dim=len(word_encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        mask_zero=True\n",
    "    ),\n",
    "    \n",
    "    # Capa bidireccional con una capa interna de tipo LSTM\n",
    "    keras.layers.Bidirectional(\n",
    "        keras.layers.LSTM(64)\n",
    "    ),\n",
    "    \n",
    "    # Capa densa con activación ReLU\n",
    "    keras.layers.Dense(64, activation=\"relu\"),\n",
    "    \n",
    "    # Capa densa con activación sigmoide\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "]\n",
    "\n",
    "model = keras.Sequential(layers, name=\"sentiment_analysis_model\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa390f29-f39a-460e-976f-cd1fcd75af00",
   "metadata": {},
   "source": [
    "### 1.7.5 - Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638cf96e-67d0-4740-8cdc-2a94c3fa2e62",
   "metadata": {},
   "source": [
    "Ya está todo preparado para iniciar nuestro proceso de entrenamiento. En este caso, hemos seleccionado un proceso de 10 epochs. En este ejemplo vamos el conjunto de datos `test` como validación.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4d020b-44f0-4ef2-990d-e366cd181a1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"sgd\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b30860f-372d-47c4-8438-c13bd6cb7629",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "# Definición de los callback de TF Board\n",
    "tensorboard_callback = TensorBoard(log_dir=\"logs\")\n",
    "\n",
    "# Ejecución del proceso de aprendizaje\n",
    "model.fit(\n",
    "    train,\n",
    "    epochs=1, # Numero de epochs\n",
    "    validation_data=test,\n",
    "    validation_steps=1,\n",
    "    callbacks=[tensorboard_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c59f62-e68f-4a7d-883e-1a18a9d10ad8",
   "metadata": {},
   "source": [
    "### 1.7.6 - Prueba del modelo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
